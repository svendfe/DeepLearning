{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO7u2lzqm7TU"
      },
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2VwMDrMAUq7"
      },
      "source": [
        "# Using and fine-tuning BERT's friends in Hugging Face\n",
        "\n",
        "In this notebook, we will explore how to use BERT based models from Hugging Face for various NLP tasks. We will start by using the pre-trained model without fine-tuning, and then we will fine-tune the model for sentiment analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wpo22iwRlOi"
      },
      "source": [
        "## Check for GPU\n",
        "Make sure you have your GPU available. You will need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.9.0-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages (from torch) (4.15.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=0.8.5 (from torch)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
            "Downloading torch-2.9.0-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
            "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8/8\u001b[0m [torch]32m7/8\u001b[0m [torch]kx]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.9.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N3LcOY0BRmoM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU detected.\n",
            "If you are using Google Colab, please go to 'Runtime' > 'Change runtime type' and select 'GPU' as the hardware accelerator.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n",
        "    print(\"If you are using Google Colab, please go to 'Runtime' > 'Change runtime type' and select 'GPU' as the hardware accelerator.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIqdGKIunhcI"
      },
      "source": [
        "## Installation of Libraries\n",
        "Let us start installing required libraries for fine tuning a model. You may need to restart the environment (Runtime ‚Üí Restart Runtime)\n",
        "\n",
        "* transformers: provides pretrained models like BERT, RoBERTa, etc., for NLP tasks\n",
        "* datasets: easy access to thousands of NLP datasets with built-in processing tools\n",
        "* evaluate: provides standard evaluation metrics (e.g., accuracy, F1) for model evaluation\n",
        "* accelerate: utility for training models efficiently on CPU, GPU, or multi-GPU setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ch0RhGLPFE"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTY4Hb15R5gG"
      },
      "source": [
        "#Using pre-trained models without fine-tuning\n",
        "\n",
        "In this section, we‚Äôll focus on how to use pretrained transformer models without fine-tuning ‚Äî leveraging their powerful language understanding capabilities right out of the box.\n",
        "\n",
        "We‚Äôll explore the following tasks using models from the ü§ó Hugging Face Transformers library:\n",
        "\n",
        "* üîç Using pretrained models for inference: Perform NLP tasks directly with models like BERT, RoBERTa, and DistilBERT.\n",
        "\n",
        "* üß© Masked Language Modeling (MLM) with DistilBERT: Fill in missing words in a sentence using a distilled version of BERT.\n",
        "\n",
        "* üß† Masked Language Modeling with RoBERTa: Run the same task with a more robust model that improves on BERT.\n",
        "\n",
        "* üîó Next Sentence Prediction (NSP) with BERT: Use BERT to evaluate whether two sentences are likely to follow one another.\n",
        "\n",
        "* üìö Natural Language Inference (NLI): Use a model like roberta-large-mnli to classify the relationship between sentence pairs (entailment, neutral, contradiction).\n",
        "\n",
        "No training or fine-tuning is required ‚Äî we‚Äôre using the models as-is, just like prebuilt tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYZnn2F7ht8t"
      },
      "source": [
        "\n",
        "##Masked Language Modeling with DistilBERT\n",
        "First, we will use **DistilBERT** to complete masked phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7LdhEYIZSr8"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained model\n",
        "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
        "\n",
        "# Provide example inputs\n",
        "examples = [\n",
        "    \"The capital of France is [MASK].\",\n",
        "    \"The largest mammal is the [MASK].\",\n",
        "    \"Deep learning is a subset of [MASK].\"\n",
        "]\n",
        "\n",
        "# Get predictions\n",
        "for example in examples:\n",
        "    predictions = unmasker(example)\n",
        "    print(f\"Input: {example}\")\n",
        "    for pred in predictions:\n",
        "        print(f\"  {pred['sequence']} (score: {pred['score']:.4f})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBAimRAZQ8P"
      },
      "source": [
        "...it does work great. What about **Roberta**?\n",
        "\n",
        "##Masked Language Modeling with RoBERTa\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fET0t3WweEYX"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model\n",
        "unmasker = pipeline('fill-mask', model='roberta-base')\n",
        "\n",
        "# Provide example inputs\n",
        "#DitlBert usese [MASK] but with Roberta is <mask>.\n",
        "examples = [\n",
        "    \"The capital of France is <mask>.\",\n",
        "    \"The largest mammal is the <mask>.\",\n",
        "    \"Deep learning is a subset of <mask>.\"\n",
        "]\n",
        "\n",
        "# Get predictions\n",
        "for example in examples:\n",
        "    predictions = unmasker(example)\n",
        "    print(f\"Input: {example}\")\n",
        "    for pred in predictions:\n",
        "        print(f\"  {pred['sequence']} (score: {pred['score']:.4f})\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmdcdraYZRMb"
      },
      "source": [
        "...a little better. And it is not even roberta-large.\n",
        "\n",
        "DistilBERT learns a distilled (approximate) version of BERT, retaining 95% performance but **using only half the number of parameters** (66M vs 110M in Bert-base vs 340M in Bert-large).\n",
        "\n",
        "RoBERTa. Introduced at Facebook, Robustly optimized BERT approach RoBERTa, is a retraining of BERT with improved training methodology, 1000% more data and compute power. To improve the training procedure, RoBERTa removes the Next Sentence Prediction (NSP) task from BERT‚Äôs pre-training and introduces dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure. **RoBERTa improves 2-20% BERT performance.**\n",
        "\n",
        "\n",
        "\n",
        " [See comparasion Bert, RoBERTa, DistilBERT & XLNet ](https://miro.medium.com/v2/resize:fit:1243/1*5PzGl1dNt_5jMH3_mm3PpA.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xo-8CunZRYN"
      },
      "source": [
        "## Next Sentence Prediction with BERT\n",
        " As seen in the previous comparison, neither DistilBert nor RoBERTa natively support Next Sentence Prediction (NSP) because it was designed as a lighter version of BERT and does not include the NSP task in its pre-training objectives.\n",
        "\n",
        " Let use BERT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubjKSDsmiKnC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def get_nsp_probability(sentence1, sentence2):\n",
        "    # Tokenize input sentences\n",
        "    input_ids = tokenizer.encode(sentence1, sentence2, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Get NSP prediction\n",
        "    with torch.no_grad(): #This context manager is used to disable gradient computation during inference. It ensures that no gradients are calculated for the operations within the block. Since we‚Äôre not training the model here, we don‚Äôt need to track gradients.\n",
        "        logits = model(input_ids).logits #model(input_ids) computes the forward pass of the BERT model on the input token IDs. .logits extracts the raw output scores (logits) from the model. These logits represent the model‚Äôs confidence scores for each possible class (in this case, ‚Äúnext sentence‚Äù or ‚Äúnot next sentence‚Äù)\n",
        "        probabilities = torch.softmax(logits, dim=1) #torch.softmax(logits, dim=1) applies the softmax function along the second dimension (columns) of the logits tensor. This converts the raw scores into probabilities. The resulting probabilities tensor contains two values: the probability of the sentences being consecutive (‚Äúnext sentence‚Äù) and the probability of them not being consecutive (‚Äúnot next sentence‚Äù)\n",
        "        is_next_probability = probabilities[0, 0].item() #probabilities[0, 0] extracts the probability of being consecutive (i.e., ‚Äúnext sentence‚Äù) from the tensor. .item() converts the scalar tensor value to a Python float.\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Sentence 1: {sentence1}\")\n",
        "    print(f\"Sentence 2: {sentence2}\")\n",
        "    print(f\"Probability of being consecutive: {is_next_probability:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "    return is_next_probability\n",
        "\n",
        "# Example sentences\n",
        "# Pair 1\n",
        "sentence1 = \"The Eiffel Tower is located in Paris.\"\n",
        "sentence2 = \"It is one of the most famous structures in the world.\"\n",
        "get_nsp_probability(sentence1, sentence2)\n",
        "\n",
        "\n",
        "\n",
        "# Pair 2\n",
        "sentence1 = \"The stock market closed higher after the latest earnings reports.\"\n",
        "sentence2 = \"Penguins can hold their breath for up to 20 minutes underwater.\"\n",
        "get_nsp_probability(sentence1, sentence2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZTju6vZSJ_"
      },
      "source": [
        "While BERT includes a `Next Sentence Prediction (NSP)` objective during pretraining, its actual performance on this task can be misleading. In many practical cases, BERT tends to assign high probabilities even to sentence pairs that are clearly unrelated\n",
        "\n",
        "Because of these limitations, many newer models have removed the NSP objective entirely. For example:\n",
        "\n",
        "* RoBERTa, DistilBERT, and ALBERT exclude the NSP loss during pretraining because research has shown it does not significantly benefit downstream tasks like question answering or sentence classification.\n",
        "\n",
        "* Instead, tasks like textual entailment (e.g., with models trained on the Multi-Genre Natural Language Inference (MNLI) benchmark like `roberta-large-mnli`) offer more robust and interpretable results when comparing sentence relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lxXRc2VragY"
      },
      "source": [
        "##Natural Language Inference (NLI)\n",
        "\n",
        "NLI is a core task in Natural Language Processing (NLP) that involves determining the logical relationship between two sentences:\n",
        "\n",
        "* The premise: a given statement.\n",
        "\n",
        "* The hypothesis: another statement to be evaluated in relation to the premise.\n",
        "\n",
        "*The goal is to classify the relationship into one of three categories:\n",
        "\n",
        "  * Entailment, the hypothesis logically follows from the premise.\n",
        "  * Neutral,\tthe hypothesis is possibly true, but not guaranteed by the premise.\n",
        "  * Contradiction, the hypothesis directly contradicts the premise.\n",
        "\n",
        "`roberta-large-mnli` is a pretrained language model developed by Facebook AI (now Meta AI). It is based on the RoBERTa architecture and fine-tuned on the MNLI dataset. **MNLI** stands for Multi-Genre Natural Language Inference. It is a large benchmark dataset designed to test a model's ability to perform NLI across multiple genres (or domains) of text, such as: Fiction, Government documents, Telephone conversations, Travel guides, Slate magazine articles...\n",
        "\n",
        "NLI models can be used to generate high-quality semantic representations of sentences. Because they are trained to reason about sentence meaning and relationships, their internal embeddings capture rich contextual information. NLI models can also be adapted for zero-shot text classification. By phrasing class labels as hypotheses, you can evaluate how likely a sentence \"entails\" a label.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJn_ImpLsH_C"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the NLI model\n",
        "classifier = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
        "\n",
        "# Define sentence pairs to test\n",
        "examples = [\n",
        "    {\n",
        "        \"premise\": \"The Eiffel Tower is located in Paris.\",\n",
        "        \"hypothesis\": \"It is one of the most famous structures in the world.\"\n",
        "    },\n",
        "    {\n",
        "        \"premise\": \"The stock market closed higher after the latest earnings reports.\",\n",
        "        \"hypothesis\": \"Penguins can hold their breath for up to 20 minutes underwater.\"\n",
        "    },\n",
        "    {\n",
        "        \"premise\": \"A man is playing guitar on a stage.\",\n",
        "        \"hypothesis\": \"A person is making music.\"\n",
        "    },\n",
        "    {\n",
        "        \"premise\": \"A cat is sleeping on the couch.\",\n",
        "        \"hypothesis\": \"The dog is barking loudly.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Evaluate each sentence pair\n",
        "for i, pair in enumerate(examples, 1):\n",
        "    input_text = f\"{pair['premise']} </s> {pair['hypothesis']}\"\n",
        "    result = classifier(input_text)[0]\n",
        "\n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"Premise   : {pair['premise']}\")\n",
        "    print(f\"Hypothesis: {pair['hypothesis']}\")\n",
        "    print(f\"Prediction: {result['label']} (score: {result['score']:.4f})\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj2-TN291aAm"
      },
      "source": [
        "#Fine-tune a pretrained model with PyTorch Trainer\n",
        "\n",
        "Here we will fine-tune a pretrained model with ü§ó Transformers **Trainer**. The code is based on the the HuggingFace [tutorial](https://huggingface.co/docs/transformers/training).\n",
        "\n",
        "\n",
        "Transformers provides a [Trainer](https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/trainer#transformers.Trainer) class optimized for training ü§ó Transformers models, making it easier to start training without manually writing your own training loop. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) API supports a wide range of training options.\n",
        "\n",
        "The Trainer class provides an API for feature-complete training in PyTorch. Other options include using TensorFlow with Keras or native PyTorch for the fine-tuning.  Using high-level libraries has its disadvantages but tends to save time (especially in debugging).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIWpzzPl7m4Y"
      },
      "source": [
        "##Load a dataset for text classification\n",
        "\n",
        "We will use the [yelp_review_full](https://huggingface.co/datasets/Yelp/yelp_review_full) dataset, where reviews about businesses are labeled with 1 to 5 stars (labels 0 to 4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc4N2c4j2fVZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "\n",
        "# Print dataset example\n",
        "print(dataset['train'][0])\n",
        "print(dataset['train'][1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW9z0xkT3kmc"
      },
      "source": [
        "##Preparing the dataset with AutoTokenizer\n",
        "\n",
        " In the Hugging Face ecosystem, a `tokenizer`'s primary tasks are to preprocess textual data into a format that can be fed into a model.\n",
        "\n",
        " The main tasks of a Tokenizer include:\n",
        "\n",
        "1.   Tokenization: Splitting the input text into subwords or tokens. For example,  the word \"playing\" might be split into [\"play\", \"##ing\"].\n",
        "Conversion to IDs: Mapping tokens to their corresponding numerical IDs as per the model's vocabulary.\n",
        "2. Padding: Adding padding tokens to ensure that all input sequences in a batch are of the same length.\n",
        "3. Truncation: Cutting off sequences that are longer than the maximum allowed length.\n",
        "4. Adding Special Tokens: Inserting special tokens such as `[CLS]` (classification token) and `[SEP]` (separator token) which are required by specific models like BERT.\n",
        "5. Creating Attention Masks: Generating masks that indicate which tokens should be attended to (1 for real tokens, 0 for padding tokens).\n",
        "\n",
        "In the example, we will use the `bert-base-cased` language model (and, therefore, its tokenizer wich is recovered using the `AutoTokenizer` class).\n",
        "\n",
        "ü§ó `Datasets` map method allows us to apply a preprocessing function over the entire dataset.  The [yelp_review_full](https://huggingface.co/datasets/Yelp/yelp_review_full).\n",
        "\n",
        "‚ö†Ô∏è The dataset is large (700K rows), so it can take a while. After mapping the dataset to the tokenized version, we will create a smaller subset of the full dataset to fine-tune the model on (just 1K samples). This is done to reduce training time, especially when working in environments with limited resources like Google Colab, which has time and compute restrictions. However, if you're working locally with better hardware (e.g., a dedicated GPU) or using a more powerful cloud setup, you can experiment with the full dataset to potentially achieve better performance and generalization.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KoxCHno4cGd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# Print dataset example after tokenizing. Data contain: label, text, input_ids, token_type_ids, and attention_mask\n",
        "print(small_train_dataset[0])\n",
        "print(small_train_dataset[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkQdDH5v63C3"
      },
      "source": [
        "##Replacing the model head with AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "Since our goal is to perform text classification on the [yelp_review_full dataset](https://huggingface.co/datasets/yelp_review_full#data-fields) ‚Äî which contains labeled reviews divided into multiple sentiment classes (five labels) ‚Äî we use the Hugging Face class `AutoModelForSequenceClassification`.\n",
        "\n",
        "The pretrained head of the BERT model is discarded and replaced with a randomly initialized classification head. This new head is specifically designed to match the number of target classes in our task, which is why we must specify the number of classes using the `num_labels` parameter when loading the model with `AutoModelForSequenceClassification`.\n",
        "\n",
        "By doing this, the model initializes a new classification layer with the appropriate output size for our problem. This new head is then trained from scratch during fine-tuning, while the pretrained base model transfers its learned knowledge to support the classification task effectively.\n",
        "Alternatives in Hugging Face for fine-tuning for different tasks include:\n",
        "\n",
        "* AutoModelForTokenClassification ‚Äî for token-level tasks like Named Entity Recognition (NER).\n",
        "\n",
        "* AutoModelForQuestionAnswering ‚Äî for extractive QA tasks.\n",
        "\n",
        "* AutoModelForMultipleChoice ‚Äî for multiple choice tasks.\n",
        "\n",
        "* AutoModelForSeq2SeqLM ‚Äî for sequence-to-sequence tasks like translation or summarization.\n",
        "\n",
        "Each of these classes replaces the pretrained head with a task-specific head initialized randomly and fine-tuned on your dataset.\n",
        "\n",
        "\n",
        "\n",
        "‚ö†Ô∏è A warning is generated about some of the pretrained weights not being used and some weights being randomly initialized.  The weights of the new classification head (the ‚Äúhead‚Äù on top of the base model) are randomly initialized, since it needs to learn from scratch for the specific task at hand.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgj8HpMl7975"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDqjwNgvehy6"
      },
      "source": [
        "##Selecting hyperparameters with the TrainingArguments class\n",
        "\n",
        " Next, create a ü§ó ü§ó [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) object. This class holds all the hyperparameters and configuration options for training your model, such as learning rate, batch size, number of epochs, optimizer settings, and other flags that control different training behaviors.\n",
        "\n",
        "You can start with the default training settings, but TrainingArguments lets you customize many parameters like learning_rate, num_train_epochs, per_device_train_batch_size, and more.\n",
        "\n",
        "For this example, we will  specify the `output_dir` parameter, which defines the folder where the model checkpoints and predictions will be saved. (In Google Colab, you can view this directory in the \"Files\" tab.). We will also specify `report_to=\"none\"` to disable all external logging. This is especially useful to prevent the trainer from prompting you for a Weights & Biases API key during training,\n",
        "\n",
        "‚ö†Ô∏è If the code throws errors, try restarting the environment (Runtime ‚Üí Restart Runtime). If there are still errors, you can also check the discussion [TrainingArgument does not work on colab](https://discuss.huggingface.co/t/trainingargument-does-not-work-on-colab/43372/10)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti7SiXDbeuoQ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",  # Directory to save model checkpoints and outputs\n",
        "    report_to=\"none\"            # Disable logging to external trackers like Weights & Biases\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPR9sFJIezGh"
      },
      "source": [
        "## Creating a compute_metric function for evaluation\n",
        "\n",
        "An evaluation function has to be passed to the Trainer to compute and report metrics since Trainer does not automatically evaluate model performance during training. The ü§ó [Evaluate](https://huggingface.co/docs/evaluate/a_quick_tour) library provides a simple accuracy function you can load with the `evaluate.load` function.\n",
        "\n",
        "A `compute_metrics` function is defined and passed as an argument to the `Trainer`. Inside this function, the compute method of the loaded `metric` object is called to calculate the metric (e.g., accuracy) based on the model‚Äôs predictions and references. The `evaluation_strategy` parameter in the TrainingArguments controls how often evaluation is performed and metrics are reported, for example, at the end of each epoch during fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHg_2LTee9c7"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load the accuracy metric from the ü§ó Evaluate library\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred): #will be passed to a Trainer object\n",
        "    logits, labels = eval_pred  # Unpack predictions (logits) and true labels from the evaluation output\n",
        "    # Convert logits to predicted class indices by selecting the index with the highest value for each example\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Compute and return the accuracy metric by comparing predictions with true labels\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Define training arguments including output directory and evaluation strategy\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",  # Directory where model checkpoints and outputs will be saved\n",
        "    eval_strategy=\"epoch\"       # Perform evaluation at the end of each training epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZnxEHgk_ADL"
      },
      "source": [
        "##Creating a Trainer object and training\n",
        "\n",
        "A [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) object can be created now  with the objects created in the previous sections:  model, training arguments, training, and test datasets, and evaluation function. Then train!\n",
        "\n",
        "‚ö†Ô∏è You should not need an API key for wandb.ai because `report_to=\"none\"` in the TrainingArguments should disable logging to Weights & Biases (wandb). If requested, load again the training arguments above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PGR0kQK_SL9"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,  # The model to be trained (e.g., BERT with a classification head)\n",
        "    args=training_args,  # Training configuration and hyperparameters (TrainingArguments object)\n",
        "    train_dataset=small_train_dataset,  # The dataset used for training the model\n",
        "    eval_dataset=small_eval_dataset,  # The dataset used for evaluation during training\n",
        "    compute_metrics=compute_metrics,  # Function to calculate metrics like accuracy or F1 during evaluation\n",
        ")\n",
        "\n",
        "trainer.train()  # Start the training process using the specified model, datasets, and arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RMi6hDE01gz"
      },
      "source": [
        "I get an accuracy around 58% for the 5 classes. But the model is trained with a thousand samples in less than 7 minutes and improvement is seen as the epochs increase.\n",
        "\n",
        "\n",
        "\n",
        "You can increase ephocs with the `num_train_epochs` parameter in  training_arg in the [TrainingArguments class](https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/trainer#transformers.TrainingArguments). If your dataset is small, BERT might overfit quickly ‚Äî more than 4 epochs can cause overfitting or  catastrophic forgetting. **Catastrophic forgetting** refers to when a pre-trained model (like BERT) forgets its previously learned general knowledge during fine-tuning on a new task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0EWp4al76By"
      },
      "source": [
        "##Predicting with the fine-tuned model  \n",
        "\n",
        "Tutorials  do not usually include information on how to use the model in the prediction phase. This code example passes new revisions to the fine-tuned BERT model for text classification. As in general machine learning, the input in the prediction phase must be processed in the same way as it was done in training: with a Tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lszXdxSDAKEe"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# New reviews to classify\n",
        "new_reviews = [\n",
        "    \"The food was amazing and the service was excellent.\",\n",
        "    \"I did not enjoy the food at all.\",\n",
        "    \"The ambiance was nice but the food was just okay.\",\n",
        "    \"Terrible experience! Will not come back.\",\n",
        "    \"Best restaurant ever! Highly recommend.\",\n",
        "    \"Bad and expensive food, noisy and unpleasant atmosphere, slow and rude waiters. That's what you'll find in other restaurants, but here it's completely the opposite.\"\n",
        "]\n",
        "\n",
        "# Tokenize the new reviews using the same settings as in training\n",
        "inputs = tokenizer(new_reviews, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Check if CUDA is available and move the model to GPU if it is. The model and the input tensors need to be on the same device.\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move the inputs to the same device as the model.  The model and the input tensors need to be on the same device.\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    # Disable gradient calculations ‚Äî this speeds up inference and reduces memory usage\n",
        "    outputs = model(**inputs)\n",
        "    # 'inputs' is a dictionary of PyTorch tensors, e.g., {'input_ids': tensor, 'attention_mask': tensor}\n",
        "    # The model returns an object with 'logits', which is a tensor of shape (batch_size, num_classes)\n",
        "    logits = outputs.logits\n",
        "    # Use torch.argmax to get the predicted class index for each example in the batch\n",
        "    # 'predictions' is a 1D tensor of shape (batch_size,), each element is an integer class index\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Map predictions to human-readable class labels\n",
        "class_labels = [\"1 star\", \"2 stars\", \"3 stars\", \"4 stars\", \"5 stars\"]\n",
        "# 'class_labels' is a Python list of strings representing each class\n",
        "\n",
        "# Loop over the original reviews and their predicted classes\n",
        "for review, pred in zip(new_reviews, predictions):\n",
        "    # 'review' is a string, 'pred' is a tensor scalar, automatically converted to int for indexing\n",
        "    print(f\"{review}\\n‚Üí {class_labels[pred]}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAeNg3KdhfPT"
      },
      "source": [
        "\n",
        "The predictions are pretty good, but the last one is tricky: *Bad and expensive food, noisy and unpleasant atmosphere, slow and rude waiters. That's what you'll find in other restaurants, but here it's completely the opposite.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ery7aT6jBwmn"
      },
      "source": [
        "#Repeating with RoBERTa\n",
        "Instead of google-bert/bert-base-cased, let us try `FacebookAI/roberta-base` changing just 2 lines of code: the `AutoTokenizer` and `AutoModelForSequenceClassification` with the new head.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVvsMeeqBjr8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "###############   PREPARING DATA ##############################\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "\n",
        "# Print dataset example\n",
        "print(dataset['train'][0])\n",
        "print(dataset['train'][1])\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# Print dataset example after tokenizing. Data contain: label, text, input_ids, token_type_ids, and attention_mask\n",
        "print(small_train_dataset[0])\n",
        "print(small_train_dataset[1])\n",
        "\n",
        "###############   TRAINING  ##############################\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-large-cased\", num_labels=5)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\", num_labels=5)\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1) # calculates the index of the maximum value along the last axis (which corresponds to the predicted class) for each prediction. This converts the raw logits into discrete class predictions.\n",
        "    return metric.compute(predictions=predictions, references=labels) #Calculates the evaluation metrics based on the predicted classes (predictions) and the true classes (labels).\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\", report_to=\"none\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "###############   PREDICTING  ##############################\n",
        "\n",
        "# Import the correct tokenizer for RoBERTa\n",
        "from transformers import AutoTokenizer # Already imported at the top, but good to be explicit\n",
        "import torch\n",
        "\n",
        "# New reviews to classify\n",
        "new_reviews = [\n",
        "    \"The food was amazing and the service was excellent.\",\n",
        "    \"I did not enjoy the food at all.\",\n",
        "    \"The ambiance was nice but the food was just okay.\",\n",
        "    \"Terrible experience! Will not come back.\",\n",
        "    \"Best restaurant ever! Highly recommend.\",\n",
        "    \"Bad and expensive food, noisy and unpleasant atmosphere, slow and rude waiters. That's what you'll find in other restaurants, but here it's completely the opposite.\"\n",
        "]\n",
        "\n",
        "# Use the RoBERTa tokenizer loaded earlier\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\") # Not needed, already loaded\n",
        "\n",
        "# Tokenize the new reviews using the same settings as in training\n",
        "inputs = tokenizer(new_reviews, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Check if CUDA is available and move the model to GPU if it is. The model and the input tensors need to be on the same device.\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move the inputs to the same device as the model.  The model and the input tensors need to be on the same device.\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    # Disable gradient calculations ‚Äî this speeds up inference and reduces memory usage\n",
        "    outputs = model(**inputs)\n",
        "    # 'inputs' is a dictionary of PyTorch tensors, e.g., {'input_ids': tensor, 'attention_mask': tensor}\n",
        "    # The model returns an object with 'logits', which is a tensor of shape (batch_size, num_classes)\n",
        "    logits = outputs.logits\n",
        "    # Use torch.argmax to get the predicted class index for each example in the batch\n",
        "    # 'predictions' is a 1D tensor of shape (batch_size,), each element is an integer class index\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Map predictions to human-readable class labels\n",
        "class_labels = [\"1 star\", \"2 stars\", \"3 stars\", \"4 stars\", \"5 stars\"]\n",
        "# 'class_labels' is a Python list of strings representing each class\n",
        "\n",
        "# Loop over the original reviews and their predicted classes\n",
        "for review, pred in zip(new_reviews, predictions):\n",
        "    # 'review' is a string, 'pred' is a tensor scalar, automatically converted to int for indexing\n",
        "    print(f\"{review}\\n‚Üí {class_labels[pred]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud0xy-LQDKo-"
      },
      "source": [
        "I get around 62% accuracy in around 7 minutes with RoBERTa... 4 points better than BERT (58%). RoBERTa, as BERT, does not seem to get the last and tricky review:  *Bad and expensive food, noisy and unpleasant atmosphere, slow and rude waiters. That's what you'll find in other restaurants, but here it's completely the opposite.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7fVWNxU-kAm"
      },
      "source": [
        "\n",
        "# Conclusions and Next Steps\n",
        "\n",
        "This notebook explores the use of the BERT language model as well as some of its adaptations such as DitilBERT and RoBERTa.\n",
        "\n",
        "First of all, these models have been used for those tasks for which they were pretrained: masked language modeling and next setence prediction.\n",
        "\n",
        "Afterwards, BERT and RoBERTa have been fine-tuned for a multi-class text classification problem (or Sequence Classification).\n",
        "\n",
        "Thanks to the **PyTorch Trainer**, this fine-tuning requires very little source code. But at the same time, it offers custom tuning of the models **with your own data** unlike using the **pipeline** function to download and use already trained models. Furthermore, the evaluation of different language models is greatly simplified, with just two lines of code we go from Bert to RoBERTa.\n",
        "\n",
        "\n",
        "**Next Steps**\n",
        "\n",
        "*   You can try fine-tuning more efficient models like [DistilBert](https://huggingface.co/distilbert) or [ALBERT](https://huggingface.co/albert).\n",
        "*   You can try fine-tuning more powerful models like [XLNet](https://huggingface.co/xlnet) or the large versions of [BERT](https://huggingface.co/google-bert) and [RoBERTA](https://huggingface.co/FacebookAI).\n",
        "*   You can  lower the level of abstraction even further.  This [tutorial](https://huggingface.co/docs/transformers/training) describes how to use native PyTorch or TensorFlow with Keras in the same example.\n",
        "*   Check also the ü§ó [Task guides](https://huggingface.co/docs/transformers/tasks/sequence_classification) with, among others, examples of Text Classification, Token Classification, Question Answering, Translation, and Summarization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
