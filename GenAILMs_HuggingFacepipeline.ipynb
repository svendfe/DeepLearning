{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsdEkGT9jyDv"
      },
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2VwMDrMAUq7"
      },
      "source": [
        "# Hugging Face\n",
        "\n",
        "In this notebook, we'll explore the Hugging Face Transformers library ‚Äî one of the most powerful and user-friendly toolkits for modern Natural Language Processing (NLP) and generative AI. Our focus will be on using the high-level pipeline API, which allows you to perform complex tasks such as sentiment analysis, text generation, translation, summarization, and more with just a few lines of code.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "- Use the `pipeline()` function to apply pretrained models to a variety of NLP tasks\n",
        "\n",
        "- Download and switch between top-performing models from the Hugging Face Model Hub\n",
        "\n",
        "- Customize hyperparameters (e.g., temperature, max length) to control model behavior\n",
        "\n",
        "- Identify key NLP task categories and match them with appropriate models\n",
        "\n",
        "This hands-on introduction will help you become familiar with state-of-the-art models while building an understanding of what makes different tasks (classification, generation, question answering, etc.) unique. No deep ML coding required ‚Äî just curiosity and a few lines of Python!\n",
        "\n",
        "Make sure to check out the official Hugging Face [documentation](https://huggingface.co/docs)  and  [course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt) to go deeper.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD9M2-uM9LTZ"
      },
      "source": [
        "# Installing the Transformers library of Hugging Face\n",
        "\n",
        "You can run system commands by preceding them with the !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6HIfAswo9qus"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.conda/lib/python3.10/site-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.conda/lib/python3.10/site-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.conda/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.conda/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in ./.conda/lib/python3.10/site-packages (2.8.0)\n",
            "Requirement already satisfied: torchvision in ./.conda/lib/python3.10/site-packages (0.23.0)\n",
            "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.conda/lib/python3.10/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.conda/lib/python3.10/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.conda/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.conda/lib/python3.10/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./.conda/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: numpy in ./.conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers\n",
        "%pip install torch torchvision\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS available: True\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Check if MPS is available\n",
        "print(\"MPS available:\", torch.backends.mps.is_available())\n",
        "\n",
        "# Pick device (MPS if available, else CPU)\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btNV0-Ek-dcb"
      },
      "source": [
        "# Pipeline function of the Transformers library\n",
        "\n",
        "\n",
        "The most basic object in the Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n",
        "\n",
        "There are three main steps involved when you pass some text to a pipeline:\n",
        "\n",
        "*   The text is preprocessed into a format the model.\n",
        "*   The preprocessed inputs are passed to the model.\n",
        "*   The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "Some of the currently available pipelines are:\n",
        "- `sentiment-analysis`: Classify the sentiment of a piece of text (e.g., positive, negative). Useful for analyzing opinions in reviews or social media.\n",
        "\n",
        "- `zero-shot-classification`: Classify text into user-defined categories without any additional training. Great for flexible, on-the-fly topic classification.\n",
        "\n",
        "- `text-generation``: Generate coherent, human-like text from a prompt using language models like GPT. Used in chatbots, creative writing, etc.\n",
        "\n",
        "- `feature-extraction`: Convert text into vector embeddings. These numerical representations can be used for clustering, similarity search, or feeding into other ML models.\n",
        "\n",
        "- `fill-mask`: Predict missing words in a sentence with a [MASK] token. Demonstrates how masked language models (like BERT) understand context.\n",
        "\n",
        "- `ner`: Named Entity Recognition, Detect and classify named entities in text (like people, places, dates, organizations). Useful for information extraction.\n",
        "\n",
        "- `question-answering`: Extract answers from a given context based on a natural language question. Often used in reading comprehension and knowledge retrieval.\n",
        "\n",
        "- `summarization`: Produce a concise summary of a longer text while preserving key information. Ideal for news, reports, and document analysis.\n",
        "\n",
        "- `translation`: Translate text between different languages using pretrained translation models.\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8uTO0_TPomU"
      },
      "source": [
        "## Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ApHoBTK8Ngq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9860339760780334}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I really like the sentiment analysis problem\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nflcD58FGPrZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9997263550758362},\n",
              " {'label': 'NEGATIVE', 'score': 0.9997612833976746}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with several sentences\n",
        "classifier([\"DL4NLP does works well\", \"DL4NLP does not work well\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wr81PaG8GP5Y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.6868619322776794},\n",
              " {'label': 'NEGATIVE', 'score': 0.988868236541748}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#in Spanish (remember that the automatically downloaded model is distilbert-base-uncased-finetuned-sst-2-english)\n",
        "classifier([\"El an√°lisis de sentimientos es entretenido.\", \"Odio el an√°lisis de sentimientos\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM-nfXhBHK3K"
      },
      "source": [
        "## The model hub\n",
        "\n",
        "The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task.\n",
        "\n",
        "Go to the [Model Hub](https://huggingface.co/models) and  click on the corresponding tag on the left to display only the supported models for that task. You can refine your search for a model by clicking on the language tags, and pick a model that will generate text in another language. The Model Hub even contains *checkpoints* for multilingual models that support several languages.\n",
        "\n",
        "Let us try a model for the \"Text classification\" task in \"Spanish\". Use the search box to find a model for \"Sentiment Analysis\". Check also number of downloads and \"likes\".\n",
        "\n",
        "\n",
        "Note: a checkpoint is a model with the exact value of all its parameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gTk1I3_ArGQo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': '4 stars', 'score': 0.49561387300491333},\n",
              " {'label': '2 stars', 'score': 0.5113745331764221}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "classifier([\"Me gusta buscar y descargar modelos de Hugging Face\", \"El problema del an√°lisis de sentimientos me parece aburrido\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aSuAOGREJ37Q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POS', 'score': 0.9499416351318359},\n",
              " {'label': 'NEG', 'score': 0.9991242289543152}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n",
        "classifier([\"Me gusta buscar y descargar modelos de Hugging Face\", \"El problema del an√°lisis de sentimientos me parece aburrido\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHyI8yi2QoMW"
      },
      "source": [
        "##Text generation\n",
        "\n",
        "Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dwWyFWOiQzKR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': \"Natural Language Processing is \\xa0an ongoing effort to understand and develop software with the same goal of providing a high level of abstraction in a language. This is done by using a set of tools, such as the.NET SDK, to build and test a set of languages.\\nYou can download the project at github.\\nHere are the project's various parts:\\nThis project has a number of other components and tools that are available in the.NET Framework and Visual Basic.\\nYou can find some of these in the documentation, and some of the documentation in the README.\\nYou can also find some of these in the README.\\nThe project has the following components:\\nThis project includes a new tool called Visual Basic.NET Core (the latest version of the language), which is available for free. It is a collection of tools that help you write and test basic C# projects. The code for this tool is hosted on GitHub.\\nThis project uses the.NET Framework and Visual Basic.NET Core. This includes the following components:\\nThis is the project's core language, and it is compatible with all versions of the.NET Framework, including Visual Basic.\\nThis project uses the.NET Framework and Visual Basic.NET Core. This includes the following components:\\n\"}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"Natural Language Processing is \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttVPN50CU5Ov"
      },
      "source": [
        "\n",
        "Text generation involves randomness. Try several times for different results.\n",
        "\n",
        "The pipeline also accepts parameters such as max_lenght and num_return_sequences\n",
        "\n",
        "Try with another model from the [Model Hub](https://huggingface.co/models) (and check the \"Hosted inference API\" to try the model before downloading it).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UQy9nqzQb_-A"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Natural Language Processing is vernacular for any language that is written by a human language, and the word is written using its own language.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
              " {'generated_text': 'Natural Language Processing is vernacular, but it can be found in many languages, including Japanese, and in many languages and languages.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
              " {'generated_text': 'Natural Language Processing is !!\\n\\nI read into my comments and I have to admit that I don\\'t like how simple and easy it is to use the language. I\\'ve been using it for a while. I\\'m pretty much always using it for various reasons. I want the language to be the perfect language for my everyday needs. I\\'ve also used it for a while. I\\'ve also used the language to make many things (read, take a look at the documentation for more information) so that I can\\'t get into the details of the language. I\\'ve also used it for a while. I\\'ve also used it for a while. I\\'ve also used it for a while. I\\'ve also used it for a while. I\\'ve also used it for a while.\\nI\\'ve also used it for a while. This is exactly what I\\'m thinking.\\nI\\'ll probably be using it a few times, before I start reading. I\\'m sure it will catch you and keep you entertained.\\nHave you ever wondered, \"What is it like to learn a language?\" I\\'ll answer that question, so you can see why I\\'m so passionate about the language.\\nSo now, let\\'s go back to the topic of the language. I\\'ll start with the basics.'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"Natural Language Processing is \",\n",
        "    max_length=100,\n",
        "    num_return_sequences=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TroEIdcO2tmf"
      },
      "source": [
        "When selecting models from the Hugging Face Model Hub for text generation, be mindful of the number of parameters. Very large models (like LLaMA 13B+) can be too large for Google Colab (especially free-tier) and may lead to memory errors, slow execution, or crashes.\n",
        "\n",
        "‚úÖ For smooth use in Colab (especially with limited RAM or no GPU), it is recommended:\n",
        "\n",
        "- Choosing models with ‚â§ 500M parameters (e.g., distilgpt2, GPT2, opt-350m, mistralai/Mistral-7B-instruct on 8-bit quantized versions).\n",
        "\n",
        "- You can filter by model size on the Model Hub using the ‚Äú# of parameters‚Äù tag.\n",
        "\n",
        "- Look for keywords like ‚Äúdistil‚Äù, ‚Äútiny‚Äù, ‚Äúsmall‚Äù, or quantized versions when selecting a model for Colab.\n",
        "\n",
        "If you have access to a paid Colab plan or a dedicated server with sufficient memory/GPU, you can use larger language models (LLMs) the same way ‚Äî the loading and usage process remains identical via the transformers library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKJtwKwYTEDT"
      },
      "source": [
        "## Zero-shot Text Classification\n",
        "\n",
        "You‚Äôve already seen how the model can classify a sentence as positive or negative using those two labels (positive and negative).\n",
        "\n",
        "Now we need to classify texts that haven‚Äôt been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise.\n",
        "\n",
        "For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don‚Äôt have to rely on the labels of the pretrained model.\n",
        "\n",
        "**This is a great advance in out of the box tools for NLP!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q1kM2N6FTyKm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'sequence': 'This module is about the use of Deep Learning for Natural Language Processing ',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.45644259452819824, 0.3797537386417389, 0.1638035923242569]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This module is about the use of Deep Learning for Natural Language Processing \",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4njzUek3PB89"
      },
      "source": [
        "##Named entity recognition\n",
        "\n",
        "Named Entity Recognition (NER) is a Natural Language Processing (NLP) task that involves identifying and classifying named entities in a text.  These entities can include people (PER), locations (LOC), organizations (ORG), dates, quantities, and more. Therefore, NER is a specialized type of word-level classification (or token classification).\n",
        "\n",
        "For example, in the sentence: *Barack Obama was born in Hawaii* A NER model should recognize:\n",
        "\n",
        "- \"Barack Obama\" ‚Üí Person (PER)\n",
        "\n",
        "- \"Hawaii\" ‚Üí Location (LOC)\n",
        "\n",
        "When using Hugging Face‚Äôs `pipeline` for NER, we often pass the argument `grouped_entities=True`. This option:\n",
        "\n",
        "- Ensures that multi-token entities (like \"New York City\") are returned as a single grouped prediction, rather than separate predictions for each token.\n",
        "\n",
        "- Helps make the output more readable and meaningful for downstream use.\n",
        "\n",
        "Without grouped_entities=True, the pipeline might split \"New York City\" into three separate entities, even though they belong together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q3WmMWhvOvBw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n",
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.99905187,\n",
              "  'word': 'Geoffrey Everest Hinton',\n",
              "  'start': 0,\n",
              "  'end': 23},\n",
              " {'entity_group': 'MISC',\n",
              "  'score': 0.99335974,\n",
              "  'word': 'British',\n",
              "  'start': 52,\n",
              "  'end': 59},\n",
              " {'entity_group': 'MISC',\n",
              "  'score': 0.9986369,\n",
              "  'word': 'Canadian',\n",
              "  'start': 60,\n",
              "  'end': 68},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9985018,\n",
              "  'word': 'Google',\n",
              "  'start': 222,\n",
              "  'end': 228},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9489272,\n",
              "  'word': 'Google Brain',\n",
              "  'start': 230,\n",
              "  'end': 242},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.996902,\n",
              "  'word': 'University of Toronto',\n",
              "  'start': 252,\n",
              "  'end': 273}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxjjXjAOQUWv"
      },
      "source": [
        "## Summarization\n",
        "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bk9aNuMvQdX6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English . Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies . The 1980s and early 1990s mark the heyday of symbolic methods in NLP .'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "\"\"\"\n",
        "Symbolic NLP (1950s ‚Äì early 1990s)\n",
        "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n",
        "\n",
        "1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[1] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[2]) until the late 1980s when the first statistical machine translation systems were developed.\n",
        "1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.[3]\n",
        "1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY).\n",
        "1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[4]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[5]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[6]\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2c3eVkkSbp6"
      },
      "source": [
        "## Question answering\n",
        "The question-answering pipeline answers questions using information from a given contex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CKKrFZmERbUD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.827448271214962,\n",
              " 'start': 176,\n",
              " 'end': 188,\n",
              " 'answer': '2013 to 2023'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "question_answerer(\n",
        "    question=\"When does Geoffrey Everest Hinton worked at Google?\",\n",
        "    context=\"Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl3R6QUHS3SF"
      },
      "source": [
        "##Translation\n",
        "Translation is one of the most historically important and challenging problems in Natural Language Processing (NLP). A default model if can be used when providing  a language pair in the task name (such as `translation_en_to_fr`), but the easiest way is to get the model you want to use on the [Model Hub](https://huggingface.co/models) after selecting a language.\n",
        "\n",
        "Let us try English to Spanish.\n",
        "\n",
        "**Note**: Check the models examples and API to find parameters. Import require libraries.\n",
        "\n",
        "**Note 2**: sentencepiece is usually needed, if this error show up *\"ValueError: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.\"*, try *\"!pip install sentencepiece\"* and restart the kernel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XFn3naAF9Egw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in ./.conda/lib/python3.10/site-packages (0.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install sentencepiece\n",
        "import sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IUOm3Gs4TMxt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': '¬øQu√© puedo aprender en el Aprendizaje Profundo para el Procesamiento Natural del Lenguaje?'}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
        "translator(\"What can I learn in Deep Learning for Natural Language Processing?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdk9U03RX0gm"
      },
      "source": [
        "##Mask filling\n",
        "\n",
        "Fill-mask is a classic NLP task where the model predicts missing words in a sentence, essentially ‚Äúfilling in the blanks.‚Äù\n",
        "\n",
        "The `top_k` argument controls how many possibilities you want to be displayed.\n",
        "\n",
        "Keep in mind that different models may use different mask tokens (e.g., `<mask>`), so it‚Äôs important to verify the correct mask token for each model you explore (you can check this in the model‚Äôs API widget).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SqtpFr7QX7AH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.3618656396865845,\n",
              "  'token': 33832,\n",
              "  'token_str': ' physicist',\n",
              "  'sequence': 'Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian physicist, most noted for his work on artificial neural networks.'},\n",
              " {'score': 0.33519676327705383,\n",
              "  'token': 43027,\n",
              "  'token_str': ' mathematician',\n",
              "  'sequence': 'Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian mathematician, most noted for his work on artificial neural networks.'},\n",
              " {'score': 0.048829086124897,\n",
              "  'token': 9744,\n",
              "  'token_str': ' scientist',\n",
              "  'sequence': 'Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian scientist, most noted for his work on artificial neural networks.'},\n",
              " {'score': 0.043861620128154755,\n",
              "  'token': 5286,\n",
              "  'token_str': ' academic',\n",
              "  'sequence': 'Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian academic, most noted for his work on artificial neural networks.'},\n",
              " {'score': 0.04273000359535217,\n",
              "  'token': 9338,\n",
              "  'token_str': ' researcher',\n",
              "  'sequence': 'Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian researcher, most noted for his work on artificial neural networks.'}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian <mask>, most noted for his work on artificial neural networks.\", top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLPy4AQzZP1I"
      },
      "source": [
        "##Bias\n",
        "Pretrained language models like BERT learn patterns from vast amounts of text data, but this data often contains social biases.\n",
        "\n",
        "As a result, when asked to fill in missing words, the model may reflect or even amplify stereotypes present in the training data.\n",
        "\n",
        "Detecting and mitigating biases in AI is a very active area of research, aiming to make models more fair, ethical, and inclusive.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pErVOLr-ZdRR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"Ta his man works as [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mlBenmiV9lwD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
          ]
        }
      ],
      "source": [
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F0W-YOrYUlBi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': 'The president of Spain will be elected in the following months ',\n",
              " 'labels': ['politics', 'business', 'education'],\n",
              " 'scores': [0.9009101986885071, 0.07395102828741074, 0.025138691067695618]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier(\n",
        "    \"The president of Spain will be elected in the following months \",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvtg8HF97HAF"
      },
      "source": [
        "#Conclusions y Next Steps\n",
        "\n",
        "In this notebook, we explored the powerful and user-friendly Hugging Face Transformers library, which enables quick access to state-of-the-art pretrained models for a variety of natural language processing tasks. We covered essential functionalities such as:\n",
        "\n",
        "- Using the pipeline API to perform tasks like sentiment analysis, text generation, zero-shot classification, named entity recognition, summarization, question answering, translation, and mask filling.\n",
        "\n",
        "- How to navigate the Model Hub to select and download pretrained models tailored to your needs.\n",
        "\n",
        "- The importance of understanding and addressing biases present in pretrained models to ensure responsible AI use.\n",
        "\n",
        "These tools dramatically simplify the process of integrating sophisticated language understanding and generation capabilities into your projects, even with minimal coding.\n",
        "\n",
        "**Next Steps**\n",
        "\n",
        "To deepen your mastery and apply these skills effectively, consider the following:\n",
        "\n",
        "- Experiment with Fine-Tuning: Try fine-tuning pretrained models on your own datasets to tailor them to specific domains or tasks.\n",
        "\n",
        "- Explore Advanced Pipelines: Look into more complex pipelines such as conversational AI, text-to-speech, or multi-modal models.\n",
        "\n",
        "- Bias Mitigation: Dive deeper into research and techniques aimed at detecting and reducing bias in language models.\n",
        "\n",
        "- Optimize for Deployment: Learn about model optimization, quantization, and serving models efficiently for real-world applications.\n",
        "\n",
        "- Stay Updated:  the NLP and GenAI fields evolve rapidly ‚Äî regularly check the Hugging Face [documentation](https://huggingface.co/docs) and community resources to stay current."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.backends.mps.is_available())   # True if MPS is usable\n",
        "print(torch.backends.mps.is_built()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.conda/lib/python3.10/site-packages (4.56.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in ./.conda/lib/python3.10/site-packages (1.26.4)\n",
            "Requirement already satisfied: sentencepiece in ./.conda/lib/python3.10/site-packages (0.2.1)\n",
            "Collecting protobuf\n",
            "  Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting soundfile\n",
            "  Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.conda/lib/python3.10/site-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.conda/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.conda/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-21.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in ./.conda/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
            "Requirement already satisfied: xxhash in ./.conda/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.12.15-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in ./.conda/lib/python3.10/site-packages (from speechbrain) (1.5.2)\n",
            "Requirement already satisfied: scipy in ./.conda/lib/python3.10/site-packages (from speechbrain) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.9 in ./.conda/lib/python3.10/site-packages (from speechbrain) (2.8.0)\n",
            "Collecting torchaudio (from speechbrain)\n",
            "  Downloading torchaudio-2.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.2 kB)\n",
            "Collecting cffi>=1.0 (from soundfile)\n",
            "  Using cached cffi-2.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.7.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading multidict-6.6.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.20.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (73 kB)\n",
            "Requirement already satisfied: idna>=2.0 in ./.conda/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
            "Collecting pycparser (from cffi>=1.0->soundfile)\n",
            "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.9->speechbrain) (1.3.0)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
            "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.12.15-cp310-cp310-macosx_11_0_arm64.whl (468 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading multidict-6.6.4-cp310-cp310-macosx_11_0_arm64.whl (44 kB)\n",
            "Downloading yarl-1.20.1-cp310-cp310-macosx_11_0_arm64.whl (89 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Using cached cffi-2.0.0-cp310-cp310-macosx_11_0_arm64.whl (180 kB)\n",
            "Downloading frozenlist-1.7.0-cp310-cp310-macosx_11_0_arm64.whl (46 kB)\n",
            "Downloading propcache-0.3.2-cp310-cp310-macosx_11_0_arm64.whl (43 kB)\n",
            "Downloading pyarrow-21.0.0-cp310-cp310-macosx_12_0_arm64.whl (31.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl (131 kB)\n",
            "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Downloading torchaudio-2.8.0-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, pycparser, pyarrow, protobuf, propcache, multidict, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, ruamel.yaml, multiprocess, cffi, aiosignal, torchaudio, soundfile, hyperpyyaml, aiohttp, speechbrain, datasets\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22/22\u001b[0m [datasets]/22\u001b[0m [datasets]in]]\n",
            "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.3.0 cffi-2.0.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 hyperpyyaml-1.2.2 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 protobuf-6.32.1 pyarrow-21.0.0 pycparser-2.23 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.12 soundfile-0.13.1 speechbrain-1.0.3 torchaudio-2.8.0 yarl-1.20.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets numpy sentencepiece protobuf speechbrain soundfile librosa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SpeechT5ForSpeechToText were not initialized from the model checkpoint at Amirhossein75/Speech-Conversion and are newly initialized: ['speecht5.decoder.prenet.embed_tokens.weight', 'text_decoder_postnet.lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The following `model_kwargs` are not used by the model: ['speech', 'reference'] (note: typos in the generate arguments will also show up in this list)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m ref \u001b[38;5;241m=\u001b[39m processor(ref_audio, sampling_rate\u001b[38;5;241m=\u001b[39mref_sr, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# The model likely expects something like:\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# The output might be raw waveform, or representation to pass into vocoder\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# If needed, run HiFiGAN vocoder\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# e.g. vocoder = AutoModel.from_pretrained(\"microsoft/speecht5_hifigan\") ...\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# vocoder(...) ‚Üí waveform\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Save output\u001b[39;00m\n\u001b[1;32m     47\u001b[0m sf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/transformers/generation/utils.py:2282\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2277\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(\n\u001b[1;32m   2280\u001b[0m     generation_config, use_model_defaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2281\u001b[0m )\n\u001b[0;32m-> 2282\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   2285\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/transformers/generation/utils.py:1584\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1587\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['speech', 'reference'] (note: typos in the generate arguments will also show up in this list)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoFeatureExtractor, AutoModelForSeq2SeqLM\n",
        "from speechbrain.pretrained import EncoderDecoderASR\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "\n",
        "# For the Speech-Conversion model\n",
        "from speechbrain.pretrained import SpeakerRecognition  # for speaker embeddings maybe, or use the ones bundled\n",
        "\n",
        "# Model ID\n",
        "MODEL_ID = \"Amirhossein75/Speech-Conversion\"\n",
        "\n",
        "# Helper: pick device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load model & processor\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer\n",
        "\n",
        "# The speech conversion model is encoder-decoder + vocoder + speaker embedding\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_ID).to(device)\n",
        "processor = AutoFeatureExtractor.from_pretrained(MODEL_ID)  # or equivalent if different name\n",
        "\n",
        "# Load source utterance (content) and reference (target speaker audio)\n",
        "src_audio, src_sr = librosa.load(\"audio/yo.wav\", sr=16000)  # forces 16kHz\n",
        "ref_audio, ref_sr = librosa.load(\"audio/clau.wav\", sr=16000)  # forces 16kHz\n",
        "\n",
        "\n",
        "# If needed: resample to 16000 Hz, mono etc\n",
        "# e.g. use librosa or torchaudio for resampling if necessary\n",
        "\n",
        "# Prepare inputs\n",
        "# Adapted from convert_once.py in the model repo\n",
        "inputs = processor(src_audio, sampling_rate=src_sr, return_tensors=\"pt\").input_values.to(device)\n",
        "ref = processor(ref_audio, sampling_rate=ref_sr, return_tensors=\"pt\").input_values.to(device)\n",
        "\n",
        "# The model likely expects something like:\n",
        "outputs = model.generate(speech=inputs, reference=ref)\n",
        "\n",
        "# The output might be raw waveform, or representation to pass into vocoder\n",
        "# If needed, run HiFiGAN vocoder\n",
        "# e.g. vocoder = AutoModel.from_pretrained(\"microsoft/speecht5_hifigan\") ...\n",
        "# vocoder(...) ‚Üí waveform\n",
        "\n",
        "# Save output\n",
        "sf.write(\"converted.wav\", outputs.cpu().numpy(), samplerate=16000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  available_backends = torchaudio.list_audio_backends()\n",
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "expand(MPSFloatType{[1, 1, 1, 512]}, size=[-1, 1, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 5. Convert speech\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     speech \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 6. Save result\u001b[39;00m\n\u001b[1;32m     36\u001b[0m sf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio/converted.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, speech\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py:3037\u001b[0m, in \u001b[0;36mSpeechT5ForSpeechToSpeech.generate_speech\u001b[0;34m(self, input_values, speaker_embeddings, attention_mask, threshold, minlenratio, maxlenratio, vocoder, output_cross_attentions, return_output_lengths)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m speaker_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3035\u001b[0m     speaker_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m), device\u001b[38;5;241m=\u001b[39minput_values\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 3037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_generate_speech\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminlenratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxlenratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_cross_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_output_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py:2348\u001b[0m, in \u001b[0;36m_generate_speech\u001b[0;34m(model, input_values, speaker_embeddings, attention_mask, threshold, minlenratio, maxlenratio, vocoder, output_cross_attentions, return_output_lengths)\u001b[0m\n\u001b[1;32m   2345\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;66;03m# Run the decoder prenet on the entire output sequence.\u001b[39;00m\n\u001b[0;32m-> 2348\u001b[0m decoder_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeecht5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;66;03m# Run the decoder layers on the last element of the prenet output.\u001b[39;00m\n\u001b[1;32m   2350\u001b[0m decoder_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mspeecht5\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mwrapped_decoder(\n\u001b[1;32m   2351\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mdecoder_hidden_states[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m   2352\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2358\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2359\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.10/site-packages/transformers/models/speecht5/modeling_speecht5.py:694\u001b[0m, in \u001b[0;36mSpeechT5SpeechDecoderPrenet.forward\u001b[0;34m(self, input_values, speaker_embeddings)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m speaker_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     speaker_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(speaker_embeddings)\n\u001b[0;32m--> 694\u001b[0m     speaker_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([inputs_embeds, speaker_embeddings], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    696\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeaker_embeds_layer(inputs_embeds))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expand(MPSFloatType{[1, 1, 1, 512]}, size=[-1, 1, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 1. Load models\n",
        "processor = SpeechT5Processor.from_pretrained(\"Amirhossein75/Speech-Conversion\")\n",
        "model = SpeechT5ForSpeechToSpeech.from_pretrained(\"Amirhossein75/Speech-Conversion\").to(device)\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
        "\n",
        "# 2. Load audios (force 16kHz)\n",
        "src_audio, _ = librosa.load(\"audio/yo.wav\", sr=16000)\n",
        "ref_audio, _ = librosa.load(\"audio/clau.wav\", sr=16000)\n",
        "\n",
        "# 3. Encode source audio with processor\n",
        "inputs = processor(audio=src_audio, sampling_rate=16000, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 4. Extract speaker embedding from reference audio\n",
        "spkrec = EncoderClassifier.from_hparams(\n",
        "    source=\"speechbrain/spkrec-xvect-voxceleb\",\n",
        "    run_opts={\"device\": \"cpu\"}\n",
        ")\n",
        "embedding = spkrec.encode_batch(torch.tensor(ref_audio).unsqueeze(0))\n",
        "speaker_embeddings = embedding.to(device)\n",
        "\n",
        "# 5. Convert speech\n",
        "with torch.no_grad():\n",
        "    speech = model.generate_speech(inputs[\"input_values\"], speaker_embeddings, vocoder=vocoder)\n",
        "\n",
        "# 6. Save result\n",
        "sf.write(\"audio/converted.wav\", speech.cpu().numpy(), samplerate=16000)\n",
        "\n",
        "print(\"‚úÖ Saved converted audio to audio/converted.wav\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
