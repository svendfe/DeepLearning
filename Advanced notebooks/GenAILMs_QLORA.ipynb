{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M64LBXk8sfEB"
      },
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXkK3P8qshg7"
      },
      "source": [
        "# A Simplified Guide to Fine-Tuning an Open LLM with QLoRA\n",
        "\n",
        "This notebook provides a step-by-step guide to performing instruction-based fine-tuning on an open-source Large Language Model.\n",
        "\n",
        "Core Objectives:\n",
        "* Simplicity: Use high-level libraries from the Hugging Face ecosystem to make the process straightforward.\n",
        "* Efficiency: Employ QLoRA (Quantization and Low-Rank Adapters) to fine-tune the model on a single, free T4 GPU in Google Colab.\n",
        "*  Verification: Demonstrate the improvement by comparing the model's performance before and after fine-tuning.\n",
        "\n",
        "\n",
        "While TensorFlow is a powerful and robust framework,  particularly for techniques like QLoRA and *parameter-efficient fine-tuning* (PEFT), is  dominated by PyTorch. The Hugging Face `peft` and `bitsandbytes` libraries, which are essential for this efficient approach, have the most stable and feature-rich implementations in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSFmnYHQtThh"
      },
      "source": [
        "#Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoOzfuF7tVTF"
      },
      "source": [
        "## Installation of Libraries\n",
        "First, we need to install the necessary Python libraries. We'll use the -q flag for a quiet installation to keep the output clean.\n",
        "\n",
        "* transformers: Provides the LLM models and tokenizers (e.g., our base model, TinyLlama).\n",
        "\n",
        "* peft: The Parameter-Efficient Fine-Tuning library. This is where the LoRA/QLoRA logic resides.\n",
        "\n",
        "* bitsandbytes: Enables the 4-bit quantization, which is the \"Q\" in QLoRA. This is what drastically reduces memory usage.\n",
        "\n",
        "* datasets: A library for easily loading and processing datasets, including those from the Hugging Face Hub.\n",
        "\n",
        "* accelerate: A library from Hugging Face that helps abstract away the hardware (CPU/GPU) and enables seamless distributed training (though we'll use a single GPU here).\n",
        "\n",
        "* trl: The Transformer Reinforcement Learning library, which provides a high-level SFTTrainer (Supervised Fine-tuning Trainer) that simplifies the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdnWc52bCTnm"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 1: INSTALL LIBRARIES\n",
        "# ==============================================================================\n",
        "# This installs all the necessary libraries for running the fine-tuning process.\n",
        "# - transformers: For models and tokenizers.\n",
        "# - peft: For LoRA/QLoRA implementation.\n",
        "# - bitsandbytes: For 4-bit quantization.\n",
        "# - datasets: For loading the training data.\n",
        "# - accelerate: For hardware abstraction.\n",
        "# - trl: For the Supervised Fine-tuning Trainer (SFTTrainer).\n",
        "# ==============================================================================\n",
        "!pip install -q transformers peft bitsandbytes datasets accelerate trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AzrkCD8CvBh"
      },
      "source": [
        "## Check for GPU\n",
        "Make sure you have your GPU available. You will need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TeYox5mCgYa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n",
        "    print(\"If you are using Google Colab, please go to 'Runtime' > 'Change runtime type' and select 'GPU' as the hardware accelerator.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oRQhh7qEdCK"
      },
      "source": [
        "##Logging in to Hugging Face Hub with a Token  \n",
        "To download or fine-tune models from the Hugging Face Hub (especially private ones), you need to authenticate using your personal API token. This code demonstrates how to securely log in using a secret stored in Google Colab‚Äôs userdata vault, instead of hardcoding your token.\n",
        "\n",
        "* Visit https://huggingface.co/settings/tokens\n",
        "* Click \"New token\", give it a name, and select the scopes you need (e.g., \"Read\" or \"Write\" if uploading models).\n",
        "* In your Colab notebook, click on the üîê ‚ÄúSecrets‚Äù tab (or use the menu: Tools > Secrets).\n",
        "\n",
        "* Add a new secret: Name: HUGGINGFACE_API_KEY Value: your copied Hugging Face token\n",
        "\n",
        "* Run the following code  to authenticate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivf3_jxSEfet"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Leer el token desde los secretos de Google Colab\n",
        "token = userdata.get('HUGGINGFACE_API_KEY')\n",
        "\n",
        "# Autenticarse con Hugging Face\n",
        "login(token=token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grd96zajFqpj"
      },
      "source": [
        "#Library Imports, model base, and loading the Dataset\n",
        "\n",
        "As the base model for fine-tuning, we will use `TinyLlama` ‚Äî a compact, fast, and efficient 1.1B parameter model. It has been pretrained on approximately 1 trillion tokens and is well-suited for instruction tuning. TinyLlama is ideal for low-resource environments such as Google Colab or GPUs with 16GB of VRAM.\n",
        "\n",
        "When illustrating QLoRA for instruction tuning, it‚Äôs important to start from a base model that has not already been fine-tuned on instructions. The `TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T` model is a partially trained checkpoint that has not undergone instruction tuning, which makes it suitable for demonstrating how QLoRA can align a model with instruction-following capabilities.  In contrast, `TinyLlama-1.1B-Chat` is already instruction-tuned.  \n",
        "\n",
        "We will fine-tune the model to follow instructions better. We'll use a small, pre-formatted instruction dataset called `mlabonne/guanaco-llama2-1k`. It's a subset of the Guanaco dataset, containing 1,000 examples, which is perfect for a quick fine-tuning demonstration.\n",
        "\n",
        "The `load_dataset` function from the `datasets` library downloads and caches the data for us.\n",
        "\n",
        "Data Structure: The dataset returns a `Dataset` object, which is similar to a Python dictionary. Each entry has a key (e.g., 'train') and a value containing the data. The data itself is structured with a single column named text that contains the full instruction prompt and response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dMEjBJ_vQvm"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 2: IMPORT LIBRARIES\n",
        "# ==============================================================================\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: CONFIGURE MODEL, TOKENIZER, AND DATASET\n",
        "# ==============================================================================\n",
        "# Using TinyLlama ‚Äî a small, fast, and efficient 1.1B parameter model\n",
        "# Pretrained for ~1 trillion tokens and compatible with instruction tuning\n",
        "# Ideal for low-resource environments (e.g. Google Colab, 16GB VRAM GPUs)\n",
        "#model_id = \"google/gemma-2b\"\n",
        "#model_id =  \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "\n",
        "# Dataset: Guanaco (LLaMA 2-style instruction-tuning dataset)\n",
        "# Contains ~1,000 examples in the format:\n",
        "# \"### Instruction:\\n{question}\\n\\n### Response:\\n{answer}\"\n",
        "# Originally designed for LLaMA2, but also useful for tuning smaller chat models\n",
        "# This helps TinyLlama learn to follow prompt-response structure\n",
        "\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset for training\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Imprimir un ejemplo\n",
        "print(\"-------- A couple of exemples of instruction/response --------\")\n",
        "print(dataset[6]['text'])\n",
        "print(dataset[10]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_XcpTApvRTO"
      },
      "source": [
        "#Loading the Base Model and Tokenizer\n",
        "\n",
        "This is the core of our setup.  \n",
        "\n",
        "**Quantization (QLoRA)**: To make this work on a Colab GPU, we will load the model in 4-bit precision. This is configured using the BitsAndBytesConfig class from transformers.\n",
        "* `load_in_4bit=True`: This is the flag that tells the model to load with 4-bit quantization.\n",
        "* `bnb_4bit_quant_type=\"nf4\"`: We use the \"nf4\" (Normalized Float 4) quantization type, which is a state-of-the-art technique for maintaining performance with 4-bit models.\n",
        "* `bnb_4bit_compute_dtype=torch.bfloat16`: While the model weights are stored in 4-bit, computations are performed in a higher-precision format (16-bit bfloat) for stability and accuracy.\n",
        "\n",
        "**Tokenizer**: The tokenizer converts our text prompts into a format the model can understand (tokens). We load the tokenizer that corresponds to our chosen model. We set `padding_side=\"right\"` to prevent issues with certain model architectures during open-ended generation. In open-ended generation tasks (for example, when you provide a prompt and the model continues the text), if padding is added to the left (i.e., at the beginning of the input), some models may get confused and start generating text based on the padding tokens.\n",
        "\n",
        "The **base model** will also be evaluated using a sample prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6Zqp-BoHA2J"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 4: LOAD MODEL AND TOKENIZER WITH 4-BIT QUANTIZATION (QLORA)\n",
        "# ==============================================================================\n",
        "# Configure BitsAndBytes for 4-bit quantization to save memory\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the base model with the specified quantization configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load the tokenizer associated with the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: EVALUATE THE BASE MODEL (BEFORE FINE-TUNING)\n",
        "# ==============================================================================\n",
        "print(\"--- CHECKING THE BASE MODEL ---\")\n",
        "prompt_before = \"Who is the Prime Minister of Canada?\"\n",
        "instruction_prompt_before = f\"### Instruction:\\n{prompt_before}\\n\\n### Response:\\n\"\n",
        "\n",
        "# Create a text generation pipeline\n",
        "base_model_pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100  # Use max_new_tokens to control the length of the new text\n",
        ")\n",
        "\n",
        "result_before = base_model_pipe(instruction_prompt_before)\n",
        "print(\"\\n--- BASE MODEL OUTPUT ---\")\n",
        "print(f\"Prompt:\\n{instruction_prompt_before}\")\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(result_before[0][\"generated_text\"].replace(instruction_prompt_before, \"\").strip())\n",
        "\n",
        "\n",
        "# Generate and store the result\n",
        "result_before = base_model_pipe(instruction_prompt_before)\n",
        "print(\"--- EVALUATION OF BASE MODEL COMPLETE ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFPJmGnEw4gx"
      },
      "source": [
        "#Configure LoRA (Low-Rank Adaptation)\n",
        "\n",
        "Now we configure LoRA. Instead of training the entire model (which has billions of parameters), we will only train small \"adapter\" layers. This is the \"PEFT\" part.\n",
        "\n",
        "* `LoraConfig`: This object holds the configuration for our LoRA layers.\n",
        "\n",
        "* `r`: The rank of the update matrices. A lower rank means fewer trainable parameters. A common value is 8 or 16.\n",
        "\n",
        "* lora_alpha: A scaling factor for the LoRA updates. It's common practice to set this to twice the rank (r).\n",
        "\n",
        "* `target_modules`: This is crucial. We specify which layers of the original model we want to attach our LoRA adapters to. For TinyLlama, these are typically the query, key, value, and output projection layers of the attention mechanism. Optionally: gate_proj, up_proj, down_proj (from the MLP/feedforward layer).\n",
        "\n",
        "* `lora_dropout`: A dropout probability for the LoRA layers to prevent overfitting.\n",
        "\n",
        "* `bias`: Specifies how to treat bias parameters. 'none' is standard.\n",
        "\n",
        "* `task_type`: We specify 'CAUSAL_LM' because we are training a model for text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjuRvq_7hVcT"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. LoRA CONFIGURATION\n",
        "# ==============================================================================\n",
        "# This config enables Low-Rank Adaptation (LoRA), which adds a small number of\n",
        "# trainable parameters to the base model for efficient fine-tuning.\n",
        "peft_config = LoraConfig(\n",
        "    r=16,  # Rank of the decomposition matrices\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[  # Specify which layers to apply LoRA to\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "    bias=\"none\",  # Don't add trainable biases\n",
        "    task_type=\"CAUSAL_LM\",  # Task type: causal language modeling\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcpY5xKchTbp"
      },
      "source": [
        "#Set Up and Run the Training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We'll use the `SFTTrainer` (Supervised Fine-Tuning Trainer), which is designed specifically for this kind of instruction fine-tuning.\n",
        "\n",
        "`TrainingArguments`: This object defines all the hyperparameters for the training process:\n",
        "\n",
        "* `max_length`: The maximum number of tokens in each input sequence. Shorter sequences use less memory and compute, which is helpful if you're fine-tuning on a limited-resource machine.  **‚ö†Ô∏è Tune this to avoid  OOM (out-of-memory) errors. The instruction and response together can be 1024 or more in the guanaco-llama2-1k dataset. Low values will make prompts + responses cut off.**\n",
        "\n",
        "* `per_device_train_batch_size`: Number of samples per batch on each GPU. Keep this low (e.g., 1 or 2) for Colab.\n",
        "\n",
        "* `gradient_accumulation_steps`: Number of steps to accumulate gradients before  updating the model's weights. This effectively increases the batch size without using more memory. batch_size * accumulation_steps is your effective batch size.\n",
        "* `learning_rate`: The speed at which the model learns. 2e-4 (or 0.0002) is a relatively high learning rate ‚Äî good for fast adaptation during LoRA or SFT.If the model is unstable, you can try lowering this.\n",
        "\n",
        "* `output_dir`: Where to save checkpoints and the final model.\n",
        "\n",
        "* `report_to`: Controls integration with experiment tracking tools like Weights & Biases\n",
        "\n",
        "* `dataset_text_field`:  The column name in your dataset that contains the full prompt + response.\n",
        "\n",
        "\n",
        "`SFTTrainer`: This orchestrates the entire training process.\n",
        "\n",
        "* model:\tYour base LLM (TinyLlama, etc.), quantized or not\n",
        "* train_dataset;\tDataset with a \"text\" column\n",
        "* args:\tTraining config (from SFTConfig)\n",
        "* peft_config:\tLoRA settings: what layers to adapt, etc.\n",
        "tokenizer\tNot required explicitly ‚Äî inferred from model\n",
        "\n",
        "Remember that one *step* is one mini training cycle using a batch of examples.  During a single step, the model:\n",
        "\n",
        "1. Processes a batch of input data\n",
        "\n",
        "2. Performs a forward pass to generate predictions\n",
        "\n",
        "3. Computes the loss by comparing predictions to the ground truth\n",
        "\n",
        "4. Performs a backward pass to compute gradients and updates the model's weights\n",
        "\n",
        "In QLoRA,\n",
        "* The base model weights are frozen and stored in quantized form (saving memory).\n",
        "* Only the LoRA adapters‚Äô parameters are updated during the backward pass.\n",
        "\n",
        "\n",
        "**‚ö†Ô∏è The PEFT takes over an hour here**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VVi_u2b1LehT"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 7. SUPERVISED FINE-TUNING CONFIGURATION (SFT)\n",
        "# ==============================================================================\n",
        "# This config controls the behavior of the SFTTrainer (Supervised Fine-Tuning).\n",
        "# It's designed to reduce memory usage and avoid Weights & Biases logging.\n",
        "sft_config = SFTConfig(\n",
        "    max_length=1024,  # Maximum sequence length (lower = less memory usage)\n",
        "    per_device_train_batch_size=1,  # Small batch size for low-memory environments\n",
        "    gradient_accumulation_steps=2,  # Simulate larger batches by accumulating gradients\n",
        "    learning_rate=2e-4,  # Learning rate for the optimizer\n",
        "    output_dir=\"./TinyLlama-tuned-results\",  # Where to save model outputs\n",
        "    report_to=\"none\",  # Disable logging to W&B or other tracking tools\n",
        "    dataset_text_field=\"text\",  # Column name in dataset that contains text\n",
        "    packing=False,  # Don't combine multiple samples into one input sequence\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. CREATE THE TRAINER\n",
        "# ==============================================================================\n",
        "# The SFTTrainer handles training, evaluation, and saving.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # Your base model (already quantized with BitsAndBytes, ideally)\n",
        "    train_dataset=dataset,  # The training data\n",
        "    args=sft_config,  # Training configuration\n",
        "    peft_config=peft_config  # LoRA configuration for parameter-efficient tuning\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 9. TRAIN THE MODEL\n",
        "# ==============================================================================\n",
        "# Launch training. This may take time depending on the model size and hardware.\n",
        "trainer.train()\n",
        "print(\"--- FINE-TUNING COMPLETE ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4LelFmXQefB"
      },
      "source": [
        "‚ö†Ô∏è **You may see the warning:**  \"No label_names provided for model class `PeftModelForCausalLM`...\"  This happens because PEFT wraps the base model, hiding its internals,  so the Trainer cannot automatically infer the training labels.  For causal language modeling  (predicting next tokens), this is usually not a problem, since the default behavior (using input_ids shifted by 1) is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh3rRf4Qqude"
      },
      "source": [
        "# Final Check - Performance After Fine-Tuning\n",
        " We will ask the model the exact same question as in Step 5 and see if its response has improved. The fine-tuned model should  provide a much more direct and accurate answer, following the instruction format it learned from the Guanaco dataset.\n",
        "\n",
        " We will use Hugging Face‚Äôs `pipeline` utility to easily create text-generation pipelines  for both the base model and the fine-tuned model. This allows us to generate text  in a simple and consistent way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9822jDnl_9X"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 10: COMPARE BASE AND FINE-TUNED MODEL RESPONSES ON MULTIPLE PROMPTS\n",
        "# ==============================================================================\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"\\n--- COMPARING BASE AND FINE-TUNED MODEL OUTPUTS ---\\n\")\n",
        "\n",
        "\n",
        "# Create a text-generation pipeline using the base model.\n",
        "# This pipeline will generate up to 100 new tokens using the base model and its tokenizer,\n",
        "# with computations performed in bfloat16 precision for efficiency.\n",
        "\n",
        "base_model_pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Convert the fine-tuned model's parameters to bfloat16 dtype to match the base model.\n",
        "trainer.model = trainer.model.to(dtype=torch.bfloat16)\n",
        "# Create a text-generation pipeline using the fine-tuned model loaded from the trainer.\n",
        "# This pipeline will also generate up to 100 new tokens, using the same tokenizer and precision.\n",
        "fine_tuned_pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=trainer.model,         # Fine-tuned model loaded from the Trainer\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "\n",
        ")\n",
        "\n",
        "# List of example prompts to evaluate\n",
        "prompts = [\n",
        "    # 1. SPECIFIC INSTRUCTION FOLLOWING\n",
        "    \"Create a numbered list of 5 benefits of renewable energy. Each point should be exactly one sentence long.\",\n",
        "\n",
        "    # 2. FORMAT AND STRUCTURE SPECIFIC\n",
        "    \"Write a professional email to a client explaining a project delay. Use formal tone and include: greeting, explanation, apology, new timeline, and closing.\",\n",
        "\n",
        "    # 3. STEP-BY-STEP REASONING\n",
        "    \"A store has 150 apples. They sell 40% on Monday and 25% of the remaining on Tuesday. How many apples are left? Show your work step by step.\", #result: 67.5 apples!\n",
        "\n",
        "    # 4. ANALYSIS AND COMPARISON\n",
        "    \"Compare Python and JavaScript programming languages. Provide exactly 3 similarities and 3 differences in a clear format.\",\n",
        "\n",
        "    # 5. INSTRUCTIONS WITH CONSTRAINTS\n",
        "    \"Explain photosynthesis in exactly 3 sentences. Use simple words that a 10-year-old could understand.\",\n",
        "\n",
        "    # 6. CREATIVITY WITH SPECIFIC PARAMETERS\n",
        "    \"Write a haiku about technology. Follow the traditional 5-7-5 syllable pattern exactly.\",\n",
        "\n",
        "    # 7. PRACTICAL PROBLEM SOLVING\n",
        "    \"I have a job interview tomorrow at 9 AM. It's 8 PM now. Create a checklist of 6 things I should do tonight to prepare.\",\n",
        "\n",
        "    # 8. SIMPLIFIED TECHNICAL EXPLANATION\n",
        "    \"Explain what machine learning is to someone who has never used a computer. Use analogies and avoid technical jargon.\",\n",
        "\n",
        "    # 9. MULTI-STEP INSTRUCTIONS\n",
        "    \"Help me plan a healthy dinner for 4 people with a budget of $25. List ingredients with estimated costs, then provide simple cooking instructions.\",\n",
        "\n",
        "    # 10. CRITICAL ANALYSIS\n",
        "    \"What are the pros and cons of working from home? Give exactly 4 pros and 4 cons, each in one sentence.\",\n",
        "\n",
        "    # 11. ROLEPLAY AND SPECIFIC CONTEXT\n",
        "    \"You are a customer service representative. A customer is complaining about a delayed delivery. Write a helpful response that shows empathy and offers solutions.\",\n",
        "\n",
        "    # 12. COMPLEX FORMAT INSTRUCTIONS\n",
        "    \"Create a simple study schedule for someone learning Spanish. Format it as a weekly table with specific activities for each day, 30 minutes per day maximum.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Run comparison\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "\n",
        "    # Generate response from base model (before fine-tuning)\n",
        "    base_output = base_model_pipe(instruction)[0][\"generated_text\"]\n",
        "    base_response = base_output.split(\"### Response:\\n\")[-1].strip()\n",
        "\n",
        "    # Generate response from fine-tuned model\n",
        "    fine_output = fine_tuned_pipe(instruction)[0][\"generated_text\"]\n",
        "    fine_response = fine_output.split(\"### Response:\\n\")[-1].strip()\n",
        "\n",
        "    # Print side-by-side comparison\n",
        "    print(f\"--- EXAMPLE {i} ---\")\n",
        "    print(f\"üî∏ PROMPT:\\n{prompt}\\n\")\n",
        "    print(f\"üîπ BASE MODEL RESPONSE:\\n{base_response}\\n\")\n",
        "    print(f\"üîπ FINE-TUNED MODEL RESPONSE:\\n{fine_response}\")\n",
        "    print(\"-\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJqZbOv1rjXV"
      },
      "source": [
        "#Conclusion\n",
        "In this notebook, we have successfully:\n",
        "\n",
        "* Loaded an open-source LLM (TinyLlama-1.1B) in a memory-efficient 4-bit format (QLoRA).\n",
        "* Loaded a dataset for instruction fine-tuning (guanaco-llama2-1k).\n",
        "* Established a baseline for the model's performance.\n",
        "* Configured LoRA to train only a small fraction of the model's parameters.\n",
        "* Used the high-level `SFTTrainer` to perform the fine-tuning process with just a few lines of code.\n",
        "* Verified  the fine-tuned model for instruction-following.\n",
        "\n",
        "From here, you can experiment with different models, datasets, and training hyperparameters,"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}