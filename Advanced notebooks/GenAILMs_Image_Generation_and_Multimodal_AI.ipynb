{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ],
      "metadata": {
        "id": "e15tQvJVIQyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Generation and Multimodal LLMs\n",
        "\n",
        "The goal of this notebook is to bridge the gap between the world of text and the world of images and multimodal understanding.\n",
        "\n",
        "We will cover:\n",
        "* **Image Generation with Diffusion Models**: We'll uncover how we can generate novel images from text prompts using the popular Stable Diffusion model.\n",
        "* **Specialized Multimodal AI**: We'll interact with a Visual Question Answering (VQA) model, a \"specialist\" trained for a single task: answering direct questions about an image.\n",
        "* **Multimodal LLMs**: We'll explore the distinction between specialists and powerful \"generalist\" models like those from Google (Gemini) or accessible via Groq API (LLaVA). We will see how to interact with them  tools like LangChain.\n",
        "\n"
      ],
      "metadata": {
        "id": "AqGhnUKSIVEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Environment\n",
        "\n",
        "First, let's install the necessary libraries from Hugging Face. We'll need diffusers for image generation and transformers for the multimodal pipeline. We also install accelerate to ensure efficient model loading.\n",
        "\n",
        "- diffusers: A library by Hugging Face that provides state-of-the-art pretrained diffusion models and makes it incredibly easy to use them.\n",
        "\n",
        "- transformers: The go-to library for all things related to Transformer models. We'll use it for our Visual Question Answering pipeline.\n",
        "\n",
        "- torch: The underlying deep learning framework.\n",
        "\n",
        "- accelerate: A library that simplifies running PyTorch code on any infrastructure (CPU, GPU, etc.)."
      ],
      "metadata": {
        "id": "_PX_P6ulvIPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This command installs the necessary Python libraries for our notebook.\n",
        "# - 'diffusers' is Hugging Face's library for diffusion models (like Stable Diffusion).\n",
        "# - 'transformers' is the core Hugging Face library for models like BERT, GPT, and our VQA model.\n",
        "# - 'accelerate' helps to load and run models efficiently, especially on GPUs.\n",
        "# - 'langchain' and 'langchain-groq'\n",
        "# The '-q' flag stands for \"quiet\", which means it will install without too much output.\n",
        "\n",
        "!pip install diffusers transformers accelerate torch langchain langchain-groq -q\n",
        "\n",
        "print(\"‚úÖ Libraries installed successfully!\")"
      ],
      "metadata": {
        "id": "tG8wRdRXvWff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us also check for a GPU, needed to speed up the generation of images."
      ],
      "metadata": {
        "id": "RDtOeh-2uhxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n",
        "    print(\"If you are using Google Colab, please go to 'Runtime' > 'Change runtime type' and select 'GPU' as the hardware accelerator.\")"
      ],
      "metadata": {
        "id": "ERnpPxRItoS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Groq to run multimodal LLMs. Make sure you have your API key ready."
      ],
      "metadata": {
        "id": "eera3lO7wBHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "#Using google.colab secrets\n",
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "if not api_key:\n",
        "    print(\"üõë Groq API Key not found. Please make sure to set it up.\")\n",
        "else:\n",
        "    print(\"‚úÖ Groq API Key configured.\")"
      ],
      "metadata": {
        "id": "AXfU8kLlMl9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Image Generation with Diffusion Models\n",
        "If LLMs like GPT-3 can generate coherent text, what is the equivalent for images? The current answer is largely \"Diffusion Models\". Stable Diffusion, DALL-E 2, and Midjourney are all based on this architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "JU2DDGCpxC6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Core Idea: What are Diffusion Models?\n",
        "Imagine you have a clear image. You start adding a tiny amount of noise to it, step by step, until all you have is pure static. This is the **forward process**. It's easy and mathematically defined.\n",
        "\n",
        "Now, what if you could learn to reverse this process? What if you could train a neural network to take the noisy static and, step by step, remove the noise until a clear image emerges? That's the **reverse process**, and it's the core of how diffusion models work.\n",
        "\n",
        "The model doesn't just remove random noise; it's **guided by your text prompt**. This guidance is achieved through a mechanism similar to the attention you know from Transformers, where the model \"pays attention\" to the words in your prompt to denoise the image in a way that matches the text.\n",
        "\n",
        "\n",
        "For a deeper understanding, check  [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/) post by Jay Alammar.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "6jZza4_TxPiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating immages with diffusers\n",
        "\n",
        "* We will use the *StableDiffusionPipeline* from the  Hugging Face diffusers library. This pipeline handles all the complexity for us.\n",
        "\n",
        "* We'll load a pre-trained model from the Hugging Face Hub. `stable-diffusion-v1-5`.\n",
        "\n",
        "* Finally, we will use the StableDiffusionPipeline from the diffusers library.\n"
      ],
      "metadata": {
        "id": "0AyM2SKYuLJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# --- 1. Setup the Pipeline ---\n",
        "# We're loading a pre-trained Stable Diffusion model.\n",
        "# \"runwayml/stable-diffusion-v1-5\" is the identifier of the model on the Hugging Face Hub.\n",
        "# torch.float16 is used for memory efficiency, which is helpful in Colab.\n",
        "# This model will be downloaded from the hub, which might take a minute.\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Move the pipeline to the GPU for faster inference.\n",
        "# If no GPU is available, this will default to CPU (much slower).\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "print(\"‚úÖ Stable Diffusion pipeline loaded successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x5H-i0YxtWcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the pipeline to generate the image with a prompt. Feel free to change the prompt and run the cell again to create your own images!"
      ],
      "metadata": {
        "id": "RCXUMCXHuz-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Define the Prompt and Generate ---\n",
        "# The prompt is the text that guides the image generation.\n",
        "# Think of it like the input you give to ChatGPT.\n",
        "prompt = \"A high-quality photograph of an astronaut riding a horse on Mars\"\n",
        "\n",
        "# We run the pipeline with our prompt.\n",
        "# The pipeline returns an object containing the generated images.\n",
        "# We access the first (and only) image with .images[0]\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "# --- 3. Display the Image ---\n",
        "print(\"\\nGenerated Image for prompt: '{}'\".format(prompt))\n",
        "display(image) # 'display()' is a handy function in Colab/Jupyter to show images."
      ],
      "metadata": {
        "id": "Rkfod6WQuxFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Process  \n",
        "We are not going to train a model, as this requires massive datasets (like LAION-5B, with 5 billion image-text pairs) and huge computational resources. However, it's crucial to understand the process.\n",
        "\n",
        "1. **Get a Dataset**: You need a vast number of images with corresponding text descriptions.\n",
        "2. **The Forward Process**: Take an image from the dataset. Add a random amount of noise to it. You now have a `(noisy_image, text_description)` pair.\n",
        "3. **The Model's Goal**: The core of the model is typically a U-Net architecture (common in image segmentation). You feed this model the `noisy_image` and the `text_description`. The model's job is to predict the noise that was added to the image.\n",
        "4. **Calculate Loss**: You compare the noise predicted by the model with the actual noise you added. The difference is the loss.\n",
        "5. **Update Weights**: You use backpropagation to update the model's weights to minimize this loss, just like in any other neural network.\n",
        "\n",
        "By repeating this process billions of times, the model becomes incredibly good at predicting and removing noise, guided by a text prompt.\n",
        "\n",
        "**Fine-Tuning**: What if you want to teach the model a new style or a specific object (like your face)? You don't need to train from scratch. You can **fine-tune** it. This involves continuing the training process on a small, specialized dataset (e.g., 15-20 images of the new object/style) for a much shorter time. This adjusts the model's weights to become an expert in your specific concept."
      ],
      "metadata": {
        "id": "pPKFsnv6zijv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: Multimodal (Specific) AI\n",
        "Multimodal AI refers to models that can process and understand information from multiple modalities (types of data) at once, like text, images, audio, etc.  "
      ],
      "metadata": {
        "id": "-kjNdPY7voii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Core Idea\n",
        "How does a model \"see\" an image? It uses a Vision Encoder (often a **Vision Transformer**, or ViT) to convert the image's pixels into a meaningful vector representation (an embedding). This is analogous to how BERT's encoder turns a sentence into a set of embeddings.\n",
        "\n",
        "For a task like **Visual Question Answering** (VQA), the typical process is as follows:\n",
        "\n",
        "- The image is converted into image embeddings by a vision encoder.\n",
        "\n",
        "- The question (text) is converted into text embeddings by a language encoder.\n",
        "\n",
        "- These two sets of embeddings are fused together and processed by a multimodal fusion layer, which learns the relationships between them.\n",
        "\n",
        "- Finally, a decoder (or a classification head) generates the answer based on this fused representation.\n",
        "\n",
        "\n",
        "You can learn more about the ViT architecture in the original paper by  Dosovitskiy et al. (2020) https://arxiv.org/abs/2010.11929  In summary, it is a BERT-like encoder-only Transformer. The self-supervision is performed learning the *masked patch prediction* task and the special token <CLS> allow the model to compress all information relevant for predicting the image label into one vector."
      ],
      "metadata": {
        "id": "SL_jnL181P-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question Answering (VQA)  \n",
        "\n",
        "\n",
        "We'll use a pipeline from the Hugging Face transformers library‚Äîa high-level abstraction that simplifies using models for specific tasks. By specifying the task as visual-question-answering without choosing a particular model, the pipeline will automatically download and load a default pre-trained model suited for this task.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "gZ17UpuO2ASw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- 1. Load Image ---\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "print(\"‚úÖ Image and VQA pipeline loaded successfully.\")\n",
        "display(image)\n",
        "\n",
        "# --- 2. Setup Pipeline and Ask Question ---\n",
        "vqa_pipeline = pipeline(\"visual-question-answering\")\n",
        "\n",
        "question = \"How many cats are there?\"\n",
        "result = vqa_pipeline(image=image, question=question)\n",
        "\n",
        "# --- 3. Display Result ---\n",
        "print(\"\\nQuestion: '{}'\".format(question))\n",
        "print(\"Answer:\", result[0]['answer'])\n",
        "print(\"Confidence Score:\", result[0]['score'])"
      ],
      "metadata": {
        "id": "yjdrHbVRlY1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Training Process  \n",
        "\n",
        "\n",
        "Many powerful multimodal models, such as [CLIP](https://arxiv.org/abs/2103.00020) (Contrastive Language‚ÄìImage Pretraining), are first pre-trained on large datasets of image-text pairs using a self-supervised objective. CLIP, developed by OpenAI in 2021, popularized the use of **Contrastive Learning** to align visual and textual representations:\n",
        "\n",
        "* The model is given a batch of images and their corresponding captions.\n",
        "* It creates embeddings for all images and all texts.\n",
        "* The model's goal is to learn to pull the embedding of a correct `(image, text)` pair closer together in the embedding space, while pushing incorrect pairs further apart.\n",
        "* This forces the model to learn a shared representation space where \"a photo of a dog\" (text) is semantically close to an actual picture of a dog (image).\n",
        "\n",
        "\n",
        "After pretraning the model with Contrastive Learning, this can be fine-tuned for a specific task like VQA.\n",
        "\n",
        "1. **Get a Task-Specific Dataset**: For VQA, you need a dataset with images, questions, and ground-truth answers (e.g., the VQAv2 dataset).\n",
        "\n",
        "2. **Add a \"Head\"**: You take a pre-trained multimodal model (like one trained with CLIP) and add a new final layer, called a \"question-answering head\". This head is often a simple linear layer that will output the final answer.\n",
        "\n",
        "3. **Train on the Task**: You feed the model an `(image, question)` pair and train it to output the correct `answer`. The loss is calculated based on how far the model's prediction is from the ground-truth answer.\n",
        "\n",
        "4. **Update Weights**: The error is backpropagated, but you might only update the weights of the new \"head\", or you might \"unfreeze\" the whole model and let all the weights be updated slightly. This second approach is full fine-tuning.\n",
        "\n",
        "This two-step process (pre-training on a general task, fine-tuning on a specific task) is very similar to how models like BERT are first pre-trained on Masked Language Modeling and then fine-tuned for tasks like text classification or question answering.\n",
        "\n",
        "**Note**: This is a general framework and starting point. In practice, training multimodal AI can vary widely depending on the model architecture, dataset availability, and specific application. Not all multimodal systems follow this exact process.\n",
        "\n",
        "**Note 2**: Contrastive Learning can be seen as a generalization or conceptual evolution inspired by ideas similar to those used in Word2Vec, but applied to multimodal tasks and using modern deep learning techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVIrYeK92z6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Multimodal (Generalist) LLMs\n",
        "\n",
        "The VQA model was a specialist. Now, let's discuss the general-purpose multimodal models that power today's most advanced AI assistants.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dAgwxnfbtOMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Key Difference: Specialist vs. Generalist\n",
        "\n",
        "Previously, we have used a specialized model for Visual Question Answering. It's excellent at its one job. However, the current cutting edge of AI is in large, general-purpose multimodal models. Let's clarify the distinction.\n",
        "\n",
        "**Specialist Multimodal Models** (like the `vilt-b32-finetuned-vqa` we used):\n",
        "- **Analogy**: Think of this as a highly-trained radiologist. They can look at an X-ray (the image) and answer very specific questions (\"Is there a fracture?\"). But you wouldn't ask them to write a poem about the X-ray or tell you a story about the patient.\n",
        "- **Function**: Designed and fine-tuned for a single task. The output is constrained, often a single word or a short phrase from a predefined set of possible answers.\n",
        "- **Architecture**: They typically use two separate encoders (one for vision, one for text) and a \"fusion\" module to combine their knowledge and produce an answer.\n",
        "\n",
        "**Multimodal LLMs** (like Google's Gemini, OpenAI's GPT-4o, LLaVA):\n",
        "- **Analogy**: This is a brilliant general practitioner with expertise in nearly every field. You can show them the X-ray and not only ask \"Is there a fracture?\", but also \"Can you explain this to me in simple terms?\", \"What are the likely next steps for treatment?\", or \"Write a short, hopeful note to the patient based on this image.\"\n",
        "- **Function**: They are fundamentally LLMs that have been augmented to accept images (and other modalities) as part of their input. Their output is flexible, generative text. They can reason, describe, create, and converse about the image.\n",
        "- **Architecture**: The core is the LLM. The image is processed by a vision encoder into a series of embeddings, which are then fed into the LLM as if they were special \"word\" tokens. The LLM sees a sequence of both image tokens and text tokens and generates a text response.\n",
        "\n",
        "This table summarizes the main differences:\n",
        "\n",
        "\n",
        "| Aspect | Specialist Models (e.g., CLIP, ViLT-VQA) | Generalist Multimodal LLMs (e.g., Gemini, GPT-4o) |\n",
        "|--------|------------------------------------------|---------------------------------------------------|\n",
        "| **Analogy** | Highly-trained radiologist who can analyze X-rays but won't write poetry about them | Brilliant general practitioner with expertise across fields |\n",
        "| **Primary Function** | Single, specific task (classification, VQA, retrieval) | General-purpose reasoning, conversation, and content generation |\n",
        "| **Output Type** | Constrained (single word, short phrase, classification score) | Flexible, generative text of any length |\n",
        "| **Architecture** | Dual encoders + fusion module | LLM core + vision encoder + projection layer |\n",
        "| **Training Focus** | Task-specific fine-tuning | Instruction-following and general reasoning |\n",
        "| **Use Cases** | Image classification, similarity search, specific VQA | Conversational AI, complex reasoning, creative tasks |\n"
      ],
      "metadata": {
        "id": "9NJJioGNjzyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## How Generalists Work: The LLM at the Core\n",
        "\n",
        "The breakthrough of generalist models is that they teach a Large Language Model‚Äîan expert in text, grammar, and reasoning‚Äîa new skill: how to read images. The architecture to achieve this generally involves two main components connected by a \"bridge.\"\n",
        "\n",
        "- **The Vision Encoder**: This is the \"eye\" of the system. Its only job is to look at an image and convert its pixels into a meaningful numerical format (embeddings).\n",
        "\n",
        "- **The Large Language Model (LLM)**: This is the \"brain.\" It's the same kind of powerful language model you're already familiar with (like GPT-3, Gemini, Llama, Mistral, etc.). It receives the numerical representation of the image and the user's text prompt to perform reasoning and generate a textual response.\n",
        "\n",
        "The general data flow looks like this:\n",
        "\n",
        "Image ‚Üí Vision Encoder ‚Üí Sequence of Image Embeddings ‚Üí Adapter/Bridge ‚Üí LLM Input + Text Prompt Input ‚Üí LLM ‚Üí Text Output\n",
        "\n",
        "Therefore, multimodal LLMs  treat images as just another part of the input language. Imagine the input to an LLM: `[token1, token2, token3, ...]`  For a multimodal LLM, the input becomes: `[img_tok1, img_tok2, ..., img_tok_N,  text_tok1, text_tok2, ...]` The model's reasoning and generation capabilities, which you are familiar with from text-only LLMs, can now be applied to the concepts and objects present in the image tokens. This is what allows for the incredible flexibility we see in multimodal models such as ChatGPT and Gemini.\n",
        "\n"
      ],
      "metadata": {
        "id": "jQEUSPRcj6Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Key: How \"Image Tokens\" are Created\n",
        "\n",
        "The term \"image token\" is a useful abstraction. In reality, the model creates a sequence of embeddings (vectors) that are dimensionally compatible with the LLM's text embeddings. This is done through a process, usually involving a **Vision Transformer (ViT)** (see Part 2).\n",
        "\n",
        "Here is the step-by-step process of creating Image Tokens:\n",
        "\n",
        "* Step 1: **Image Patching**. The Vision Encoder doesn't look at the whole image at once. Instead, it slices the image into a grid of smaller, fixed-size squares called patches. Think of it like cutting a photograph into a mosaic of puzzle pieces. This allows the model to process the image as a sequence, similar to how it processes a sequence of words.\n",
        "\n",
        "* Step 2: **Embedding the Patches**. Each patch is then \"embedded.\" It's fed through a neural network that converts the raw pixels of that small square into a numerical vector, or an embedding. This vector represents the visual content of that specific patch in a high-dimensional space. At this stage, you have a sequence of vectors, one for each piece of the original image.\n",
        "\n",
        "* Step 3: The **Projection Layer** (The \"Bridge\").  This is the most critical step for making the two systems compatible. The embeddings produced by the Vision Encoder are in a \"visual space,\" which the LLM doesn't understand. The LLM understands a \"language space.\"  A small, trainable neural network, often called a projection layer or an adapter, acts as a translator. Its sole purpose is to take the sequence of image patch embeddings and convert them into a sequence of embeddings that live in the exact same dimensional space as the LLM's word embeddings.\n",
        "\n",
        "After this step, the image has been transformed into a sequence of vectors that the LLM can read and process just as if they were embeddings from a sentence. The LLM can now apply its attention mechanisms across both the text tokens from the prompt and these new \"image tokens.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OHc1mm6mk3fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The Fuel: Training Data and Process\n",
        "\n",
        "The model learns to connect vision and language through a two-stage training process using massive datasets.\n",
        "\n",
        "* Pre-training (Learning to See). The model is first trained on enormous datasets of paired images and text descriptions. A famous example is the LAION dataset, which contains billions of image-alt-text pairs scraped from the web. In this stage, the model's goal is to align the two modalities. It learns that the visual information from an image of a dog (processed by the Vision Encoder) should correspond to the semantic meaning of the words \"a photo of a dog\" (processed by the text embedder).\n",
        "\n",
        "* Instruction Fine-Tuning (Learning to Obey). After pre-training, the model knows what's in an image, but it isn't yet a helpful assistant. The second stage uses curated, high-quality datasets of (image, instruction, desired_output) triplets. This teaches the model to follow commands. For example:\n",
        "\n",
        " - **Image**: A picture of a birthday party.\n",
        " - **Instruction**: \"Describe what is happening in this image.\"\n",
        " - **Desired Output**: \"This image shows a group of people celebrating a birthday party. There is a cake with candles on the table...\"\n",
        "\n",
        "This fine-tuning stage is what turns a descriptive model into a conversational and reasoning agent.\n",
        "\n",
        "**Note**: This is a common ‚Äústandard recipe‚Äù for multimodal models, but the exact process can vary depending on the architecture, available data, and training goals.\n",
        "\n"
      ],
      "metadata": {
        "id": "XGg99aIIk_h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-World Examples\n",
        "\n",
        "Several prominent models use this exact architecture:\n",
        "\n",
        "Open-Source Models include:\n",
        "- **[LLaVA (Large Language and Vision Assistant)](https://arxiv.org/abs/2304.08485)**: This is a classic, open-source example from 2023. It explicitly uses a pre-trained Vision Encoder from CLIP (a ViT) and a simple projection layer (an MLP) to feed image features into an instruction-tuned LLM like Vicuna. Its architecture is a textbook implementation of the process described above.\n",
        "\n",
        "- **IDEFICS (by Hugging Face)**: This is another open model that builds on this principle. It's designed to handle interleaved image and text sequences, making it effective for tasks like visual storytelling or analyzing documents with multiple images. It still uses a vision encoder and an adapter to bridge the modalities.\n",
        "\n",
        "Commercial Models include **GPT-4o & Google's Gemini**. These are state-of-the-art, closed-source models that follow the same fundamental paradigm but on a much larger scale. They use highly advanced, proprietary vision encoders and LLMs, and are trained on vast, private datasets. However, the core principle of patching an image, embedding the patches, and projecting them into the LLM's language space remains the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "_8AeloE1lDEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing Multimodal GenAI Programmatically\n",
        "\n",
        "You typically access these massive models through an API. LangChain provides a universal interface to interact with many different LLM providers, making your code cleaner and more portable.\n",
        "\n",
        "Let's look at how you would use `langchain_groq` to interact with `llama-4-maverick-17b-128e-instruct`, a powerful open-source multimodal LLM available through Groq's fast inference engine.\n",
        "\n",
        "The model will be instructed to analyze the provided image and respond exclusively with a JSON object containing two specific fields:\n",
        "* \"description\": A detailed, accurate description of what is depicted in the image.\n",
        "* \"funny_story\": A brief, humorous story inspired by the content of the image.\n",
        "\n",
        "The image is sent as a `Base64` string, which embeds the actual image data directly into the body of the API request instead of simply providing a URL.A Base64 string is a way of encoding binary data (like an image file) into ASCII text characters."
      ],
      "metadata": {
        "id": "IN1PgVdOlFDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "from PIL import Image\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from IPython.display import display\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "# 1. Download the image from the URL and encode it in base64\n",
        "#To explore more images from the COCO dataset, you can visit the official COCO Dataset website. https://cocodataset.org/\n",
        "url = \"http://images.cocodataset.org/val2017/000000397133.jpg\" # man in a kitchen\n",
        "#url = \"http://images.cocodataset.org/val2017/000000039769.jpg\" #cats\n",
        "response = requests.get(url)\n",
        "\n",
        "# Show image on screen\n",
        "image = Image.open(BytesIO(response.content))\n",
        "display(image)\n",
        "\n",
        "#When you send an image as a Base64 string, you are embedding the actual image data directly into the body of your API request.\n",
        "# The model receives the image pixels and the text prompt in a single, self-contained package.\n",
        "image_b64 = base64.b64encode(response.content).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "# 2. Prepare the multimodal message content\n",
        "# The content is a list where each item is a dictionary representing a part of the message:\n",
        "# - The first part is a text instruction for the model.\n",
        "# - The second part is the image encoded as a data URL inside an object with the key \"image_url\".\n",
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": (\n",
        "                \"Please analyze the following image and respond ONLY with a JSON object \"\n",
        "                \"with two fields:\\n\"\n",
        "                \"1. \\\"description\\\": A detailed description of the image.\\n\"\n",
        "                \"2. \\\"funny_story\\\": A short, funny story inspired by the image.\\n\"\n",
        "                \"Do not include any additional text.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            # The 'image_url' field is an object with a mandatory 'url' key,\n",
        "            # which must contain the image data as a base64-encoded data URL.\n",
        "            \"image_url\": {\n",
        "                \"url\": f\"data:image/jpeg;base64,{image_b64}\"\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Invoke the multimodal model on Groq's API\n",
        "# Replace 'api_key' with your actual Groq API key\n",
        "chat = ChatGroq(model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\", groq_api_key=api_key)\n",
        "response = chat.invoke([message])\n",
        "\n",
        "# 4. Print the raw JSON response from the model\n",
        "print(response.content)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CosJY8_-N5w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusions and Next Steps\n",
        "\n",
        "We have:\n",
        "* Generated an image from text using a diffusion model.\n",
        "* Used a specialist AI to answer a direct question about an image.\n",
        "* Understood the architecture of generalist multimodal LLMs and how to interact with them programmatically.\n",
        "\n",
        "The skills developed for  LLMs ‚Äîunderstanding embeddings, attention, and the pre-training/fine-tuning paradigm‚Äî are the bedrock for multimodal GenAI.\n",
        "\n",
        "Next steps\n",
        "\n",
        "- Explore other HuggingFace pipelines: Try Image-to-Image in diffusers or Image-Captioning in transformers.\n",
        "- Experiment with different prompts and models from GenAI providers like Groq, OpenAI, or Google.\n",
        "-  Read the original papers for models like [Vision Transformers](https://arxiv.org/abs/2010.11929) (2020), [CLIP](https://arxiv.org/abs/2103.00020) (2021), and [LLaVA](https://arxiv.org/abs/2304.08485)(2023) to understand their innovations firsthand. Also [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/) post by Jay Alammar.\n"
      ],
      "metadata": {
        "id": "w31k9U4EJjVw"
      }
    }
  ]
}