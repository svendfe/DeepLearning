{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU3ZOodfZakj"
      },
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geEIF9DSZg_I"
      },
      "source": [
        "# Advanced LangChain Workflows: Summarization, RAG, Memory, and Web Chatbots\n",
        "\n",
        "In this notebook, we will build more complex generative AI systems, assuming a basic understanding of LangChain.\n",
        "\n",
        "Learning Objectives:\n",
        "\n",
        "*  Summarize large documents that exceed the context window size\n",
        "\n",
        "* Build a Retrieval-Augmented Generation (RAG) system to answer questions based on multiple documents.\n",
        "\n",
        "* Manage conversation history with structured memory using LangGraph.\n",
        "\n",
        "* Build an interactive web-based chatbot with Gradio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxVL3xHsayPT"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "First, let's install the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kXyKfbkxcerR"
      },
      "outputs": [],
      "source": [
        "# Install all necessary libraries for the notebook\n",
        "!pip -q install langchain langchain-community langchain_huggingface langchain_groq langgraph  ddgs faiss-cpu beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: langchain\n",
            "Version: 1.0.3\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://docs.langchain.com/\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages\n",
            "Requires: langchain-core, langgraph, pydantic\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# This forces the shell (!) to use the kernel's Python executable (sys.executable)\n",
        "!{sys.executable} -m pip show langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkv9WTxdz8Oq"
      },
      "source": [
        "Configure the Groq API key.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WeUDHQHDbIrm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Groq API Key configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#Using google.colab secrets\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"üõë Groq API Key not found. Please make sure to set it up.\")\n",
        "else:\n",
        "    print(\"‚úÖ Groq API Key configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMjiNE9nqei"
      },
      "source": [
        "#Summarizing Long Documents with LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-gHUYOY1HNq"
      },
      "source": [
        "##Testing the model context window\n",
        "\n",
        "Here we will check if our model can summarize a long text. [Groq models](https://console.groq.com/docs/models) specify the size of the context window for each model.\n",
        "\n",
        "We will use LangChain WebBaseLoader to download the wikipedia page of the History of Artificial Intelligence.\n",
        "\n",
        "‚ö†Ô∏è Alert: Context overflow wanted. Error \"Request too large for model `meta-llama/llama-4-scout-17b-16e-instruct\". If not, increase the text_excerpt length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "STdS0Wlj17pb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text has 142018 characters.\n",
            "Text excerpt has 140535 characters.\n",
            "\n",
            "--- Cleaned Text Sample ---\n",
            "\n",
            "Quantum\n",
            "By country\n",
            "Bulgaria\n",
            "Eastern Bloc\n",
            "Poland\n",
            "Romania\n",
            "South America\n",
            "Soviet Union\n",
            "Yugoslavia\n",
            "Timeline of computing\n",
            "before 1950\n",
            "1950‚Äì1979\n",
            "1980‚Äì1989\n",
            "1990‚Äì1999\n",
            "2000‚Äì2009\n",
            "2010‚Äì2019\n",
            "2020‚Äìpresent\n",
            "more timelines ...\n",
            "Glossary of computer science\n",
            "Categoryvte\n",
            "The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the prese\n",
            "Groq LLM loaded.\n",
            "\n",
            "--- Invoking the LLM for summarizing (error expected) ---\n"
          ]
        },
        {
          "ename": "APIStatusError",
          "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01k5pqw61sf97t06kkar2fk64p` service tier `on_demand` on tokens per minute (TPM): Limit 30000, Requested 34842, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAPIStatusError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 6. Summarize\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Invoking the LLM for summarizing (error expected) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarize the following text:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext_excerpt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 7. Show summary\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- LLM Summary ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:382\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    370\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    375\u001b[39m     **kwargs: Any,\n\u001b[32m    376\u001b[39m ) -> AIMessage:\n\u001b[32m    377\u001b[39m     config = ensure_config(config)\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    379\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    380\u001b[39m         cast(\n\u001b[32m    381\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    392\u001b[39m         ).message,\n\u001b[32m    393\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1084\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m     **kwargs: Any,\n\u001b[32m   1089\u001b[39m ) -> LLMResult:\n\u001b[32m   1090\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:906\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    904\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    905\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m         )\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1195\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1193\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1194\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1199\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_groq/chat_models.py:544\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    540\u001b[39m params = {\n\u001b[32m    541\u001b[39m     **params,\n\u001b[32m    542\u001b[39m     **kwargs,\n\u001b[32m    543\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/groq/resources/chat/completions.py:464\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    245\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    246\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    303\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    304\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    307\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/groq/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mAPIStatusError\u001b[39m: Error code: 413 - {'error': {'message': 'Request too large for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01k5pqw61sf97t06kkar2fk64p` service tier `on_demand` on tokens per minute (TPM): Limit 30000, Requested 34842, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ],
      "source": [
        "from langchain_classic.document_loaders import WebBaseLoader\n",
        "from langchain_classic.prompts import PromptTemplate\n",
        "import re\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# 1. Load article\n",
        "url = \"https://en.wikipedia.org/wiki/History_of_artificial_intelligence\"\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. Combine content\n",
        "raw_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "print(f\"Original text has {len(raw_text)} characters.\")\n",
        "\n",
        "# 3. Clean up excessive line breaks\n",
        "clean_text = re.sub(r'\\n{2,}', '\\n', raw_text)\n",
        "# Optional: trim each line\n",
        "clean_text = \"\\n\".join([line.strip() for line in clean_text.splitlines() if line.strip()])\n",
        "text_excerpt = clean_text\n",
        "print(f\"Text excerpt has {len(text_excerpt)} characters.\")\n",
        "\n",
        "# 4. Show a preview\n",
        "print(\"\\n--- Cleaned Text Sample ---\")\n",
        "print(text_excerpt[5000:5500])\n",
        "\n",
        "# 5. Load LLM\n",
        "llm = ChatGroq(model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\", temperature=0.2, groq_api_key=api_key)\n",
        "print(\"Groq LLM loaded.\")\n",
        "\n",
        "# 6. Summarize\n",
        "print(\"\\n--- Invoking the LLM for summarizing (error expected) ---\")\n",
        "response = llm.invoke(f\"Summarize the following text:\\n{text_excerpt}\")\n",
        "\n",
        "# 7. Show summary\n",
        "print(\"\\n--- LLM Summary ---\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZDJbfj24xHx"
      },
      "source": [
        "##Map-reduce approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WSk-UJdnvwC"
      },
      "source": [
        "When dealing with text that is too long to fit into a single API call, a common strategy is to use a `Map-Reduce` approach. This involves:\n",
        "\n",
        "* Splitting the large document into smaller, more manageable chunks.\n",
        "\n",
        "* Mapping by generating a summary for each individual chunk.\n",
        "\n",
        "* Reducing the results by combining all the smaller summaries into one final, concise summary.\n",
        "\n",
        "`load_summarize_chain`is a function provided by LangChain that creates a summarization pipeline. It accepts an LLM and a `chain_type` parameter.  `chain_type=\"map_reduce\"` indicates to summarize each chunk individually (\"map\") and then combine the results into a final summary (\"reduce\").\n",
        "\n",
        "`RecursiveCharacterTextSplitter` will be used to break long documents into manageable chunks. `RecursiveCharacterTextSplitter` does this intelligently by attempting to split at natural boundaries (such as paragraphs or sentences), while also allowing for **overlap between chunks** to preserve context across segments. For example, with a `chunk_size` of 1000 characters and a `chunk_overlap` of 200, the splitter ensures that each new chunk shares 200 characters with the previous one.  \n",
        "\n",
        "\n",
        "‚ö†Ô∏è Alert: the code may exceeds token-per-minute (TPM) limit. This is not a context window overflow. Groq's service tier (on_demand) restricts the number of tokens you can send per minute. Check [Groq Rate Limits](https://console.groq.com/docs/rate-limits). For instance, meta-llama/llama-4-scout-17b-16e-instruct has 30K TPM vs llama-3.1-8b-instant with 6K TPM. Note that 1 token ‚âà 3.5‚Äì4 average English characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (2.3.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2025.11.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m153.5 kB/s\u001b[0m  \u001b[33m0:01:17\u001b[0mm0:00:02\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.11.3-cp311-cp311-macosx_11_0_arm64.whl (288 kB)\n",
            "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
            "Installing collected packages: safetensors, regex, transformers\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed regex-2025.11.3 safetensors-0.6.2 transformers-4.57.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FulcAPZZnvGv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq LLM loaded successfully.\n",
            "The text has 140535 characters.\n",
            "The text has been divided into 17 documents.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     40\u001b[39m map_reduce_chain = load_summarize_chain(\n\u001b[32m     41\u001b[39m     llm=llm,\n\u001b[32m     42\u001b[39m     chain_type=\u001b[33m\"\u001b[39m\u001b[33mmap_reduce\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Run the chain on our documents!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m result = \u001b[43mmap_reduce_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_documents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Step 5: View the Final Result\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Finally, we print the generated summary.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Final Generated Summary ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_classic/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_classic/chains/combine_documents/base.py:147\u001b[39m, in \u001b[36mBaseCombineDocumentsChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[32m    146\u001b[39m other_keys = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[38;5;28mself\u001b[39m.input_key}\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m output, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m extra_return_dict[\u001b[38;5;28mself\u001b[39m.output_key] = output\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_classic/chains/combine_documents/map_reduce.py:250\u001b[39m, in \u001b[36mMapReduceDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, token_max, callbacks, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m question_result_key = \u001b[38;5;28mself\u001b[39m.llm_chain.output_key\n\u001b[32m    245\u001b[39m result_docs = [\n\u001b[32m    246\u001b[39m     Document(page_content=r[question_result_key], metadata=docs[i].metadata)\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[32m    249\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m result, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduce_documents_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_intermediate_steps:\n\u001b[32m    257\u001b[39m     intermediate_steps = [r[question_result_key] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m map_results]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_classic/chains/combine_documents/reduce.py:253\u001b[39m, in \u001b[36mReduceDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, token_max, callbacks, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcombine_docs\u001b[39m(\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    233\u001b[39m     docs: \u001b[38;5;28mlist\u001b[39m[Document],\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     **kwargs: Any,\n\u001b[32m    237\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    238\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[32m    239\u001b[39m \n\u001b[32m    240\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    251\u001b[39m \u001b[33;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     result_docs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collapse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.combine_documents_chain.combine_docs(\n\u001b[32m    260\u001b[39m         docs=result_docs,\n\u001b[32m    261\u001b[39m         callbacks=callbacks,\n\u001b[32m    262\u001b[39m         **kwargs,\n\u001b[32m    263\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_classic/chains/combine_documents/reduce.py:308\u001b[39m, in \u001b[36mReduceDocumentsChain._collapse\u001b[39m\u001b[34m(self, docs, token_max, callbacks, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m result_docs = docs\n\u001b[32m    307\u001b[39m length_func = \u001b[38;5;28mself\u001b[39m.combine_documents_chain.prompt_length\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m num_tokens = \u001b[43mlength_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_collapse_docs_func\u001b[39m(docs: \u001b[38;5;28mlist\u001b[39m[Document], **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._collapse_chain.run(\n\u001b[32m    312\u001b[39m         input_documents=docs,\n\u001b[32m    313\u001b[39m         callbacks=callbacks,\n\u001b[32m    314\u001b[39m         **kwargs,\n\u001b[32m    315\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_classic/chains/combine_documents/stuff.py:245\u001b[39m, in \u001b[36mStuffDocumentsChain.prompt_length\u001b[39m\u001b[34m(self, docs, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._get_inputs(docs, **kwargs)\n\u001b[32m    244\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.llm_chain.prompt.format(**inputs)\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_classic/chains/llm.py:422\u001b[39m, in \u001b[36mLLMChain._get_num_tokens\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_language_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/base.py:290\u001b[39m, in \u001b[36mBaseLanguageModel.get_num_tokens\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    279\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[32m    280\u001b[39m \n\u001b[32m    281\u001b[39m \u001b[33;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    288\u001b[39m \n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/base.py:276\u001b[39m, in \u001b[36mBaseLanguageModel.get_token_ids\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_get_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_get_token_ids(text)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/base.py:92\u001b[39m, in \u001b[36m_get_token_ids_default_method\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m tokenizer = \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.encode(text)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/langchain_core/language_models/base.py:84\u001b[39m, in \u001b[36mget_tokenizer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_TRANSFORMERS:\n\u001b[32m     79\u001b[39m     msg = (\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m GPT2TokenizerFast.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mImportError\u001b[39m: Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`."
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_classic.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Step 1: Loading a LLM model\n",
        "llm = ChatGroq(model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\", temperature=0.2, groq_api_key=api_key)\n",
        "print(\"Groq LLM loaded successfully.\")\n",
        "\n",
        "# Step 2: Prepare the Long Text\n",
        "long_text=text_excerpt\n",
        "\n",
        "print(f\"The text has {len(long_text)} characters.\")\n",
        "\n",
        "# Step 3: Split the Text into Chunks\n",
        "# We'll use RecursiveCharacterTextSplitter from LangChain. It's the recommended tool for this task as it intelligently splits text while trying to keep paragraphs and sentences intact.\n",
        "\n",
        "# Create an instance of the splitter\n",
        "# Fit the chunk_size to the context window of LLL, the larger and more permissible the better\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=9000,  # Maximum size of each chunk in characters\n",
        "    chunk_overlap=500, # Overlapping characters to maintain context between chunks\n",
        "    length_function=len # Function used to measure the length of each chunk (default: character count)\n",
        ")\n",
        "\n",
        "# Split the text. The output is a list of strings.\n",
        "text_chunks = text_splitter.split_text(long_text)\n",
        "\n",
        "# To use the LangChain chain, we convert the strings into Document objects.\n",
        "docs = [Document(page_content=t) for t in text_chunks]\n",
        "\n",
        "print(f\"The text has been divided into {len(docs)} documents.\")\n",
        "#print(\"First chunk:\", docs[0].page_content)\n",
        "\n",
        "# Step 4: Create and Execute the Map-Reduce Chain\n",
        "# LangChain makes this easy with the `load_summarize_chain` function. We specify the `map_reduce` or `refine` chain type.\n",
        "# LangChain will use default prompts, but we can also define custom prompts for more control. For this example, we'll use the optimized default prompts.\n",
        "# Create the chain, specifying the LLM and the strategy type.\n",
        "# `verbose=True` can show us the steps LangChain is performing behind the scenes.\n",
        "map_reduce_chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"map_reduce\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Run the chain on our documents!\n",
        "result = map_reduce_chain.invoke({\"input_documents\": docs})\n",
        "\n",
        "# Step 5: View the Final Result\n",
        "# Finally, we print the generated summary.\n",
        "print(\"\\n--- Final Generated Summary ---\\n\")\n",
        "print(result[\"output_text\"])\n",
        "\n",
        "# You have now successfully implemented a Map-Reduce summarization pipeline. The verbose output showed you how the LLMChain was first run on each chunk (the Map phase),\n",
        "# and then a different chain was used to combine those intermediate summaries (the Reduce phase) into the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXLF7UuL5HBu"
      },
      "source": [
        "##Refine approach\n",
        "Another powerful technique is the `Refine` method. This approach works differently:\n",
        "\n",
        "* It starts by generating an initial summary of the first chunk of text.\n",
        "\n",
        "* It then iteratively refines this summary by taking the next chunk and the existing summary, and asking the LLM to update the summary with the new information.\n",
        "\n",
        "* This process continues until all chunks have been processed.\n",
        "\n",
        "The `Refine` method is particularly effective when the final summary needs to be highly detailed and coherent, as it builds on previous context. However, because it is a sequential process, it is often slower than the Map-Reduce approach.\n",
        "\n",
        "In the previous code, you only need to change  `chain_type` parameter of the `load_summarize_chain` function that returns the summarization pipeline.\n",
        "\n",
        "Let us see if we get a better summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRsfiY4aKmeL"
      },
      "outputs": [],
      "source": [
        "# Steps 1-3: steps 1 to 3 equal to the map reduce code\n",
        "\n",
        "# Step 4: Create and execute a Refine summarization chain\n",
        "# The \"refine\" chain summarizes progressively by improving the summary with each new chunk\n",
        "refine_chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    verbose=False  # Set to True to see step-by-step execution\n",
        ")\n",
        "\n",
        "# Run the refine summarization chain\n",
        "result = refine_chain.invoke({\"input_documents\": docs})\n",
        "\n",
        "# Step 5: Output the final summary\n",
        "print(\"\\n--- Final Generated Summary (Refine Chain) ---\\n\")\n",
        "print(result[\"output_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSEkJvYJRFwo"
      },
      "source": [
        "## Customizing the summary\n",
        "You  can  customize both map_reduce and refine chains in LangChain by providing your own prompt templates. This is useful when you want more control over:\n",
        "* The tone, detail, or style of the summaries\n",
        "* Instructions for how to summarize\n",
        "* Contextual awareness in multi-stage summarization\n",
        "\n",
        "Let us repeat the refine summary... as a pirate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIr3Q7JdRfHr"
      },
      "outputs": [],
      "source": [
        "# Steps 1-3: steps 1 to 3 equal to the map reduce code\n",
        "\n",
        "initial_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Write an initial summary of the following content AS A PIRATE:\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"existing_answer\", \"text\"],\n",
        "    template=\"Here is the current summary:\\n\\n{existing_answer}\\n\\nRefine it using the additional text below AS A PIRATE:\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "refine_chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    question_prompt=initial_prompt,\n",
        "    refine_prompt=refine_prompt,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result = refine_chain.invoke({\"input_documents\": docs})\n",
        "\n",
        "print(\"\\n--- Final Generated Summary (Refine Chain) ---\\n\")\n",
        "print(result[\"output_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfqZq0SwVYCB"
      },
      "source": [
        "##Beyond Summarization\n",
        "LangChain offers a broader set of chain pipelines beyond `load_summarize_chain`, many of which support Map-Reduce, Refine, and other strategies to process long texts that exceed an LLM‚Äôs context window.\n",
        "\n",
        "These pipelines cover a wide range of use cases, including question answering (QA), retrieval-based generation, structured reasoning, and multi-prompt workflows.\n",
        "\n",
        "For example, `load_qa_chain` enables question answering over large document sets and supports several chain types:\n",
        "\n",
        "* \"stuff\" ‚Äì Concatenates all documents into a single prompt (best for small inputs)\n",
        "\n",
        "* \"map_reduce\" ‚Äì Answers each chunk independently, then summarizes the results\n",
        "\n",
        "* \"refine\" ‚Äì Generates an initial answer and iteratively refines it with new chunks\n",
        "\n",
        "* \"map_rerank\" ‚Äì Generates multiple answers per chunk, scores them, and selects the bes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wbxk7W4bkgP"
      },
      "source": [
        "#RAG: Answering Questions with External Documents\n",
        "\n",
        "In this section, we will build the a **Retrieval-Augmented Generation (RAG)** system. The goal is to enable a language model (LLM) to answer questions based on external documents ‚Äî especially useful when those documents contain up-to-date or domain-specific knowledge not included in the model‚Äôs pretraining.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QftHk3kFQ3Nc"
      },
      "source": [
        "##Testing the recent fatual knowledge of a LLM\n",
        "\n",
        "In this section, we will test how much our Language Model (LLM) knows about a recent or updated topic. This will help us understand the limitations of the model's internal knowledge when it comes to current events or newly emerged facts.\n",
        "\n",
        "We‚Äôll be using the `llama-3.1-8b-instant model`, which was pretrained before 2025. This means it should not have direct knowledge of events or information that occurred after its cutoff date.\n",
        "\n",
        "To ensure accurate and consistent results for factual queries, we‚Äôll also set a low temperature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6D9k28nMmh5"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "#Let us load a LLM from groq\n",
        "# Let us select a low temperature, good for tasks requiring consistency and precision as fact-based tasks\n",
        "llm_low_temp = ChatGroq(model_name=\"llama-3.1-8b-instant\",temperature=0.2, groq_api_key=api_key)\n",
        "\n",
        "response_NO_RAG = llm_low_temp.invoke(\"¬øCu√°ntos par√°metros tiene el modelo DeepSeek?\")\n",
        "print(\"\\n--- LLM Answer without RAG contex ---\")\n",
        "print(response_NO_RAG.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF5tw3b7TVay"
      },
      "source": [
        "## Document Loading and Splitting Pipeline  \n",
        "\n",
        "This code performs two main tasks:\n",
        "\n",
        "* It loads web content from specified URLs using LangChain‚Äôs `WebBaseLoader`.\n",
        "* It splits the content into smaller, manageable chunks using a `RecursiveCharacterTextSplitter`, making it suitable for LLM input.\n",
        "\n",
        "`WebBaseLoader` is a utility in LangChain that allows you to automatically fetch and extract content from web pages (HTML). It simplifies the process of turning online information into a format that LLMs can use.\n",
        "\n",
        "Since LLMs have a limit on the amount of text they can process at once, long documents must be split into smaller chunks. The `RecursiveCharacterTextSplitter` does this intelligently by attempting to split at natural boundaries (such as paragraphs or sentences), while also allowing for **overlap between chunks** to preserve context across segments. For example, with a `chunk_size` of 1000 characters and a `chunk_overlap` of 200, the splitter ensures that each new chunk shares 200 characters with the previous one.  \n",
        "\n",
        "\n",
        "Note: In this notebook, we illustrate RAG using a fixed list of websites loaded with WebBaseLoader, which is ideal for loading specific internal, private, or curated documents. However, apps like [Tavily](https://www.tavily.com/), a dynamic web search API designed for LLMs, can be seamlessly integrated to enable the system to autonomously discover relevant and up-to-date content from the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8AoQoRfMl1n"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# --- 1. Load Documents (Loader) ---\n",
        "# We use a web loader to fetch content from multiple authoritative URLs.\n",
        "print(\"Loading documents from the web...\")\n",
        "loader = WebBaseLoader([\n",
        "     \"https://en.wikipedia.org/wiki/DeepSeek\", #not that it is in English\n",
        "     \"https://en.wikipedia.org/wiki/DeepSeek_(chatbot)\",\n",
        "     # Add more  URLs here if needed\n",
        "])\n",
        "\n",
        "# Load documents from the specified URLs\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} document(s).\")\n",
        "\n",
        "# Optional: Clean Up Excessive Line Breaks in Page Content ---\n",
        "def clean_text(text):\n",
        "    # Remove multiple line breaks and strip trailing spaces\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "    text = '\\n'.join(line.strip() for line in text.splitlines() if line.strip())\n",
        "    return text\n",
        "\n",
        "for doc in docs:\n",
        "    doc.page_content = clean_text(doc.page_content)\n",
        "\n",
        "# --- 2. Split Text (Splitter) ---\n",
        "# We split each document into smaller chunks so the model can process them effectively.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "#Each element in splits is a Document with a page_content attribute that contains the text of that fragment.\n",
        "\n",
        "# Confirm number of splits\n",
        "print(f\"Split into {len(splits)} chunks (chunk_size=1000, overlap=200).\")\n",
        "\n",
        "print(\"\\n--- Content of 10th chunk ---\")\n",
        "print(splits[10].page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlP0K9m1Mmwn"
      },
      "source": [
        "##RAG Pipeline: Creating Embeddings and Vector Store\n",
        "\n",
        "We will perform three main steps:\n",
        "\n",
        "1. **Create vector embeddings**  \n",
        "   We transform each text chunk into a numerical vector using a **multilingual sentence embedding model** from Hugging Face. This captures the semantic meaning of each chunk, allowing for accurate similarity comparisons.  We're using `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`, which supports multiple languages ‚Äî ideal for multilingual document sets.    There are many alternatives depending on your needs (e.g., English-only, faster, domain-specific).\n",
        "\n",
        "2. **Store vectors in a FAISS database**  \n",
        "   [FAISS](https://faiss.ai/) (Facebook AI Similarity Search) is a high-performance library for fast vector similarity search. It allows us to quickly find the most relevant chunks for a given user query. You could replace FAISS with other vector stores such as Chroma, Qdrant, Weaviate, or Pinecone ‚Äî especially in production use cases.\n",
        "\n",
        "3. **Create a RAG chain**  \n",
        "   We combine the retriever with a prompt-driven LLM chain.\n",
        "\n",
        "This architecture enhances factual accuracy and reduces hallucination by grounding the model‚Äôs answers in verifiable sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJj9mNt3bsKJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# --- 3. Create Embeddings and Vector Store ---\n",
        "# We convert the text chunks into dense vectors (embeddings) using a multilingual model.\n",
        "# These vectors capture the semantic meaning of the text for similarity search.\n",
        "print(\"Creating vector database...\")\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "# Note: You can replace this model with another multilingual or domain-specific one if needed.\n",
        "\n",
        "# FAISS stores the embeddings and supports fast vector similarity search.\n",
        "# Alternatives include Chroma, Qdrant, Weaviate, Pinecone, etc.\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings_model)\n",
        "\n",
        "# --- 4. Create the Retriever ---\n",
        "# This component queries the vector store and returns the most relevant text chunks.\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# --- 5. Create the RAG Chain ---\n",
        "print(\"Building the RAG chain...\")\n",
        "\n",
        "# This prompt defines the system's behavior and instructs it to use the retrieved context.\n",
        "# The {context} placeholder will be automatically filled by LangChain\n",
        "system_prompt = \"\"\"\n",
        "You are an expert assistant for answering questions.\n",
        "Use the following retrieved context to answer the user's question.\n",
        "If you don't know the answer based on the context, just say so.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "# Define a prompt template that includes both the system instruction and the user's question.\n",
        "# {input} will be filled when asking the question\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# This document chain takes the retrieved documents and formats them for the LLM.\n",
        "# This sub-chain injects the context + user input into the LLM using the above prompt.\n",
        "rag_prompt_chain = create_stuff_documents_chain(llm_low_temp, prompt_template)\n",
        "\n",
        "# The final RAG chain combines retrieval + generation.\n",
        "# It retrieves documents and passes them to the LLM with the prompt.\n",
        "rag_chain = create_retrieval_chain(retriever, rag_prompt_chain)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZANdlg7jmCna"
      },
      "source": [
        "## Querying the RAG System\n",
        "\n",
        "Now that we've built the full RAG pipeline ‚Äî including document embeddings, retrieval, and prompt formatting ‚Äî we can test it by asking a question.\n",
        "\n",
        "In this step:\n",
        "\n",
        "- We send a natural language query to the `rag_chain`.\n",
        "- The system uses the retriever to find relevant document chunks based on semantic similarity.\n",
        "- Those chunks are injected into the prompt as context.\n",
        "- The language model then generates an answer based only on that context.\n",
        "- If the context does not contain the necessary information, the model should respond with \"I don't know\".\n",
        "\n",
        "Note that the retriever returns several document chunks that are combined and passed as context for the LLM. *Passing too many or large chunks can exceed the model‚Äôs token limit*. To prevent this, it‚Äôs common to limit the number of chunks retrieved, split documents into smaller pieces, or use more advanced chains like `map_reduce` that handle large contexts iteratively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIodRstJeJR1"
      },
      "outputs": [],
      "source": [
        "# --- 6. Ask a Question ---\n",
        "question = \"¬øCu√°ntos par√°metros tiene el modelo DeepSeek?\"\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "\n",
        "response = rag_chain.invoke({\"input\": question})\n",
        "\n",
        "print(\"\\n--- RAG System Answer ---\")\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65yVXJ0jtllY"
      },
      "source": [
        "# The Memory Challenge: Building a Conversational Chat\n",
        "\n",
        "A crucial concept to understand is that LLMs are stateless. By default, every time you send a request (`invoke`), the model treats it as a completely new, independent interaction. It has no memory of your previous questions or its own previous answers.\n",
        "\n",
        "This is a problem for building chatbots. Let's demonstrate this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZjGAoeZtr22"
      },
      "source": [
        "##The Problem: A Stateless Conversation\n",
        "We will tell the model our name and its name in one request. Then, in a separate request, we'll ask it to recall the information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D67o2V0ufLy"
      },
      "outputs": [],
      "source": [
        "# First interaction: We provide names\n",
        "print(\"--- Interaction 1: Providing Information ---\")\n",
        "first_prompt = \"Hi! My name is Emilio, and I'll call you Smart-Bot.\"\n",
        "first_response = llm.invoke(first_prompt)\n",
        "print(f\"Me: {first_prompt}\")\n",
        "print(f\"LLM: {first_response.content}\")\n",
        "\n",
        "# Second, separate interaction: We ask it to recall the information\n",
        "print(\"\\n--- Interaction 2: Asking for Recall ---\")\n",
        "second_prompt = \"What is my name?\"\n",
        "second_response = llm.invoke(second_prompt)\n",
        "print(f\"Me: {second_prompt}\")\n",
        "print(f\"LLM: {second_response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E84eyoiKuf01"
      },
      "source": [
        "As you can see, the LLM has no idea what our name is. The second call was a completely separate transaction with no context from the first one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvhNxBtGui4m"
      },
      "source": [
        "## The Solution: Managing Chat History\n",
        "\n",
        "To have a conversation, we must manually include the history of the conversation in every new request. We provide the previous turns of dialogue as context for the model to use.\n",
        "\n",
        "LangChain uses a specific message format for this (`HumanMessage`, `AIMessage`). Let's manage the history ourselves to see how it works under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryKu2-4JunoE"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# We will store the conversation history in a simple list\n",
        "chat_history = []\n",
        "\n",
        "# --- Interaction 1 ---\n",
        "print(\"--- Interaction 1 (with history) ---\")\n",
        "prompt_1 = HumanMessage(content=\"Hi! My name is Emilio, and I'll call you Smart-Bot.\")\n",
        "response_1 = llm.invoke([prompt_1]) # We send the message inside a list\n",
        "\n",
        "# Add the first exchange to our history\n",
        "chat_history.append(prompt_1)\n",
        "chat_history.append(response_1)\n",
        "\n",
        "print(f\"Me: {prompt_1.content}\")\n",
        "print(f\"LLM: {response_1.content}\")\n",
        "\n",
        "\n",
        "# --- Interaction 2 ---\n",
        "print(\"\\n--- Interaction 2 (with history) ---\")\n",
        "prompt_2 = HumanMessage(content=\"Great! Now, do you remember what my name is?\")\n",
        "\n",
        "# IMPORTANT: We include the previous history in our new request!\n",
        "# The LLM now has the context it needs.\n",
        "response_2 = llm.invoke(chat_history + [prompt_2])\n",
        "\n",
        "# We could continue adding this new exchange to the history if the chat were to continue\n",
        "# chat_history.append(prompt_2)\n",
        "# chat_history.append(response_2)\n",
        "\n",
        "print(f\"Me: {prompt_2.content}\")\n",
        "print(f\"LLM: {response_2.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ReEeqqupQT"
      },
      "source": [
        "Success! By passing the previous messages along with our new question, the model had the necessary context to answer correctly.\n",
        "\n",
        "This code demonstrates the fundamental principle of conversational memory. For more complex applications, the current best practice is to use LangGraph, which replaces LangChain‚Äôs previous memory components and offers a more robust and scalable system for managing dialogue history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umQmWcoKutiU"
      },
      "source": [
        "##  Building an Interactive Chatbot with Memory and a GUI\n",
        "\n",
        "\n",
        "The previous example showed why explicit memory management‚Äîmanually passing a list of messages‚Äîis necessary but can quickly get complicated. Now, let‚Äôs build a much more elegant and powerful chatbot that feels like a real application.\n",
        "\n",
        "This chatbot will:\n",
        "\n",
        "\n",
        "- Employ LangGraph‚Äôs `StateGraph` as memory backend, which manages conversation states in a graph structure. This allows handling complex dialogue flows, including cycles and state transitions, more naturally than simple lists.\n",
        "\n",
        "- Leverage `MessagesState`, a  state schema designed for managing message histories within LangGraph.\n",
        "\n",
        "- Define a clear System Role to give the assistant a distinct personality and rules as done in previous examples.\n",
        "\n",
        "- Include a  Graphical User Interface (GUI) built with Gradio, enabling direct interaction through a browser.\n",
        "\n",
        "`LangGraph` complements LangChain by adding orchestration capabilities and fine-grained control over conversation state using graph-based logic. While LangChain provides the foundational components‚Äîsuch as prompt templates, chains, and tool integration‚ÄîLangGraph focuses on modeling dialogue as dynamic state transitions. This separation of concerns allows for more scalable and maintainable conversational agents.\n",
        "\n",
        "To build the GUI, we'll use `Gradio`. Gradio is a  Python library that allows us to create and share a web-based user interface for our machine learning models with just a few lines of code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KfQBl-Hauwlg"
      },
      "outputs": [],
      "source": [
        "# 1. Install necessary libraries\n",
        "# We need gradio for the GUI and langgraph for the modern conversational flow.\n",
        "!pip install langchain langchain-groq gradio langgraph -q\n",
        "\n",
        "# 2. Import all required modules\n",
        "import os\n",
        "import gradio as gr\n",
        "from typing import List\n",
        "\n",
        "# --- LangChain & LangGraph Core Data Structures ---\n",
        "# BaseMessage: The abstract base class for all message types. Every turn in a conversation is a `BaseMessage`.\n",
        "# HumanMessage: Represents a message from the user.\n",
        "# AIMessage: Represents a message from the AI model.\n",
        "# SystemMessage: A message that sets the context and instructions for the AI, not part of the back-and-forth conversation.\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# StateGraph: The main class for building stateful, potentially cyclical graphs.\n",
        "# END: A special node that signifies the end of a graph's execution.\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# MessagesState: A pre-built helper from LangGraph for managing state.\n",
        "# It's essentially a TypedDict with one key: `messages`, which is a list of `BaseMessage` objects.\n",
        "# Its structure is: {\"messages\": [BaseMessage, HumanMessage, AIMessage, ...]}\n",
        "from langgraph.graph.message import MessagesState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdHzX4MjuxTN"
      },
      "source": [
        "Let us create *improBot ‚Äî Your Instant Comedy Sketch Creator*.\n",
        "\n",
        "A witty chatbot that crafts original, hilarious comedy sketches by blending all the ideas you‚Äôve shared throughout the conversation ‚Äî no questions asked, just pure improv humor.\n",
        "\n",
        "This task is quite challenging‚Äîeven for a human‚Äîbut it serves as a way to evaluate the memory capabilities of our chatbot.\n",
        "\n",
        "Note: After multiple unsuccessful attempts using the LLaMA 3 8B model, I switched to the more powerful LLaMA 3 70B.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pvMl7NLdu1Qz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Groq API Key configured.\n",
            "LangGraph graph compiled.\n",
            "\n",
            "üöÄ Launching the chat interface (LangGraph version)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/t2/d3hrzr8x7zd9cnztszgjcw0h0000gn/T/ipykernel_13750/425621199.py:98: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(height=400),\n",
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/gradio/chat_interface.py:330: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/11/06 13:14:29 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> None\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Configure the Groq LLM\n",
        "try:\n",
        "    load_dotenv()\n",
        "    #Using google.colab secrets\n",
        "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    os.environ['GROQ_API_KEY'] = api_key\n",
        "    print(\"‚úÖ Groq API Key configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"üõë Error getting API Key: {e}\")\n",
        "    print(\"Please configure the 'GROQ_API_KEY' secret in Google Colab.\")\n",
        "\n",
        "# 4. Initialize the language model and chatbot personality\n",
        "#let us use a bigger and more powerful model!\n",
        "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
        "# The bot's personality is defined here as a `SystemMessage`.\n",
        "# This message will be added to the beginning of the conversation history on every single call.\n",
        "system_prompt_text = \"\"\"\n",
        "\n",
        "You are improBot, a creative comedy sketch writer with a sharp sense of humor\n",
        "and an uncanny ability to weave diverse ideas into hilarious, seamless sketches.\n",
        "Your task is to write original comedic sketches that incorporate all relevant elements\n",
        "and ideas the user has provided during the entire conversation.\n",
        "You never ask questions or request clarifications.\n",
        "Instead, you use the full conversation history as your sole inspiration to craft each new sketch from scratch.\n",
        "Do NOT ignore earlier parts of the conversation.\n",
        "Use every piece of information shared before to build a funny, engaging sketch that entertains and surprises.\n",
        "\n",
        "\"\"\"\n",
        "system_message = SystemMessage(content=system_prompt_text)\n",
        "\n",
        "# 5. Define the Conversation Graph using LangGraph\n",
        "\n",
        "# NOTE: This is a minimal linear graph with a single node and no branching or cycles.\n",
        "# It's a great starting point to understand how LangGraph works before adding complexity.\n",
        "\n",
        "# Define the graph's node: a function that will call our LLM\n",
        "#    It takes the current state (`MessagesState` dict) as input\n",
        "#    This dictionary has the structure {\"messages\": [BaseMessage, HumanMessage, AIMessage, ...]}\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"Invokes the LLM with the current list of messages.\"\"\"\n",
        "    # The `state` dictionary contains the key 'messages', which holds the conversation history.\n",
        "    response = llm.invoke(state['messages'])\n",
        "    # We return a dictionary that matches the `MessagesState` structure.\n",
        "    # The graph will automatically append this new AI message to the 'messages' list.\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Instantiate the StateGraph. We define the \"shape\" of our state using `MessagesState`.\n",
        "# This tells the graph that its state will always be a dictionary with a 'messages' key.\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# Add our defined function call_model as a node named \"llm\" to the graph.\n",
        "workflow.add_node(\"llm\", call_model)\n",
        "\n",
        "# Set the entry point of the graph. Execution will always start at the \"llm\" node.\n",
        "workflow.set_entry_point(\"llm\")\n",
        "\n",
        "# Set the end point. After the \"llm\" node runs, the graph execution finishes.\n",
        "workflow.add_edge(\"llm\", END)\n",
        "\n",
        "# Compile the workflow into a runnable application.\n",
        "runnableApp= workflow.compile()\n",
        "\n",
        "print(\"LangGraph graph compiled.\")\n",
        "\n",
        "\n",
        "# 6. Create the function to be called by the Gradio GUI\n",
        "def myChatbot_langgraph(user_message: str, history: List[List[str]]):\n",
        "    \"\"\"\n",
        "    This function bridges Gradio with our LangGraph application.\n",
        "    1. Converts Gradio's simple list-based history into LangChain's structured message format.\n",
        "    2. Invokes the LangGraph app with the complete conversation state.\n",
        "    3. Returns the AI's response string to the Gradio UI.\n",
        "    \"\"\"\n",
        "    # Gradio's `history` format is a simple list of lists: [[\"user input\", \"ai response\"], ...].\n",
        "    # We need to convert this into the `List[BaseMessage]` format LangChain/LangGraph expects.\n",
        "    langchain_messages = [system_message] # Always start with the system prompt with the chatbot personality\n",
        "    for human, ai in history:\n",
        "        langchain_messages.append(HumanMessage(content=human))\n",
        "        langchain_messages.append(AIMessage(content=ai))\n",
        "\n",
        "    # Add the user's latest message (parameter of myChatbot_langgraph that will be called by the Gradio Interface)\n",
        "    langchain_messages.append(HumanMessage(content=user_message))\n",
        "\n",
        "    # Invoke the graph with a state dictionary that matches the `MessagesState` schema.\n",
        "    response_state = runnableApp.invoke({\"messages\": langchain_messages})\n",
        "\n",
        "    # The `response_state` is the final state of the graph. Its 'messages' list contains the entire conversation,\n",
        "    # with the very last message being the new AI response (index -1 in a Python List).\n",
        "    return response_state['messages'][-1].content\n",
        "\n",
        "\n",
        "# 6. Launch the Gradio Chat Interface\n",
        "print(\"\\nüöÄ Launching the chat interface (LangGraph version)...\")\n",
        "gr.ChatInterface(\n",
        "    myChatbot_langgraph,\n",
        "    title=\"improBot ‚Äî Your Instant Comedy Sketch Creator\",\n",
        "    description=\"A witty chatbot that crafts original, hilarious comedy sketches by blending all the ideas you‚Äôve shared throughout the conversation ‚Äî no questions asked, just pure improv humor\",\n",
        "    chatbot=gr.Chatbot(height=400),\n",
        "    theme=\"soft\"\n",
        ").launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeWBfr-Wr0eg"
      },
      "source": [
        "# Conclusions and Future Steps\n",
        "\n",
        "In this notebook, we explored  advanced architectures for building generative AI systems:\n",
        "\n",
        "* Summarizing Long Documents with LangChain by using the Map-reduce and Refine approaches.\n",
        "\n",
        "* Retrieval-Augmented Generation (RAG): We saw how RAG systems enable language models to retrieve and synthesize information from external document collections, improving factual accuracy and grounding responses.\n",
        "\n",
        "*  Manage conversation history with structured memory using LangGraph and bulding  an interactive web-based chatbot with Gradio.\n",
        "\n",
        "**Next Steps**\n",
        "\n",
        "* Deploy your own chatbot prototype for free using [Gradio on Hugging Face Spaces](https://www.gradio.app/main/guides/deploying-gradio-with-docker).\n",
        "\n",
        "* Build a multi-agent LLM system with LangGraph or AgentCrew:   Build networks of agents that collaborate, challenge, or specialize in different roles.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
