{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sd92gC-J3Mc"
      },
      "source": [
        "# License and Attribution\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KJiOjdMKZMZ"
      },
      "source": [
        "# Building a Multi-Agent Research Assistant with LangGraph\n",
        "\n",
        "This notebook demonstrates how to build a  multi-agent system using LangGraph.\n",
        "Our goal is to create an autonomous research assistant that can take a high-level research topic and produce a well-structured, detailed report.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks6S7ORNNH5o"
      },
      "source": [
        "#Initial setup\n",
        "\n",
        "We will use a Groq API key (get it from the [Groq Console](https://console.groq.com/home)) and a  Tavily API key (get it from the [Tavily AI Dashboard](https://app.tavily.com/home)).\n",
        "* Groq is an AI platform that offers an API for ultra-fast inference of large language models like Llama and Mixtral. Its API provides low-latency, real-time access to LLMs, making it ideal for building high-performance AI applications such as chatbots or real-time assistants.\n",
        "*  Tavily is a web search API designed specifically for large language models (LLMs) and RAG (retrieval-augmented generation) workflows. It allows AI agents to access real-time, factual web results through a simple API call, helping them retrieve and cite up-to-date information with ease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d-qDYUAv2tMa"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 1. ENVIRONMENT SETUP\n",
        "# ==============================================================================\n",
        "# Install necessary Python libraries.\n",
        "# - langgraph: The core library for building stateful, multi-agent applications.\n",
        "# - langchain_groq: Allows us to use the super-fast Groq API for our LLMs.\n",
        "# - langchain_core: Provides core abstractions for building with LLMs.\n",
        "# - langchain: The broader LangChain ecosystem.\n",
        "# - langchain-tavily: A client for the Tavily search API for agent research.\n",
        "# - langchain-community: Open-source collection of connectors and modules\n",
        "# ==============================================================================\n",
        "!pip install -qU langgraph langchain_groq langchain_core langchain langchain-tavily langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7udcNZfyG0KC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API keys successfully retrieved from Colab secrets.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
            "/var/folders/t2/d3hrzr8x7zd9cnztszgjcw0h0000gn/T/ipykernel_20516/2763382193.py:69: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
            "  tool = TavilySearchResults(\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 2. API KEY CONFIGURATION\n",
        "# ==============================================================================\n",
        "# Import necessary libraries for handling environment variables and secure password entry.\n",
        "import os\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "# For Google Colab: Import the module to access stored secrets\n",
        "\n",
        "# Retrieve API keys stored securely in Colab (add them via the Secrets tab)\n",
        "load_dotenv()\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Basic checks to ensure the API keys were retrieved successfully\n",
        "if not groq_api_key:\n",
        "    raise ValueError(\"‚ùå GROQ_API_KEY not found in Colab secrets. Please add it via the Secrets tab.\")\n",
        "if not tavily_api_key:\n",
        "    raise ValueError(\"‚ùå TAVILY_API_KEY not found in Colab secrets. Please add it via the Secrets tab.\")\n",
        "\n",
        "print(\"‚úÖ API keys successfully retrieved from Colab secrets.\")\n",
        "\n",
        "# Set them as environment variables (optional, for libraries that expect them there)\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. IMPORTS\n",
        "# ==============================================================================\n",
        "# Import typing utilities for defining the state structure.\n",
        "from typing import TypedDict, List, Annotated\n",
        "import operator\n",
        "\n",
        "# Import core LangChain message types.\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
        "# Import prompt templates for creating structured prompts for the LLM.\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "# Import a JSON output parser to handle LLM outputs.\n",
        "from langchain_core.output_parsers.json import JsonOutputParser\n",
        "# Import the ChatGroq model for high-speed LLM inference.\n",
        "from langchain_groq import ChatGroq\n",
        "# Import the Tavily search tool for web research.\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "# Import the main components for building the graph.\n",
        "from langgraph.graph import StateGraph, END\n",
        "# Import display utilities for rendering the final output nicely.\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INITIALIZING THE LLM AND TOOLS\n",
        "# ==============================================================================\n",
        "# Initialize the ChatGroq model.\n",
        "# We use llama3-70b for its strong reasoning and large context window.\n",
        "# The temperature is set to 0 to encourage deterministic and factual outputs.\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    api_key=groq_api_key  # explicitly passing the key\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the Tavily Search tool.\n",
        "# max_results=3 means it will return the top 3 search results for a query.\n",
        "# The system can handle more queries, but be cautious: Groq may throw an error if too many are processed in a short time.\n",
        "tool = TavilySearchResults(\n",
        "    max_results=2,\n",
        "    api_key=tavily_api_key  # explicitly passing the key\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ezZAvjwI7I2"
      },
      "source": [
        "‚ö†Ô∏è Alert: the code may exceeds token-per-minute (TPM) limit. This is not a context window overflow. Groq's service tier (on_demand) restricts the number of tokens you can send per minute. Check Groq Rate Limits. For instance, meta-llama/llama-4-scout-17b-16e-instruct has 30K TPM vs llama-3.1-8b-instant with 6K TPM. Note that 1 token ‚âà 3.5‚Äì4 average English characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGEl4sCFNPRv"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#Core Concepts & Agentic Patterns\n",
        "This notebook will implement several key \"agentic patterns\" to make our system robust and intelligent:\n",
        "\n",
        "*  *Planning*: An agent that breaks down a complex goal into a sequence of manageable steps.\n",
        "*  *Tool Use*: Agents that can use external tools (in this case, an internet search engine) to gather information.\n",
        "*  *Reflection & Self-Correction*: A cyclical reasoning process in which an agent (or multiple agents) critically evaluates its own outputs (or those of another agent), identifies flaws or areas for improvement, and iteratively refines the response until a desired quality threshold is met.\n",
        "*  *Multi-Agent Collaboration*: Multiple specialized agents (Planner, Searcher, Summarizer, Writer, Critique) that work together, passing information along a defined workflow.\n",
        "\n",
        "\n",
        "The Workflow\n",
        "Here is the flow of our multi-agent system:\n",
        "* *Input*: We provide a research topic (e.g., \"The Rise of DeepSeek\").\n",
        "* *Planner Agent*: Creates a step-by-step research plan.\n",
        "* *Search Agent*: Executes the plan, performing web searches for each step and gathering raw data.\n",
        "* *Summarizer Agent*: Reads all the raw data and condenses it into concise summaries. This is crucial for handling large amounts of information.\n",
        "* *Writer Agent*: Uses the summaries to write the first draft of a comprehensive report.\n",
        "\n",
        "* Critique & Refine Loop:\n",
        "   * The Critique Agent reviews the draft for errors, omissions, and areas for improvement.\n",
        "   * If improvements are needed, the draft and the critique are sent back to the Writer Agent for revision.\n",
        "   * This loop repeats several times, enhancing the report with each iteration.\n",
        "* *Output*: A final, polished research report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-rfMOtBRwGs"
      },
      "source": [
        "## Some design decisions\n",
        "**About THIS planner agent**:  it is intentionally restricted to generating only a list of search queries, based on preliminary information gathered via a first web search. This design choice supports two goals: (1) ensuring modularity by giving the planner a narrow, well-defined role, and (2) enabling it to handle unknown or niche topics‚Äîespecially useful when using LLMs without internet access or limited domain knowledge (e.g., asking LLaMA about \"DeepSeek\"). Generic research plans like ‚Äúgather information, analyze, synthesize‚Äù are avoided because later agents in the system are already dedicated to those tasks. Instead, the planner focuses solely on formulating precise, actionable search queries to bootstrap the agentic pipeline.\n",
        "\n",
        "**About THIS summarizer agent**:  uses a refine strategy to incrementally build a comprehensive summary from a list of retrieved documents. Instead of concatenating all documents at once‚Äîwhich could easily exceed the LLM‚Äôs context window‚Äîit processes each document sequentially, updating the summary step-by-step. This approach is particularly useful for long or numerous inputs, making the agent scalable and memory-efficient. While we implement the refinement loop manually here for clarity, LangChain provides built-in summarization chains that encapsulate this logic and streamline the process further.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B68HKyYJrIAp"
      },
      "source": [
        "#Defining the graph's state, the agents (nodes), and the graph's flow\n",
        "\n",
        "The core components of this LangGraph-based multi-agent system are:\n",
        "\n",
        "* State Definition: The shared memory (AgentState) used to pass information between agents throughout the workflow. It tracks the research topic, search plan, documents, summaries, drafts, critiques, and revision counts.\n",
        "\n",
        "* Agents (Nodes): Specialized LLM-based functions that each perform a single role in the research process:\n",
        "  * Planner: Generates search queries from the initial topic.\n",
        "  * Searcher: Executes those queries via an external tool (like Tavily).\n",
        "  * Summarizer: Synthesizes retrieved documents using a refine strategy.\n",
        "  * Writer: Drafts a structured report in Markdown.\n",
        "  * Critique: Reviews and identifies flaws in the draft.\n",
        "  * Reviser: Improves the draft based on critique.\n",
        "\n",
        "* Graph Flow: The execution logic that wires the nodes together. It defines a linear flow (from planner to writer), followed by a reflection loop where the draft is iteratively critiqued and revised until it reaches acceptable quality or a revision limit.\n",
        "\n",
        "\n",
        "**Comparison with Other Frameworks**\n",
        "\n",
        "LangGraph differs from tools like CrewAI, Autogen, or OpenAgents in a few key ways:\n",
        "\n",
        "* Stateful Design: LangGraph emphasizes fine-grained, persistent state tracking between nodes ‚Äî useful for complex or long-running tasks.\n",
        "* Graph-based Control Flow: Instead of linear or chat-like interactions, you explicitly define node transitions, conditionals, and loops using a graph.\n",
        "* Low-level Flexibility: While CrewAI focuses on higher-level abstractions like roles, goals, and tool routing, LangGraph gives you more direct control over logic and memory ‚Äî making it ideal for developers who want custom, deterministic behaviors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d7Dm7ofSJgLh"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 5. DEFINING THE GRAPH'S STATE\n",
        "# ==============================================================================\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the state that will be passed between nodes.\n",
        "    It's a TypedDict, which means it's a dictionary with a predefined set of keys and value types.\n",
        "    \"\"\"\n",
        "    # The initial research topic provided by the user.\n",
        "    research_topic: str\n",
        "\n",
        "    # A step-by-step plan of search queries generated by the Planner agent.\n",
        "    plan: List[str]\n",
        "\n",
        "    # A list of documents retrieved from web searches.\n",
        "    retrieved_docs: List[str]\n",
        "\n",
        "    # A list of concise summaries for the retrieved documents.\n",
        "    # Even if it is a list, it will only contain a summary made with refine strategy\n",
        "    summaries: List[str]\n",
        "\n",
        "    # The current draft of the final report.\n",
        "    draft: str\n",
        "\n",
        "    # Feedback and critique from the Critique agent on the current draft.\n",
        "    critique: str\n",
        "\n",
        "    # A counter for the number of revisions made.\n",
        "    # Annotated and operator.add ensure this value is accumulated across nodes.\n",
        "    revision_number: Annotated[int, operator.add]\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. DEFINING THE AGENTS (NODES)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- AGENT 1: ENHANCED PLANNER ---\n",
        "# This planner first performs a preliminary search to inform its planning process.\n",
        "# This makes the plan more robust, especially for novel or unknown topics.\n",
        "\n",
        "planner_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    You are an expert research planner. Your role is to devise a detailed, step-by-step plan of search queries to research a given topic.\n",
        "    A preliminary search has been conducted to provide you with some initial context.\n",
        "\n",
        "    **Research Topic:** {research_topic}\n",
        "\n",
        "    **Preliminary Information:**\n",
        "    {preliminary_info}\n",
        "\n",
        "    **Instructions:**\n",
        "    1.  Based on the preliminary information provided, as well as any relevant prior knowledge you may have about the research topic, generate a list of 3 to 5 focused and distinct search queries.\n",
        "    2.  These queries should be designed to gather detailed information for writing a comprehensive report.\n",
        "    3.  **Crucially, the plan must consist solely of a list of no more than 5 search queries.** Do not include steps like \"summarize the findings\", \"analyze the results\", or \"write the report\", as other specialized agents will handle those tasks.\n",
        "    4.  Output your plan as a JSON object with a single key \"plan\" which contains a list of strings (the search queries).\n",
        "    \"\"\",\n",
        "    input_variables=[\"research_topic\", \"preliminary_info\"],\n",
        ")\n",
        "\n",
        "# Create the chain for the Planner agent.\n",
        "planner_agent = planner_prompt | llm | JsonOutputParser()\n",
        "\n",
        "def planner_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Represents the enhanced Planner agent.\n",
        "    1. It performs a preliminary search on the research topic.\n",
        "    2. It uses the results to generate a focused, high-quality plan of search queries.\n",
        "    \"\"\"\n",
        "    print(\"--- üß† INVOKING ENHANCED PLANNER ---\")\n",
        "    print(\"Performing preliminary search...\")\n",
        "    # 1. Perform the preliminary search.\n",
        "    prelim_results = tool.invoke({\"query\": state[\"research_topic\"]})\n",
        "    prelim_info = \"\\n\\n\".join([res[\"content\"] for res in prelim_results])\n",
        "\n",
        "    print(\"Generating a focused plan based on preliminary findings...\")\n",
        "    # 2. Invoke the planner LLM with the context.\n",
        "    plan_result = planner_agent.invoke({\n",
        "        \"research_topic\": state[\"research_topic\"],\n",
        "        \"preliminary_info\": prelim_info\n",
        "    })\n",
        "    plan = plan_result.get('plan', [])\n",
        "\n",
        "    print(\"--- Generated Plan ---\")\n",
        "    for i, step in enumerate(plan):\n",
        "        print(f\"{i+1}. {step}\")\n",
        "    print(\"--------------------\")\n",
        "\n",
        "    return {\"plan\": plan}\n",
        "\n",
        "\n",
        "# --- AGENT 2: SEARCHER ---\n",
        "# This agent executes the plan by performing web searches.\n",
        "\n",
        "def search_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Represents the Searcher agent.\n",
        "    It iterates through the plan, uses the Tavily search tool for each step,\n",
        "    and aggregates the results into a list of documents.\n",
        "    \"\"\"\n",
        "    print(\"--- üîç INVOKING SEARCHER ---\")\n",
        "    plan = state[\"plan\"]\n",
        "    all_docs = []\n",
        "    seen_docs = set()  # For deduplication based on document content\n",
        "\n",
        "    for step in plan:\n",
        "        print(f\"Searching for: '{step}'\")\n",
        "        search_results = tool.invoke({\"query\": step})\n",
        "        for result in search_results:\n",
        "            content = result[\"content\"]\n",
        "            if content not in seen_docs:\n",
        "                all_docs.append(content)\n",
        "                seen_docs.add(content)\n",
        "        print(f\"Found {len(search_results)} documents for this step (unique documents so far: {len(seen_docs)}).\")\n",
        "\n",
        "    print(f\"--- Total unique documents retrieved: {len(all_docs)} ---\")\n",
        "    return {\"retrieved_docs\": all_docs}\n",
        "\n",
        "# --- AGENT 3: SUMMARIZER ---\n",
        "# This agent condenses the retrieved documents into a single, cohesive summary.\n",
        "\n",
        "summarizer_prompt = PromptTemplate(\n",
        "\n",
        "\n",
        "    template=\"\"\"\n",
        "    You are an expert summarizer. Your task is to create a concise and comprehensive summary to support a research report on the following topic:\n",
        "\n",
        "    **Research Topic:** {research_topic}\n",
        "\n",
        "    Read the \"Existing Summary\" (if any) and the \"New Document\", and integrate only the key information that is relevant to the research topic. Ignore content that is off-topic or redundant.\n",
        "\n",
        "    Keep the summary factual, clear, and focused on the main points.\n",
        "\n",
        "    **Existing Summary:**\n",
        "    {existing_summary}\n",
        "\n",
        "    **New Document:**\n",
        "    {document}\n",
        "    \"\"\",\n",
        "    input_variables=[\"existing_summary\", \"document\"],\n",
        ")\n",
        "\n",
        "summarizer_agent = summarizer_prompt | llm\n",
        "\n",
        "def summarizer_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Represents the Summarizer agent.\n",
        "    It iterates through all retrieved documents and builds a consolidated summary\n",
        "    using a \"refine\" strategy.\n",
        "    \"\"\"\n",
        "    print(\"--- üìù INVOKING SUMMARIZER ---\")\n",
        "    docs = state[\"retrieved_docs\"]\n",
        "    current_summary = \"No summary yet.\"\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        print(f\"Summarizing document {i+1}/{len(docs)}\")\n",
        "        refined_summary = summarizer_agent.invoke({\n",
        "            \"existing_summary\": current_summary,\n",
        "            \"document\": doc,\n",
        "            \"research_topic\": state[\"research_topic\"]\n",
        "        }).content\n",
        "        current_summary = refined_summary\n",
        "\n",
        "\n",
        "    print(\"--- Finished Summarization ---\")\n",
        "    return {\"summaries\": [current_summary]}\n",
        "\n",
        "\n",
        "# --- AGENT 4: WRITER ---\n",
        "# This agent writes the initial draft of the research report.\n",
        "\n",
        "writer_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    You are a professional report writer. Your task is to compose a detailed, well-structured research report based on the provided summary.\n",
        "    The report should have a clear structure: introduction, body, and conclusion. Use Markdown for formatting.\n",
        "    Ensure the tone is professional and objective.\n",
        "\n",
        "    **Research Topic:** {research_topic}\n",
        "    **Summary of Information:**\n",
        "    {summary}\n",
        "    \"\"\",\n",
        "    input_variables=[\"research_topic\", \"summary\"],\n",
        ")\n",
        "\n",
        "writer_agent = writer_prompt | llm\n",
        "\n",
        "def writer_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Represents the Writer agent.\n",
        "    It generates the initial draft of the research report based on the summaries.\n",
        "    \"\"\"\n",
        "    print(\"--- ‚úçÔ∏è INVOKING WRITER ---\")\n",
        "    summary_text = \"\\n\\n\".join(state[\"summaries\"]) #It joins all those blocks of text into a single string, inserting two line breaks (\\n\\n) between each one.\n",
        "\n",
        "    draft = writer_agent.invoke({\n",
        "        \"research_topic\": state[\"research_topic\"],\n",
        "        \"summary\": summary_text\n",
        "    }).content\n",
        "\n",
        "    print(\"--- Draft Generated ---\")\n",
        "    return {\"draft\": draft, \"revision_number\": 1}\n",
        "\n",
        "\n",
        "# --- AGENT 5: CRITIQUE ---\n",
        "# This agent reviews the draft and provides feedback for reflection.\n",
        "\n",
        "critique_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    You are an expert critic and editor. You review a research report draft and provide constructive feedback.\n",
        "    Identify any flaws (e.g., factual inaccuracies, logical gaps, poor structure).\n",
        "    Provide specific, actionable suggestions for improvement.\n",
        "    If the draft is excellent and requires no changes, respond with the single word: \"perfect\".\n",
        "\n",
        "    **Research Report Draft:**\n",
        "    {draft}\n",
        "    \"\"\",\n",
        "    input_variables=[\"draft\"],\n",
        ")\n",
        "\n",
        "critique_agent = critique_prompt | llm\n",
        "\n",
        "def critique_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Represents the Critique agent.\n",
        "    It reviews the writer's draft and provides feedback.\n",
        "    \"\"\"\n",
        "    print(\"--- üßê INVOKING CRITIQUE ---\")\n",
        "    critique = critique_agent.invoke({\"draft\": state[\"draft\"]}).content\n",
        "    print(f\"Critique Received: {critique}\")\n",
        "    return {\"critique\": critique}\n",
        "\n",
        "\n",
        "# --- AGENT 6: REVISER ---\n",
        "# This agent revises the draft based on the critique.\n",
        "\n",
        "reviser_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    You are a professional report writer. Your task is to revise a research report based on the provided critique.\n",
        "    Rewrite the draft to address all points in the critique. Maintain a professional tone and structure.\n",
        "\n",
        "    **Original Research Topic:** {research_topic}\n",
        "    **Original Draft:**\n",
        "    {draft}\n",
        "    **Critique and Revision Instructions:**\n",
        "    {critique}\n",
        "    \"\"\",\n",
        "    input_variables=[\"research_topic\", \"draft\", \"critique\"],\n",
        ")\n",
        "\n",
        "reviser_agent = reviser_prompt | llm\n",
        "\n",
        "def revise_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Represents the revision step.\n",
        "    It takes the draft and critique, and generates a new, improved draft.\n",
        "    \"\"\"\n",
        "    print(\"--- üîÑ INVOKING REVISER ---\")\n",
        "    revised_draft = reviser_agent.invoke({\n",
        "        \"research_topic\": state[\"research_topic\"],\n",
        "        \"draft\": state[\"draft\"],\n",
        "        \"critique\": state[\"critique\"]\n",
        "    }).content\n",
        "    print(\"--- Draft Revised ---\")\n",
        "    return {\"draft\": revised_draft, \"revision_number\": 1}\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. DEFINING THE GRAPH'S FLOW\n",
        "# ==============================================================================\n",
        "\n",
        "# --- CONDITIONAL LOGIC FOR THE REFLECTION LOOP ---\n",
        "\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    \"\"\"\n",
        "    This function determines the next step based on the critique.\n",
        "    It checks for the word \"perfect\" or if the maximum revision limit has been reached.\n",
        "    \"\"\"\n",
        "    print(\"--- ü§î CHECKING CRITIQUE ---\")\n",
        "    critique = state[\"critique\"]\n",
        "    revision_number = state[\"revision_number\"]\n",
        "    max_revisions = 3\n",
        "\n",
        "    if \"perfect\" in critique.lower():\n",
        "        print(\"--- Critique approved. Finishing workflow. ---\")\n",
        "        return \"end\"\n",
        "    elif revision_number >= max_revisions:\n",
        "        print(f\"--- Reached max revisions ({max_revisions}). Finishing workflow. ---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"--- Critique requires revision. Looping back. ---\")\n",
        "        return \"revise\"\n",
        "\n",
        "# --- ASSEMBLING THE GRAPH ---\n",
        "\n",
        "# Create a new StateGraph with our defined AgentState.\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add each function as a node in the graph.\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"searcher\", search_node)\n",
        "workflow.add_node(\"summarizer\", summarizer_node)\n",
        "workflow.add_node(\"writer\", writer_node)\n",
        "workflow.add_node(\"critique\", critique_node)\n",
        "workflow.add_node(\"reviser\", revise_node)\n",
        "\n",
        "# Set the entry point of the graph.\n",
        "workflow.set_entry_point(\"planner\")\n",
        "\n",
        "# Add the edges that define the standard flow.\n",
        "workflow.add_edge(\"planner\", \"searcher\")\n",
        "workflow.add_edge(\"searcher\", \"summarizer\")\n",
        "workflow.add_edge(\"summarizer\", \"writer\")\n",
        "workflow.add_edge(\"writer\", \"critique\")\n",
        "workflow.add_edge(\"reviser\", \"critique\") # After revising, it goes back for another critique.\n",
        "\n",
        "# Add the conditional edge for the reflection loop.\n",
        "workflow.add_conditional_edges(\n",
        "    \"critique\",       # The source node\n",
        "    should_continue,  # The function that decides the path\n",
        "    {\n",
        "        \"revise\": \"reviser\", # If \"revise\", go to the 'reviser' node\n",
        "        \"end\": END           # If \"end\", finish the workflow\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile the graph into a runnable application.\n",
        "app = workflow.compile()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu1dDZWGrdEv"
      },
      "source": [
        "#Running the multi-agent system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF10ixyvlRdp"
      },
      "source": [
        "In this section, we execute the LangGraph-based multi-agent workflow to perform a research task.\n",
        "\n",
        "The variable app is the compiled instance of the workflow graph, typically referred to as a `StateGraphApplication` in LangGraph. This object represents the fully assembled multi-agent system and provides methods to execute the graph, such as:\n",
        "\n",
        "- `invoke(inputs, options)`: Runs the entire workflow once with the given inputs and returns the final state.\n",
        "\n",
        "- `stream(inputs, options)`: Runs the workflow step-by-step, yielding intermediate outputs from each node as they complete, useful for observing progress in real time.\n",
        "\n",
        "The `recursion_limit`  parameter acts as the graph‚Äôs emergency brake. It‚Äôs a safety mechanism designed to halt the process if it gets caught in a loop, preventing infinite execution. The step count accumulates quickly:\n",
        "\n",
        "- The initial path up to the first critique involves 5 steps.\n",
        "\n",
        "- Each revision cycle (reviser ‚Üí critique) adds 2 more steps.\n",
        "\n",
        "If the recursion limit is exceeded, LangGraph stops execution with a `RecursionError` and restarts the workflow from the planner node.\n",
        "\n",
        "This approach stands in contrast to using a single LLM with a single prompt. Instead of relying on one-shot generation, we decompose the task into modular, interpretable steps‚Äîenabling better control, traceability, and adaptability across the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T3buQfmXJhwu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- üß† INVOKING ENHANCED PLANNER ---\n",
            "Performing preliminary search...\n",
            "Generating a focused plan based on preliminary findings...\n",
            "--- Generated Plan ---\n",
            "1. DeepSeek AI models architecture and design\n",
            "2. Comparison of DeepSeek AI models with OpenAI models performance and efficiency\n",
            "3. Liang Wenfeng background and experience in AI research and development\n",
            "4. High-Flyer quantitative hedge fund involvement in DeepSeek AI research\n",
            "5. DeepSeek AI models applications and potential use cases in various industries\n",
            "--------------------\n",
            "--- üîç INVOKING SEARCHER ---\n",
            "Searching for: 'DeepSeek AI models architecture and design'\n",
            "Found 2 documents for this step (unique documents so far: 2).\n",
            "Searching for: 'Comparison of DeepSeek AI models with OpenAI models performance and efficiency'\n",
            "Found 2 documents for this step (unique documents so far: 4).\n",
            "Searching for: 'Liang Wenfeng background and experience in AI research and development'\n",
            "Found 2 documents for this step (unique documents so far: 6).\n",
            "Searching for: 'High-Flyer quantitative hedge fund involvement in DeepSeek AI research'\n",
            "Found 2 documents for this step (unique documents so far: 8).\n",
            "Searching for: 'DeepSeek AI models applications and potential use cases in various industries'\n",
            "Found 2 documents for this step (unique documents so far: 10).\n",
            "--- Total unique documents retrieved: 10 ---\n",
            "--- üìù INVOKING SUMMARIZER ---\n",
            "Summarizing document 1/10\n",
            "Summarizing document 2/10\n",
            "Summarizing document 3/10\n",
            "Summarizing document 4/10\n",
            "Summarizing document 5/10\n",
            "Summarizing document 6/10\n",
            "Summarizing document 7/10\n",
            "Summarizing document 8/10\n",
            "Summarizing document 9/10\n",
            "Summarizing document 10/10\n",
            "--- Finished Summarization ---\n",
            "--- ‚úçÔ∏è INVOKING WRITER ---\n",
            "--- Draft Generated ---\n",
            "--- üßê INVOKING CRITIQUE ---\n",
            "Critique Received: **Overall Assessment:**\n",
            "The research report draft on DeepSeek is well-structured and provides a comprehensive overview of the model architecture, benefits, and applications. However, there are some areas that require improvement to enhance the report's clarity, accuracy, and overall quality.\n",
            "\n",
            "**Flaws and Suggestions:**\n",
            "\n",
            "1.  **Lack of Clear Definition of Key Terms:**\n",
            "    *   The report assumes that the reader is familiar with key terms such as \"MoE Architecture,\" \"Dynamic Parameter Selection,\" and \"Multi-head Latent Attention (MLA).\" Consider adding a brief explanation or definition for these terms to ensure clarity.\n",
            "    *   Suggestion: Include a glossary or a brief explanation of key terms in the introduction or appendix.\n",
            "\n",
            "2.  **Inconsistent Use of Terminology:**\n",
            "    *   The report uses both \"DeepSeekMoE\" and \"MoE Architecture\" interchangeably. Ensure consistent use of terminology throughout the report.\n",
            "    *   Suggestion: Use a single term consistently and provide a clear explanation of its meaning.\n",
            "\n",
            "3.  **Lack of Quantitative Data:**\n",
            "    *   The report makes several claims about DeepSeek's performance and benefits, but lacks quantitative data to support these claims.\n",
            "    *   Suggestion: Include specific numbers, percentages, or metrics to demonstrate the effectiveness of DeepSeek.\n",
            "\n",
            "4.  **Insufficient Explanation of Technical Concepts:**\n",
            "    *   The report assumes a high level of technical expertise from the reader. Consider adding more explanation or analogies to help non-technical readers understand complex concepts.\n",
            "    *   Suggestion: Use simple language and provide examples or analogies to explain technical concepts.\n",
            "\n",
            "5.  **Lack of Critical Evaluation:**\n",
            "    *   The report presents DeepSeek as a revolutionary solution without critically evaluating its limitations or potential drawbacks.\n",
            "    *   Suggestion: Include a balanced evaluation of DeepSeek's strengths and weaknesses to provide a more comprehensive understanding.\n",
            "\n",
            "6.  **Inadequate Citation of Sources:**\n",
            "    *   The report lacks proper citation of sources, which can make it difficult to verify the accuracy of the information presented.\n",
            "    *   Suggestion: Include proper citations and references to credible sources to support the claims made in the report.\n",
            "\n",
            "7.  **Conclusion Section:**\n",
            "    *   The conclusion section is brief and lacks a clear summary of the report's main points.\n",
            "    *   Suggestion: Include a concise summary of the report's key findings and recommendations for future research or implementation.\n",
            "\n",
            "**Actionable Suggestions:**\n",
            "\n",
            "1.  **Revise the Introduction:** Provide a clear definition of key terms and a concise overview of the report's content.\n",
            "2.  **Add Quantitative Data:** Include specific numbers, percentages, or metrics to demonstrate the effectiveness of DeepSeek.\n",
            "3.  **Simplify Technical Concepts:** Use simple language and provide examples or analogies to explain complex concepts.\n",
            "4.  **Balance Evaluation:** Include a critical evaluation of DeepSeek's strengths and weaknesses to provide a more comprehensive understanding.\n",
            "5.  **Proper Citation:** Include proper citations and references to credible sources to support the claims made in the report.\n",
            "6.  **Revise the Conclusion:** Include a concise summary of the report's key findings and recommendations for future research or implementation.\n",
            "\n",
            "**Overall Rating:** 7.5/10\n",
            "\n",
            "The report provides a comprehensive overview of DeepSeek's architecture, benefits, and applications. However, it requires improvement in terms of clarity, accuracy, and overall quality. By addressing the suggested flaws and incorporating actionable recommendations, the report can be strengthened and made more effective.\n",
            "--- ü§î CHECKING CRITIQUE ---\n",
            "--- Critique requires revision. Looping back. ---\n",
            "--- üîÑ INVOKING REVISER ---\n",
            "--- Draft Revised ---\n",
            "--- üßê INVOKING CRITIQUE ---\n",
            "Critique Received: **Overall Assessment:**\n",
            "\n",
            "The research report draft on DeepSeek is well-structured and provides a comprehensive overview of the model architecture, its key benefits, potential applications, and real-world implementations. However, there are some areas that require improvement to enhance the report's clarity, accuracy, and overall quality.\n",
            "\n",
            "**Flaws and Suggestions for Improvement:**\n",
            "\n",
            "1.  **Lack of Clear Definition of Key Terms:** While the report defines key terms such as MoE Architecture, Dynamic Parameter Selection, and Multi-head Latent Attention (MLA), it would be beneficial to provide more detailed explanations and examples to ensure readers understand these concepts.\n",
            "2.  **Insufficient Quantitative Data:** The report provides some quantitative data, but it would be more effective to include more specific metrics, such as accuracy rates, processing times, and resource utilization, to demonstrate the effectiveness of DeepSeek.\n",
            "3.  **Limited Discussion of Challenges and Limitations:** The report focuses primarily on the benefits and potential applications of DeepSeek, but it would be valuable to discuss potential challenges, limitations, and areas for future research to provide a more balanced view.\n",
            "4.  **Inconsistent Formatting and Style:** The report's formatting and style are inconsistent, with some sections using bullet points and others using numbered lists. It would be beneficial to maintain a consistent formatting style throughout the report.\n",
            "5.  **Lack of Transparency in Methodology:** The report does not provide sufficient information about the methodology used to develop and evaluate DeepSeek, which could be a concern for readers who want to replicate the results or understand the underlying assumptions.\n",
            "6.  **References and Citations:** The report includes references and citations, but it would be more effective to use a consistent citation style throughout the report and to provide a separate bibliography or reference list.\n",
            "7.  **Conclusion and Recommendations:** The report's conclusion and recommendations are brief and do not provide a clear summary of the key findings and implications. It would be beneficial to expand on these sections to provide a more comprehensive overview of the report's results and recommendations.\n",
            "\n",
            "**Actionable Suggestions:**\n",
            "\n",
            "1.  **Revise the Definition of Key Terms:** Provide more detailed explanations and examples of key terms, such as MoE Architecture, Dynamic Parameter Selection, and Multi-head Latent Attention (MLA), to ensure readers understand these concepts.\n",
            "2.  **Include More Quantitative Data:** Provide more specific metrics, such as accuracy rates, processing times, and resource utilization, to demonstrate the effectiveness of DeepSeek.\n",
            "3.  **Discuss Challenges and Limitations:** Include a discussion of potential challenges, limitations, and areas for future research to provide a more balanced view of DeepSeek.\n",
            "4.  **Maintain Consistent Formatting and Style:** Use a consistent formatting style throughout the report, such as bullet points or numbered lists, to improve readability.\n",
            "5.  **Provide Transparency in Methodology:** Include more information about the methodology used to develop and evaluate DeepSeek, such as data sources, experimental design, and evaluation metrics.\n",
            "6.  **Use Consistent Citation Style:** Use a consistent citation style throughout the report and provide a separate bibliography or reference list.\n",
            "7.  **Expand Conclusion and Recommendations:** Provide a more comprehensive overview of the report's results and recommendations in the conclusion and recommendations sections.\n",
            "\n",
            "**Overall Rating:**\n",
            "\n",
            "Based on the assessment, I would rate the research report draft on DeepSeek as 7 out of 10. While the report provides a comprehensive overview of the model architecture and its potential applications, it requires improvement in areas such as clarity, accuracy, and transparency. With revisions to address these areas, the report has the potential to be a valuable resource for readers interested in DeepSeek and its applications.\n",
            "--- ü§î CHECKING CRITIQUE ---\n",
            "--- Critique requires revision. Looping back. ---\n",
            "--- üîÑ INVOKING REVISER ---\n",
            "--- Draft Revised ---\n",
            "--- üßê INVOKING CRITIQUE ---\n",
            "Critique Received: **Overall Assessment:**\n",
            "\n",
            "The research report draft on DeepSeek is well-structured and provides a comprehensive overview of the model architecture, its benefits, and potential applications. However, there are some areas that require improvement to enhance the report's clarity, accuracy, and overall quality.\n",
            "\n",
            "**Flaws and Suggestions for Improvement:**\n",
            "\n",
            "1.  **Factual Inaccuracies:**\n",
            "\n",
            "    *   The report states that DeepSeek matches the performance of OpenAI's o1 at 10% of the cost. However, this claim requires further verification and evidence to support its accuracy.\n",
            "    *   The report mentions that DeepSeek has been successfully applied in finance, but it would be beneficial to provide more specific examples and case studies to demonstrate its effectiveness in this industry.\n",
            "2.  **Logical Gaps:**\n",
            "\n",
            "    *   The report discusses the challenges and limitations of DeepSeek, but it would be helpful to provide more detailed explanations and potential solutions to address these issues.\n",
            "    *   The report mentions the importance of data quality in training and fine-tuning DeepSeek models, but it would be beneficial to provide more information on how to ensure high-quality data and mitigate potential biases.\n",
            "3.  **Poor Structure:**\n",
            "\n",
            "    *   The report jumps abruptly from discussing the key benefits of DeepSeek to its potential applications. It would be beneficial to provide a smoother transition between these sections to improve the report's flow and coherence.\n",
            "    *   The report could benefit from a more detailed introduction that provides a clear overview of the research question, objectives, and significance of the study.\n",
            "4.  **Lack of Transparency:**\n",
            "\n",
            "    *   The report could benefit from more transparency in its methodology, including the development and evaluation of DeepSeek.\n",
            "    *   The report mentions the use of quantitative data to demonstrate the effectiveness of DeepSeek, but it would be beneficial to provide more detailed information on the data collection, analysis, and interpretation methods used.\n",
            "5.  **Style and Formatting:**\n",
            "\n",
            "    *   The report could benefit from a more consistent citation style throughout the document.\n",
            "    *   The report could benefit from a more detailed and comprehensive bibliography or reference list.\n",
            "\n",
            "**Actionable Suggestions:**\n",
            "\n",
            "1.  **Verify Factual Claims:** Provide evidence and references to support the claim that DeepSeek matches the performance of OpenAI's o1 at 10% of the cost.\n",
            "2.  **Provide More Specific Examples:** Include more specific examples and case studies to demonstrate the effectiveness of DeepSeek in finance and other industries.\n",
            "3.  **Address Logical Gaps:** Provide more detailed explanations and potential solutions to address the challenges and limitations of DeepSeek.\n",
            "4.  **Improve Structure:** Smoothly transition between sections and provide a more detailed introduction to improve the report's flow and coherence.\n",
            "5.  **Increase Transparency:** Provide more information on the methodology, including the development and evaluation of DeepSeek.\n",
            "6.  **Consistent Citation Style:** Use a consistent citation style throughout the report.\n",
            "7.  **Comprehensive Bibliography:** Provide a more detailed and comprehensive bibliography or reference list.\n",
            "\n",
            "**Recommendations for Future Research:**\n",
            "\n",
            "1.  **Further Optimization:** Investigate ways to further optimize DeepSeek's architecture for improved performance and efficiency.\n",
            "2.  **Expanded Applications:** Explore new applications for DeepSeek in emerging industries, such as autonomous vehicles and smart cities.\n",
            "3.  **Scalability:** Develop strategies to scale DeepSeek's architecture for large-scale deployments and enterprise adoption.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The research report draft on DeepSeek provides a comprehensive overview of the model architecture, its benefits, and potential applications. However, there are some areas that require improvement to enhance the report's clarity, accuracy, and overall quality. By addressing these flaws and suggestions for improvement, the report can be strengthened and provide a more comprehensive understanding of DeepSeek's capabilities and limitations.\n",
            "--- ü§î CHECKING CRITIQUE ---\n",
            "--- Reached max revisions (3). Finishing workflow. ---\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Revised Research Report: DeepSeek**\n",
              "\n",
              "**Introduction**\n",
              "\n",
              "DeepSeek is a cutting-edge model architecture designed to strike a balance between performance and resource usage. Developed with the aim of making AI more accessible and cost-effective, DeepSeek has garnered significant attention in recent times. This report provides a detailed overview of DeepSeek's architecture, key benefits, potential applications, and real-world implementations.\n",
              "\n",
              "**Key Terms and Definitions:**\n",
              "\n",
              "For the purpose of this report, the following key terms are defined:\n",
              "\n",
              "*   **MoE Architecture (Multi-Expert Architecture):** A type of neural network architecture that selectively activates different subsets of parameters for different inputs. This approach enables the model to adapt to different inputs and optimize its performance.\n",
              "*   **Dynamic Parameter Selection:** A feature of MoE architecture that dynamically selects a subset of experts for each token, reducing the number of parameters used. This feature enables the model to optimize its performance and reduce computational costs.\n",
              "*   **Multi-head Latent Attention (MLA):** A key component of DeepSeek's architecture that enables efficient inference and reduces computational costs by processing inputs in parallel. MLA allows the model to process multiple inputs simultaneously, making it more scalable and efficient.\n",
              "\n",
              "**Body**\n",
              "\n",
              "### Architecture\n",
              "\n",
              "DeepSeek's architecture is built around several key features that enable its efficiency and scalability:\n",
              "\n",
              "#### 1. **MoE Architecture**\n",
              "\n",
              "DeepSeek employs a MoE architecture, which selectively activates different subsets of parameters for different inputs. This approach reduces computational costs by only using the necessary parameters for each input.\n",
              "\n",
              "#### 2. **Dynamic Parameter Selection**\n",
              "\n",
              "DeepSeek dynamically selects a subset of experts for each token, further reducing the number of parameters used. This feature enables the model to adapt to different inputs and optimize its performance.\n",
              "\n",
              "#### 3. **Efficient Scaling**\n",
              "\n",
              "DeepSeek's architecture allows the model to scale efficiently while keeping inference more resource-efficient. This is achieved through the use of MLA, which enables the model to process inputs in parallel.\n",
              "\n",
              "#### 4. **Multi-head Latent Attention (MLA)**\n",
              "\n",
              "MLA is a key component of DeepSeek's architecture, enabling efficient inference and reducing computational costs. MLA allows the model to process inputs in parallel, making it more scalable and efficient.\n",
              "\n",
              "#### 5. **DeepSeekMoE**\n",
              "\n",
              "DeepSeekMoE is a variant of the MoE architecture specifically designed for cost-effective training. This approach enables the model to be trained on a smaller dataset while maintaining its performance.\n",
              "\n",
              "### Key Benefits\n",
              "\n",
              "DeepSeek offers several key benefits that make it an attractive solution for enterprises and developers:\n",
              "\n",
              "#### 1. **Cost Efficiency**\n",
              "\n",
              "DeepSeek matches the performance of OpenAI's o1 at 10% of the cost, redefining the economics of AI. This makes it an attractive solution for enterprises looking to reduce their AI-related expenses.\n",
              "\n",
              "#### 2. **Customization**\n",
              "\n",
              "Enterprises can securely train and fine-tune DeepSeek models on proprietary data, ensuring industry relevance. This feature enables organizations to tailor the model to their specific needs and requirements.\n",
              "\n",
              "#### 3. **Open-Source Accessibility**\n",
              "\n",
              "Developers and organizations can use DeepSeek's models without hefty infrastructure investments, sparking innovation across industries. This open-source approach makes it easier for developers to access and utilize the model.\n",
              "\n",
              "### Potential Applications\n",
              "\n",
              "DeepSeek has a wide range of potential applications across various industries:\n",
              "\n",
              "#### 1. **Creative Industries**\n",
              "\n",
              "DeepSeek can provide creative suggestions or automate parts of creative processes, enhancing productivity. This feature makes it an attractive solution for industries such as advertising, media, and entertainment.\n",
              "\n",
              "#### 2. **Explainable AI (XAI)**\n",
              "\n",
              "DeepSeek emphasizes transparency in its AI processes, providing visibility into decisions. This feature makes it an attractive solution for industries where explainability is crucial, such as finance and healthcare.\n",
              "\n",
              "#### 3. **Modularity and Adaptability**\n",
              "\n",
              "DeepSeek's AI system is flexible, allowing customers to modify and alter the models to meet their needs. This feature makes it an attractive solution for industries where adaptability is crucial, such as manufacturing and logistics.\n",
              "\n",
              "### Real-World Applications\n",
              "\n",
              "DeepSeek AI has been successfully applied in various industries, including finance, where it has revolutionized banking and financial services. Its transformative potential extends to healthcare, education, and manufacturing, among others.\n",
              "\n",
              "### Founding and Leadership\n",
              "\n",
              "DeepSeek was founded by Liang Wenfeng, a renowned Chinese entrepreneur and businessman with a strong background in finance and AI. Under his leadership, DeepSeek has made significant strides in developing and implementing its AI solutions.\n",
              "\n",
              "**Quantitative Data:**\n",
              "\n",
              "To demonstrate the effectiveness of DeepSeek, the following quantitative data is presented:\n",
              "\n",
              "*   **Cost Efficiency:** DeepSeek matches the performance of OpenAI's o1 at 10% of the cost, resulting in a 90% reduction in AI-related expenses.\n",
              "*   **Customization:** Enterprises can securely train and fine-tune DeepSeek models on proprietary data, achieving an average accuracy of 95% in industry-specific applications.\n",
              "*   **Open-Source Accessibility:** DeepSeek's open-source approach has led to a 300% increase in developer adoption and innovation across industries.\n",
              "\n",
              "**Challenges and Limitations:**\n",
              "\n",
              "While DeepSeek offers several key benefits, there are some challenges and limitations to consider:\n",
              "\n",
              "*   **Scalability:** DeepSeek's architecture may not be suitable for large-scale deployments, requiring further optimization and development.\n",
              "*   **Data Quality:** The quality of the data used to train and fine-tune DeepSeek models can significantly impact its performance and accuracy.\n",
              "*   **Explainability:** While DeepSeek emphasizes transparency in its AI processes, there may be limitations to its explainability, particularly in complex applications.\n",
              "\n",
              "**Conclusion**\n",
              "\n",
              "DeepSeek is a cutting-edge model architecture that offers several key benefits, including cost efficiency, customization, and open-source accessibility. Its potential applications are vast, spanning various industries such as creative industries, explainable AI, and modularity and adaptability. With its successful implementation in finance and other industries, DeepSeek is poised to revolutionize the way we approach AI and its applications.\n",
              "\n",
              "**Recommendations for Future Research:**\n",
              "\n",
              "1.  **Further Optimization:** Investigate ways to further optimize DeepSeek's architecture for improved performance and efficiency.\n",
              "2.  **Expanded Applications:** Explore new applications for DeepSeek in emerging industries, such as autonomous vehicles and smart cities.\n",
              "3.  **Scalability:** Develop strategies to scale DeepSeek's architecture for large-scale deployments and enterprise adoption.\n",
              "\n",
              "**References:**\n",
              "\n",
              "*   [1] Liang, W. (2022). DeepSeek: A Cost-Effective Model Architecture for AI Applications. Journal of Artificial Intelligence Research, 123(1), 1-15.\n",
              "*   [2] Wang, X. (2020). Explainable AI: A Survey of Techniques and Applications. IEEE Transactions on Neural Networks and Learning Systems, 31(1), 1-15.\n",
              "\n",
              "**Appendix:**\n",
              "\n",
              "*   **Glossary:** A list of key terms and definitions used in this report.\n",
              "*   **Technical Details:** Additional technical information and specifications for DeepSeek's architecture and implementation.\n",
              "\n",
              "**Changes Made:**\n",
              "\n",
              "*   Revised the definition of key terms to provide more detailed explanations and examples.\n",
              "*   Included more quantitative data to demonstrate the effectiveness of DeepSeek.\n",
              "*   Discussed challenges and limitations to provide a more balanced view of DeepSeek.\n",
              "*   Maintained consistent formatting and style throughout the report.\n",
              "*   Provided transparency in methodology to include more information about the development and evaluation of DeepSeek.\n",
              "*   Used consistent citation style throughout the report and provided a separate bibliography or reference list.\n",
              "*   Expanded the conclusion and recommendations to provide a more comprehensive overview of the report's results and recommendations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 8. RUNNING THE MULTI-AGENT SYSTEM\n",
        "# ==============================================================================\n",
        "\n",
        "# Define the research topic and initialize the graph state.\n",
        "# This is the starting input passed to the LangGraph app.\n",
        "inputs = {\n",
        "    \"research_topic\": \"DeepSeek\",  # The topic to be investigated\n",
        "    \"revision_number\": 0               # Tracks how many times the draft has been revised\n",
        "}\n",
        "\n",
        "# Executes the entire workflow from the beginning to get the final state.\n",
        "# The `recursion_limit` sets the maximum number of times the graph can recurse (i.e., loop through nodes) to prevent infinite cycles.\n",
        "final_state = app.invoke(inputs, {\"recursion_limit\": 25})\n",
        "\n",
        "\n",
        "# Display the final draft as formatted Markdown.\n",
        "display(Markdown(final_state['draft']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz3ssESEtFX9"
      },
      "source": [
        "Here, the method stream is used instead of invoke for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMvieIF-tKVn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- üß† INVOKING ENHANCED PLANNER ---\n",
            "Performing preliminary search...\n",
            "Generating a focused plan based on preliminary findings...\n",
            "--- Generated Plan ---\n",
            "1. DeepSeek LLM architecture and design decisions\n",
            "2. Comparison of DeepSeek LLM performance with Llama 2 and other open-source LLMs\n",
            "3. Cost-effectiveness and training efficiency of DeepSeek LLM models\n",
            "4. Inference and deployment frameworks for DeepSeek LLM models, including LMDeploy and TRT-LLM\n",
            "5. Methodology and results of distilling reasoning capabilities from DeepSeek R1 series models into standard LLMs\n",
            "--------------------\n",
            "--- Output from node: planner ---\n",
            "{'plan': ['DeepSeek LLM architecture and design decisions', 'Comparison of DeepSeek LLM performance with Llama 2 and other open-source LLMs', 'Cost-effectiveness and training efficiency of DeepSeek LLM models', 'Inference and deployment frameworks for DeepSeek LLM models, including LMDeploy and TRT-LLM', 'Methodology and results of distilling reasoning capabilities from DeepSeek R1 series models into standard LLMs']}\n",
            "\n",
            "---\n",
            "\n",
            "--- üîç INVOKING SEARCHER ---\n",
            "Searching for: 'DeepSeek LLM architecture and design decisions'\n",
            "Found 2 documents for this step (unique documents so far: 2).\n",
            "Searching for: 'Comparison of DeepSeek LLM performance with Llama 2 and other open-source LLMs'\n",
            "Found 2 documents for this step (unique documents so far: 4).\n",
            "Searching for: 'Cost-effectiveness and training efficiency of DeepSeek LLM models'\n",
            "Found 2 documents for this step (unique documents so far: 6).\n",
            "Searching for: 'Inference and deployment frameworks for DeepSeek LLM models, including LMDeploy and TRT-LLM'\n",
            "Found 2 documents for this step (unique documents so far: 8).\n",
            "Searching for: 'Methodology and results of distilling reasoning capabilities from DeepSeek R1 series models into standard LLMs'\n",
            "Found 2 documents for this step (unique documents so far: 10).\n",
            "--- Total unique documents retrieved: 10 ---\n",
            "--- Output from node: searcher ---\n",
            "{'retrieved_docs': ['From DeepSeek V3 to Kimi 2, in my new article, I take a closer look at eight of the main open-weight LLM architectures released this year. (I focused specifically on architecture design choices.) The topics include: - DeepSeek V3 / R1: pairs Multi‚ÄëHead Latent Attention (MLA) with a 256‚Äëexpert MoE - OLMo 2: New norm layer placement and QK-Norm - Gemma 3:sliding‚Äëwindow GQA and more norm layers - Gemma 3n: introduces Per‚ÄëLayer Embedding and MatFormer slicing - Mistral Small 3.1: trims layer count', 'Mixture-of-Experts Architecture: Instead of using a dense architecture where all parameters are active for every task, DeepSeek‚Äôs design activates only a small, specialized subset of parameters (the ‚Äúexperts‚Äù) for each input. This selective activation enhances performance in specialized domains like math and coding while managing resources efficiently. [...] Wider Accessibility for Developers: With its cost-effective, open-source design, DeepSeek‚Äôs technology enables startups and research groups with limited resources to access and build upon advanced AI models.\\n   Enhanced Application Development: The efficient and flexible architecture is ideal for creating applications such as smart chatbots, intelligent coding assistants, and educational tools, as well as systems for complex problem solving. [...] Conclusion\\n\\nDeepSeek is redefining what is possible in AI by demonstrating that high-performing language models can be built through smart engineering and efficient resource management rather than massive hardware investments. Its innovative use of techniques like Multi-Head Latent Attention, mixture-of-experts architectures, and multi-token prediction not only boosts performance in specialized tasks but also makes training and deployment far more cost-effective.', 'The successor to LLaMA (henceforce \"Llama 1\"), Llama 2 was trained on 40% more data, has double the context length, and was tuned on a large dataset of human preferences (over 1 million such annotations) to ensure helpfulness and safety. It outperforms other open source models on both natural language understanding datasets as well as in head-to-head face-offs.\\n\\n  \\n\\nInitial release: 2023-07-18\\n\\n#### Reference\\n\\n#### Further Reading [...] #####\\n\\n# DeepSeek vs. Llama 2\\n\\n#### LLM Comparison\\n\\n  \\n\\n#### DeepSeek\\n\\n#### Overview\\n\\nDeepSeek currently offers V3 and R1 models, both of which are highly efficient and performant. V3 is comparable to models such as Anthropic\\'s Sonnet 3.5, while R1 is comparable to models such as OpenAI\\'s o1. [...] Looking for an LLM API/SDK that works out of the box? No prompts or ad hoc guardrails.\\n\\nSapling API  \\n More Comparisons\\n\\n| DeepSeek | Llama 2 |\\n --- |\\n| Products & Features |\\n| Instruct Models |\\n| Coding Capability |\\n| Customization |\\n| Finetuning |\\n| Open Source |\\n| License | MIT | Custom (Commercial OK) |\\n| Model Sizes | 67B, 671B | 7B, 13B, 70B |\\n\\n### Other DeepSeek Comparisons', 'Open-source LLMs on benchmarks like lmarena.ai typically have shown inferior performance compared with proprietary state-of-the art LLMs such as GPT-4o. Nonetheless, open-source LLMs have caught up as new models such as Llama 3.1 or Mistral Large 2 demonstrate substantial improvements12.\"). Recent advancements in LLM have seen the emergence of state-of-the-art open-source models such as DeepSeek-V3 and the development of explicit reasoning models such as Gemini-2.0 Flash Thinking Experimental', 'Conclusion\\n\\nDeepSeek is redefining what is possible in AI by demonstrating that high-performing language models can be built through smart engineering and efficient resource management rather than massive hardware investments. Its innovative use of techniques like Multi-Head Latent Attention, mixture-of-experts architectures, and multi-token prediction not only boosts performance in specialized tasks but also makes training and deployment far more cost-effective. [...] Mixed-Precision Training: By using lower-bit representations (such as 8-bit floating point numbers) instead of standard 32-bit numbers during training, DeepSeek reduces memory usage and speeds up calculations, all while maintaining accuracy.\\n   Efficient Communication Techniques: Overlapping computation with communication minimizes data transfer delays between GPUs, making the training process both faster and more cost-effective. [...] High-Quality Models: Its models perform at levels comparable to those developed by major Western companies, especially in tasks such as mathematical reasoning and programming.\\n   Cost-Effective Approach: Rather than relying on vast amounts of hardware, DeepSeek achieves impressive results with significantly lower computational resources.', '> Bigger is no longer always smarter. DeepSeek demonstrates an alternative path to efficient model training than the current arm‚Äôs race among hyperscalers by significantly increasing the data quality and improving the model architecture. DeepSeek is now the lowest cost of LLM manufacturing, allowing frontier AI performance at a fraction of the cost with 9-13x lower price on output tokens vs. GPT-4o and Claude 3.5.\\n>', 'Use rtp-llm to deploy Qwen inference services\\n           Use LMDeploy to deploy the Qwen model inference service\\n           Use Triton to deploy Qwen inference services\\n           Use TGI to deploy Qwen inference services\\n           Use TensorRT-LLM to deploy a Qwen2 model as an inference service [...] vLLM\\n\\nvLLM is a high-performance and easy-to-use LLM inference service framework. vLLM supports most commonly used LLMs, including Qwen models. vLLM is powered by technologies such as PagedAttention optimization, continuous batching, and model quantification to significantly improve the inference efficiency of LLMs. For more information about the vLLM framework, see vLLM GitHub repository.\\n\\n   Arena', 'LMI containers are a set of high-performance Docker containers purpose built for LLM inference. With these containers, you can use high-performance open source inference libraries like vLLM, TensorRT-LLM, and Transformers NeuronX to deploy LLMs on SageMaker endpoints. These containers bundle together a model server with open source inference libraries to deliver an all-in-one LLM serving solution.\\n\\nLMI containers provide many features, including:', 'As shown in Table 5, simply distilling DeepSeek-R1‚Äôs outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-tion. Additionally, we found that applying RL [...] 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in ¬ß2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, [...] ‚Ä¢ Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform', 'To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in ¬ß2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and [...] Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform [...] We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a)']}\n",
            "\n",
            "---\n",
            "\n",
            "--- üìù INVOKING SUMMARIZER ---\n",
            "Summarizing document 1/10\n",
            "Summarizing document 2/10\n",
            "Summarizing document 3/10\n"
          ]
        }
      ],
      "source": [
        "# Define the research topic and initialize the graph state.\n",
        "# This is the starting input passed to the LangGraph app.\n",
        "inputs = {\n",
        "    \"research_topic\": \"DeepSeek LLM\",  # The topic to be investigated\n",
        "    \"revision_number\": 0               # Tracks how many times the draft has been revised\n",
        "}\n",
        "# Run the graph using LangGraph's `.stream()` method.\n",
        "# This allows us to observe how each agent (node) updates the state step-by-step.\n",
        "for output in app.stream(inputs, {\"recursion_limit\": 25}):\n",
        "    for key, value in output.items():\n",
        "        print(f\"--- Output from node: {key} ---\")\n",
        "        # Print the updated part of the state returned by that node\n",
        "        print(value)\n",
        "        print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfd3fPznsC1m"
      },
      "source": [
        "#Conclusions and next steps\n",
        "\n",
        "In this notebook, we built a powerful and flexible autonomous research assistant by combining specialized LLM agents using LangGraph and the high-speed inference of Groq. Each agent focused on a distinct part of the workflow‚Äîsuch as planning, searching, summarizing, and revising‚Äîdemonstrating how modular agent design can replicate human-like research and writing processes. This architecture is not only functional but also highly extensible.  \n",
        "\n",
        "Next steps for improvement could include:\n",
        "\n",
        "-  Adding agents for fact-checking, source validation, or visualization.\n",
        "\n",
        "- Integrating long-term memory to maintain context across sessions.\n",
        "\n",
        "- Allowing user interaction for human-in-the-loop guidance or feedback.\n",
        "\n",
        "- Testing with different LLMs and tools to explore performance trade-offs.\n",
        "\n",
        "- Comparing and benchmarking this system with other well-known multi-agent frameworks such as CrewAI or Microsoft AutoGen."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
