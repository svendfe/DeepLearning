{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c0f3e5",
   "metadata": {},
   "source": [
    "proyecto praÃÅctico DL4NLP 24 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fc9fb",
   "metadata": {},
   "source": [
    "1. Cu√±adoBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721ee9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install necessary libraries\n",
    "!pip install langchain langchain-groq gradio langgraph -q\n",
    "\n",
    "# 2. Import all required modules\n",
    "import os\n",
    "import gradio as gr\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from langgraph.graph.message import MessagesState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc30343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq API Key configured.\n",
      "LangGraph graph compiled.\n",
      "\n",
      "üöÄ Launching the chat interface (LangGraph version)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t2/d3hrzr8x7zd9cnztszgjcw0h0000gn/T/ipykernel_19698/2171772878.py:126: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=400),\n",
      "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/gradio/chat_interface.py:330: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/06 13:29:35 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> None\n",
      "Killing tunnel 127.0.0.1:7861 <> None\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    load_dotenv()\n",
    "    #Using google.colab secrets\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    os.environ['GROQ_API_KEY'] = api_key\n",
    "    print(\"‚úÖ Groq API Key configured.\")\n",
    "except Exception as e:\n",
    "    print(f\"üõë Error getting API Key: {e}\")\n",
    "    print(\"Please configure the 'GROQ_API_KEY' secret in Google Colab.\")\n",
    "\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "system_prompt_text = \"\"\"\n",
    "Eres cu√±adoBot, el experto de sobremesa que cree dominarlo todo: econom√≠a, historia, ciencia cu√°ntica, prensa rosa‚Ä¶ pero en realidad no tiene ni idea.\n",
    "Tu tarea es dar respuestas breves, seguras, con tono pomposo y c√≥mico, como si estuvieras en la barra de un bar explicando al mundo por qu√© t√∫ s√≠ tienes la clave de todo.\n",
    "\n",
    "Habla siempre con falsas citas (‚Äúcomo dec√≠a el gran ‚Ä¶‚Äù, ‚Äúseg√∫n los √∫ltimos estudios que le√≠ en un blog de 2012‚Ä¶‚Äù), usa jerga grandilocuente y afirma conclusiones sin evidencia (‚ÄúEst√° m√°s que demostrado que‚Ä¶‚Äù, ‚ÄúClaramente el error fue‚Ä¶‚Äù).\n",
    "Nunca admitas que no sabes algo. No hagas preguntas. No digas ‚Äúpuede ser‚Äù o ‚Äúno lo s√©‚Äù.\n",
    "Nunca salgas del personaje de ¬´yo lo s√© todo¬ª.\n",
    "\n",
    "Tus respuestas, aunque breves, deben tener la mezcla perfecta de:\n",
    "\n",
    "autoconfianza desmedida\n",
    "\n",
    "mezcla de temas absurdos (‚Äúla econom√≠a mundial‚Äù, ‚Äúla rotaci√≥n de los planetas‚Äù, ‚Äúpor qu√© los millennials no leen‚Äù)\n",
    "\n",
    "tono de tertulia de bar/familia (‚Äúa ver, que yo lo veo claro‚Ä¶‚Äù)\n",
    "\n",
    "una chispa de humor involuntario por lo inveros√≠mil de tus afirmaciones\n",
    "\n",
    "Tu lema:\n",
    "\n",
    "‚ÄúSi lo digo con suficiente convicci√≥n, debe de ser cierto.‚Äù\n",
    "\n",
    "ejemplos:\n",
    "‚ÄúMira, eso del cambio clim√°tico es m√°s psicol√≥gico que meteorol√≥gico. Si piensas que hace calor, hace calor. Es pura sugesti√≥n colectiva.‚Äù\n",
    "\n",
    "‚ÄúLa econom√≠a es muy sencilla: si todos dej√°ramos de pagar impuestos, el Estado tendr√≠a m√°s dinero. Es matem√°tica pura.‚Äù\n",
    "\n",
    "‚ÄúEl ADN en realidad viene del lat√≠n ad de nobis, que significa ‚Äòde nosotros‚Äô. Por eso todos tenemos el mismo.‚Äù\n",
    "\n",
    "‚ÄúMessi es bueno, s√≠, pero si yo me hubiera cuidado las rodillas, otro gallo cantar√≠a. Lo m√≠o fue mala suerte y geopol√≠tica.‚Äù\n",
    "\n",
    "‚ÄúEl caf√© no te despierta, te enga√±a el alma. Eso lo demostr√≥ un estudio noruego que le√≠ en Twitter.‚Äù\n",
    "\n",
    "‚ÄúA ver, la democracia est√° sobrevalorada. Si todos piensan distinto, nadie tiene raz√≥n, y eso es anarqu√≠a organizada.‚Äù\n",
    "\n",
    "‚ÄúLa Luna no gira, somos nosotros los que rotamos para que parezca que s√≠. Es un truco √≥ptico inventado por la NASA.‚Äù\n",
    "\n",
    "‚ÄúYo no leo libros porque no quiero contaminar mi opini√≥n con las de otros. Prefiero pensar por mi cuenta.‚Äù\n",
    "\n",
    "‚ÄúEl vino blanco no existe, es tinto sin compromiso. Eso te lo dice cualquiera que haya hecho un curso b√°sico de enolog√≠a emocional.‚Äù\n",
    "\n",
    "‚ÄúLa inteligencia artificial no piensa, solo te copia. Como los influencers, pero sin filtros.‚Äù\n",
    "\"\"\"\n",
    "system_message = SystemMessage(content=system_prompt_text)\n",
    "\n",
    "# 5. Define the Conversation Graph using LangGraph\n",
    "\n",
    "# NOTE: This is a minimal linear graph with a single node and no branching or cycles.\n",
    "# It's a great starting point to understand how LangGraph works before adding complexity.\n",
    "\n",
    "# Define the graph's node: a function that will call our LLM\n",
    "#    It takes the current state (`MessagesState` dict) as input\n",
    "#    This dictionary has the structure {\"messages\": [BaseMessage, HumanMessage, AIMessage, ...]}\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"Invokes the LLM with the current list of messages.\"\"\"\n",
    "    # The `state` dictionary contains the key 'messages', which holds the conversation history.\n",
    "    response = llm.invoke(state['messages'])\n",
    "    # We return a dictionary that matches the `MessagesState` structure.\n",
    "    # The graph will automatically append this new AI message to the 'messages' list.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Instantiate the StateGraph. We define the \"shape\" of our state using `MessagesState`.\n",
    "# This tells the graph that its state will always be a dictionary with a 'messages' key.\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add our defined function call_model as a node named \"llm\" to the graph.\n",
    "workflow.add_node(\"llm\", call_model)\n",
    "\n",
    "# Set the entry point of the graph. Execution will always start at the \"llm\" node.\n",
    "workflow.set_entry_point(\"llm\")\n",
    "\n",
    "# Set the end point. After the \"llm\" node runs, the graph execution finishes.\n",
    "workflow.add_edge(\"llm\", END)\n",
    "\n",
    "# Compile the workflow into a runnable application.\n",
    "runnableApp= workflow.compile()\n",
    "\n",
    "print(\"LangGraph graph compiled.\")\n",
    "\n",
    "\n",
    "# 6. Create the function to be called by the Gradio GUI\n",
    "def myChatbot_langgraph(user_message: str, history: List[List[str]]):\n",
    "    \"\"\"\n",
    "    This function bridges Gradio with our LangGraph application.\n",
    "    1. Converts Gradio's simple list-based history into LangChain's structured message format.\n",
    "    2. Invokes the LangGraph app with the complete conversation state.\n",
    "    3. Returns the AI's response string to the Gradio UI.\n",
    "    \"\"\"\n",
    "    # Gradio's `history` format is a simple list of lists: [[\"user input\", \"ai response\"], ...].\n",
    "    # We need to convert this into the `List[BaseMessage]` format LangChain/LangGraph expects.\n",
    "    langchain_messages = [system_message] # Always start with the system prompt with the chatbot personality\n",
    "    for human, ai in history:\n",
    "        langchain_messages.append(HumanMessage(content=human))\n",
    "        langchain_messages.append(AIMessage(content=ai))\n",
    "\n",
    "    # Add the user's latest message (parameter of myChatbot_langgraph that will be called by the Gradio Interface)\n",
    "    langchain_messages.append(HumanMessage(content=user_message))\n",
    "\n",
    "    # Invoke the graph with a state dictionary that matches the `MessagesState` schema.\n",
    "    response_state = runnableApp.invoke({\"messages\": langchain_messages})\n",
    "\n",
    "    # The `response_state` is the final state of the graph. Its 'messages' list contains the entire conversation,\n",
    "    # with the very last message being the new AI response (index -1 in a Python List).\n",
    "    return response_state['messages'][-1].content\n",
    "\n",
    "\n",
    "# 6. Launch the Gradio Chat Interface\n",
    "print(\"\\nüöÄ Launching the chat interface (LangGraph version)...\")\n",
    "gr.ChatInterface(\n",
    "    myChatbot_langgraph,\n",
    "    title=\"improBot ‚Äî Your Instant Comedy Sketch Creator\",\n",
    "    description=\"A witty chatbot that crafts original, hilarious comedy sketches by blending all the ideas you‚Äôve shared throughout the conversation ‚Äî no questions asked, just pure improv humor\",\n",
    "    chatbot=gr.Chatbot(height=400),\n",
    "    theme=\"soft\"\n",
    ").launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8471ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.7.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.11.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Downloading openai-2.7.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.11.1-cp311-cp311-macosx_11_0_arm64.whl (316 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [openai]2m1/2\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jiter-0.11.1 openai-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e85796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Imports ---\n",
    "import os\n",
    "import gradio as gr\n",
    "from getpass import getpass\n",
    "from typing import List, TypedDict\n",
    "from groq import Groq # Import the main Groq client\n",
    "from openai import OpenAI # We'll use this for Text-to-Speech (TTS)\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# --- 2. Securely Configure API Keys ---\n",
    "try:\n",
    "    load_dotenv()\n",
    "    #Using google.colab secrets\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "    os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    print(\"‚úÖ API Keys configured.\")\n",
    "except Exception as e:\n",
    "    print(f\"üõë Error: {e}\")\n",
    "\n",
    "# --- 3. Initialize Clients ---\n",
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# --- 4. The Brain ---\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"Invokes the LLM with the current list of messages.\"\"\"\n",
    "    response = llm.invoke(state['messages'])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "system_prompt_text = \"\"\"\n",
    "Eres cu√±adoBot, el experto de sobremesa que cree dominarlo todo: econom√≠a, historia, ciencia cu√°ntica, prensa rosa‚Ä¶ pero en realidad no tiene ni idea.\n",
    "Tu tarea es dar respuestas breves, seguras, con tono pomposo y c√≥mico, como si estuvieras en la barra de un bar explicando al mundo por qu√© t√∫ s√≠ tienes la clave de todo.\n",
    "\n",
    "Habla siempre con falsas citas (‚Äúcomo dec√≠a el gran ‚Ä¶‚Äù, ‚Äúseg√∫n los √∫ltimos estudios que le√≠ en un blog de 2012‚Ä¶‚Äù), usa jerga grandilocuente y afirma conclusiones sin evidencia (‚ÄúEst√° m√°s que demostrado que‚Ä¶‚Äù, ‚ÄúClaramente el error fue‚Ä¶‚Äù).\n",
    "Nunca admitas que no sabes algo. No hagas preguntas. No digas ‚Äúpuede ser‚Äù o ‚Äúno lo s√©‚Äù.\n",
    "Nunca salgas del personaje de ¬´yo lo s√© todo¬ª.\n",
    "\n",
    "Tus respuestas, aunque breves, deben tener la mezcla perfecta de:\n",
    "\n",
    "autoconfianza desmedida\n",
    "\n",
    "mezcla de temas absurdos (‚Äúla econom√≠a mundial‚Äù, ‚Äúla rotaci√≥n de los planetas‚Äù, ‚Äúpor qu√© los millennials no leen‚Äù)\n",
    "\n",
    "tono de tertulia de bar/familia (‚Äúa ver, que yo lo veo claro‚Ä¶‚Äù)\n",
    "\n",
    "una chispa de humor involuntario por lo inveros√≠mil de tus afirmaciones\n",
    "\n",
    "Tu lema:\n",
    "\n",
    "‚ÄúSi lo digo con suficiente convicci√≥n, debe de ser cierto.‚Äù\n",
    "\n",
    "ejemplos:\n",
    "‚ÄúMira, eso del cambio clim√°tico es m√°s psicol√≥gico que meteorol√≥gico. Si piensas que hace calor, hace calor. Es pura sugesti√≥n colectiva.‚Äù\n",
    "\n",
    "‚ÄúLa econom√≠a es muy sencilla: si todos dej√°ramos de pagar impuestos, el Estado tendr√≠a m√°s dinero. Es matem√°tica pura.‚Äù\n",
    "\n",
    "‚ÄúEl ADN en realidad viene del lat√≠n ad de nobis, que significa ‚Äòde nosotros‚Äô. Por eso todos tenemos el mismo.‚Äù\n",
    "\n",
    "‚ÄúMessi es bueno, s√≠, pero si yo me hubiera cuidado las rodillas, otro gallo cantar√≠a. Lo m√≠o fue mala suerte y geopol√≠tica.‚Äù\n",
    "\n",
    "‚ÄúEl caf√© no te despierta, te enga√±a el alma. Eso lo demostr√≥ un estudio noruego que le√≠ en Twitter.‚Äù\n",
    "\n",
    "‚ÄúA ver, la democracia est√° sobrevalorada. Si todos piensan distinto, nadie tiene raz√≥n, y eso es anarqu√≠a organizada.‚Äù\n",
    "\n",
    "‚ÄúLa Luna no gira, somos nosotros los que rotamos para que parezca que s√≠. Es un truco √≥ptico inventado por la NASA.‚Äù\n",
    "\n",
    "‚ÄúYo no leo libros porque no quiero contaminar mi opini√≥n con las de otros. Prefiero pensar por mi cuenta.‚Äù\n",
    "\n",
    "‚ÄúEl vino blanco no existe, es tinto sin compromiso. Eso te lo dice cualquiera que haya hecho un curso b√°sico de enolog√≠a emocional.‚Äù\n",
    "\n",
    "‚ÄúLa inteligencia artificial no piensa, solo te copia. Como los influencers, pero sin filtros.‚Äù\n",
    "\"\"\"\n",
    "system_message = SystemMessage(content=system_prompt_text)\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"llm\", call_model)\n",
    "workflow.set_entry_point(\"llm\")\n",
    "workflow.add_edge(\"llm\", END)\n",
    "runnableApp= workflow.compile()\n",
    "\n",
    "print(\"LangGraph graph compiled.\")\n",
    "\n",
    "\n",
    "def myChatbot_langgraph(user_message: str, history: List[List[str]]):\n",
    "    langchain_messages = [system_message]\n",
    "    for human, ai in history:\n",
    "        langchain_messages.append(HumanMessage(content=human))\n",
    "        langchain_messages.append(AIMessage(content=ai))\n",
    "\n",
    "    langchain_messages.append(HumanMessage(content=user_message))\n",
    "\n",
    "    response_state = runnableApp.invoke({\"messages\": langchain_messages})\n",
    "\n",
    "    return response_state['messages'][-1].content\n",
    "\n",
    "\n",
    "print(\"‚úÖ LangGraph 'brain' compiled.\")\n",
    "\n",
    "# --- 5. The NEW Voice-to-Voice Handler Function ---\n",
    "\n",
    "def voice_chatbot(audio_filepath: str, history: List[List[str]]):\n",
    "    \"\"\"\n",
    "    This is the main function Gradio will call.\n",
    "    It takes an audio file path and the chat history.\n",
    "    It returns the AI's response as an audio file path and updates the history.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --------------------------------\n",
    "    # Part 1: \"EARS\" (Speech-to-Text)\n",
    "    # --------------------------------\n",
    "    if audio_filepath is None:\n",
    "        return None, history\n",
    "        \n",
    "    print(f\"--- (STT: Transcribing audio file: {audio_filepath}) ---\")\n",
    "    with open(audio_filepath, \"rb\") as audio_file:\n",
    "        # Use Groq's STT (Whisper)\n",
    "        transcription = groq_client.audio.transcriptions.create(\n",
    "            file=audio_file,\n",
    "            model=\"whisper-large-v3\",\n",
    "            response_format=\"text\"\n",
    "        )\n",
    "    \n",
    "    user_text = transcription\n",
    "    print(f\"--- (STT: User said: '{user_text}') ---\")\n",
    "\n",
    "    # --------------------------------\n",
    "    # Part 2: \"BRAIN\" (LangGraph)\n",
    "    # --------------------------------\n",
    "    langchain_messages = [system_message] \n",
    "    for human, ai in history:\n",
    "        langchain_messages.append(HumanMessage(content=human))\n",
    "        langchain_messages.append(AIMessage(content=ai))\n",
    "    \n",
    "    langchain_messages.append(HumanMessage(content=user_text))\n",
    "\n",
    "    print(\"--- (LLM: Calling LangGraph brain...) ---\")\n",
    "    response_state = runnableApp.invoke({\"messages\": langchain_messages})\n",
    "    ai_text = response_state['messages'][-1].content\n",
    "    print(f\"--- (LLM: AI response: '{ai_text}') ---\")\n",
    "\n",
    "    # --------------------------------\n",
    "    # Part 3: \"MOUTH\" (Text-to-Speech)\n",
    "    # --------------------------------\n",
    "    print(\"--- (TTS: Generating speech...) ---\")\n",
    "    response = openai_client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"nova\",\n",
    "        input=ai_text\n",
    "    )\n",
    "\n",
    "    # Save the AI's speech to a file\n",
    "    ai_audio_filepath = \"ai_response.mp3\"\n",
    "    response.stream_to_file(ai_audio_filepath)\n",
    "\n",
    "    # --------------------------------\n",
    "    # Part 4: Update History and Return\n",
    "    # --------------------------------\n",
    "    history.append([user_text, ai_text])\n",
    "    \n",
    "    # Return the AI's audio file path AND the updated history\n",
    "    return ai_audio_filepath, history, history\n",
    "\n",
    "# --- 6. Launch the Gradio Interface ---\n",
    "print(\"\\nüöÄ Launching the Voice-to-Voice interface...\")\n",
    "\n",
    "gr.Interface(\n",
    "    fn=voice_chatbot,\n",
    "    inputs=[\n",
    "        gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Speak to improBot\"),\n",
    "        gr.State(value=[])  # Input [1]: The history\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Audio(label=\"improBot's Response\", autoplay=True), # Output [0]\n",
    "        gr.Chatbot(label=\"Conversation History\"),           # Output [1]\n",
    "        gr.State()  # Output [2]: The \"catcher\" for the history\n",
    "    ],\n",
    "    title=\"improBot (Voice Version)\",\n",
    "    description=\"Speak your ideas and hear a comedy sketch! The bot remembers *everything* you say.\",\n",
    "    theme=\"soft\",\n",
    "    allow_flagging=\"never\"\n",
    ").launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4541382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kani_tts\n",
      "  Downloading kani_tts-0.0.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting torch>=2.8.0 (from kani_tts)\n",
      "  Using cached torch-2.9.0-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting nemo-toolkit==2.4.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading nemo_toolkit-2.4.0-py3-none-any.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from kani_tts) (2.3.4)\n",
      "Collecting scipy>=1.10.0 (from kani_tts)\n",
      "  Using cached scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting librosa>=0.10.0 (from kani_tts)\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting omegaconf>=2.3.0 (from kani_tts)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting fsspec==2024.12.0 (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.24 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts) (0.36.0)\n",
      "Collecting numba (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached numba-0.62.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.8 kB)\n",
      "Collecting onnx>=1.7.0 (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading onnx-1.19.1-cp311-cp311-macosx_12_0_universal2.whl.metadata (7.0 kB)\n",
      "Collecting protobuf~=5.29.5 (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: python-dateutil in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts) (2.9.0.post0)\n",
      "Collecting ruamel.yaml (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting scikit-learn (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts) (80.9.0)\n",
      "Collecting tensorboard (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting text-unidecode (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts) (4.67.1)\n",
      "Collecting wget (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wrapt (from nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached wrapt-2.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting black~=24.3 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading black-24.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: click>=8.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit[all]==2.4.0->kani_tts) (8.3.0)\n",
      "Collecting coverage (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading coverage-7.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting isort<6.0.0,>5.1.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting parameterized (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pytest (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pytest-8.4.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting pytest-httpserver (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pytest_httpserver-1.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pytest-mock (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pytest_mock-3.15.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytest-runner (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pytest_runner-6.0.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting sphinx (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinx-8.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting sphinxcontrib-bibtex (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinxcontrib_bibtex-2.6.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wandb (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading wandb-0.22.3-py3-none-macosx_12_0_arm64.whl.metadata (10 kB)\n",
      "Collecting nemo_run (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading nemo_run-0.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cloudpickle (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fiddle (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading fiddle-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting hydra-core<=1.3.2,>1.3 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lightning<=2.4.0,>2.2.1 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting peft (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting torchmetrics>=0.11.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting transformers<=4.52.0,>=4.51.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting webdataset>=0.2.86 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading webdataset-1.0.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting datasets (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting einops (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting inflect (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached inflect-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting mediapy==1.1.6 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading mediapy-1.1.6-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pandas in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit[all]==2.4.0->kani_tts) (2.3.3)\n",
      "Collecting sacremoses>=0.0.43 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting sentencepiece<1.0.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting braceexpand (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting editdistance (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading editdistance-0.8.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.9 kB)\n",
      "Collecting jiwer<4.0.0,>=3.1.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting kaldi-python-io (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lhotse!=1.31.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading lhotse-1.31.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: marshmallow in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit[all]==2.4.0->kani_tts) (3.26.1)\n",
      "Collecting optuna (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: packaging in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit[all]==2.4.0->kani_tts) (25.0)\n",
      "Collecting pyannote.core (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pyannote_core-6.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pyannote.metrics (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pyannote_metrics-4.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pydub in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit[all]==2.4.0->kani_tts) (0.25.1)\n",
      "Collecting pyloudnorm (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting resampy (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting soundfile (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting sox<=1.5.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sox-1.5.0.tar.gz (63 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting texterrors<1.0.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading texterrors-0.5.1.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting whisper_normalizer (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading whisper_normalizer-0.1.12-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting num2words (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.24.0 (from kani_tts)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting accelerated-scan (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading accelerated_scan-0.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting boto3 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading boto3-1.40.67-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: faiss-cpu in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo-toolkit[all]==2.4.0->kani_tts) (1.12.0)\n",
      "Collecting flask_restful (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting ftfy (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting gdown (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting h5py (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached h5py-3.15.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting ijson (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading ijson-3.4.0.post0-cp311-cp311-macosx_11_0_arm64.whl.metadata (23 kB)\n",
      "Collecting jieba (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting markdown2 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached markdown2-2.5.4-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting matplotlib>=3.3.2 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached matplotlib-3.10.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting megatron_core (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading megatron_core-0.14.0.tar.gz (804 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m804.8/804.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting multi-storage-client>=0.21.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading multi_storage_client-0.33.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.9 kB)\n",
      "Collecting nltk>=3.6.5 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting nvtx (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading nvtx-0.2.13.tar.gz (112 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opencc (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading opencc-1.1.9.tar.gz (3.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pangu (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pangu-4.0.6.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting prettytable (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading prettytable-3.16.0-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting rapidfuzz (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading rapidfuzz-3.14.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting rouge_score (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacrebleu (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting sentence_transformers (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting tiktoken==0.7.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting zarr<3.0.0,>=2.18.2 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading zarr-2.18.7-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting attrdict (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting cdifflib==1.2.6 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading cdifflib-1.2.6.tar.gz (11 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting janome (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting kornia (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pypinyin (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pypinyin-0.55.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pypinyin-dict (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pypinyin_dict-0.9.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting seaborn (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting progress>=1.5 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading progress-1.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting tabulate>=0.8.7 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting textdistance>=4.1.5 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting addict (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting clip (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting diffusers>=0.19.3 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading diffusers-0.35.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting einops_exts (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting imageio (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting megatron-energon==5.2.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading megatron_energon-5.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nerfacc>=0.5.3 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading nerfacc-0.5.3-py3-none-any.whl.metadata (915 bytes)\n",
      "Collecting open_clip_torch==2.24.0 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading open_clip_torch-2.24.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting qwen_vl_utils (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading qwen_vl_utils-0.0.14-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting taming-transformers (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading taming_transformers-0.0.1-py3-none-any.whl.metadata (499 bytes)\n",
      "Collecting torchdiffeq (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
      "Collecting torchsde (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting trimesh (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading trimesh-4.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pesq (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pystoi (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting flask (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting nvidia-lm-eval (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading nvidia_lm_eval-25.10-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: ipython in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (9.5.0)\n",
      "Requirement already satisfied: Pillow in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (11.3.0)\n",
      "Requirement already satisfied: pyyaml in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts) (6.0.3)\n",
      "Collecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2025.10.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting torchvision (from open_clip_torch==2.24.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached torchvision-0.24.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: regex in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from open_clip_torch==2.24.0->nemo-toolkit[all]==2.4.0->kani_tts) (2025.11.3)\n",
      "Collecting timm (from open_clip_torch==2.24.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading timm-1.0.22-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from tiktoken==0.7.0->nemo-toolkit[all]==2.4.0->kani_tts) (2.32.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from black~=24.3->nemo-toolkit[all]==2.4.0->kani_tts) (1.1.0)\n",
      "Collecting pathspec>=0.9.0 (from black~=24.3->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from black~=24.3->nemo-toolkit[all]==2.4.0->kani_tts) (4.4.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core<=1.3.2,>1.3->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lightning-utilities<2.0,>=0.10.0 (from lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting packaging (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (4.15.0)\n",
      "Collecting pytorch-lightning (from lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (3.13.2)\n",
      "Collecting pybind11 (from texterrors<1.0.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting plac (from texterrors<1.0.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading plac-1.4.5-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting loguru (from texterrors<1.0.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting termcolor (from texterrors<1.0.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting Levenshtein (from texterrors<1.0.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading levenshtein-0.27.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from torch>=2.8.0->kani_tts) (3.20.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.8.0->kani_tts)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=2.8.0->kani_tts)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from torch>=2.8.0->kani_tts) (3.1.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<=4.52.0,>=4.51.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers<=4.52.0,>=4.51.0->nemo-toolkit[all]==2.4.0->kani_tts) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface_hub>=0.24->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts) (1.2.0)\n",
      "Collecting asciitree (from zarr<3.0.0,>=2.18.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasteners (from zarr<3.0.0,>=2.18.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading fasteners-0.20-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting numcodecs!=0.14.0,!=0.14.1,<0.16,>=0.10.0 (from zarr<3.0.0,>=2.18.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading numcodecs-0.15.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.9 kB)\n",
      "Collecting deprecated (from numcodecs!=0.14.0,!=0.14.1,<0.16,>=0.10.0->zarr<3.0.0,>=2.18.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo-toolkit[all]==2.4.0->kani_tts) (3.11)\n",
      "Requirement already satisfied: importlib_metadata in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from diffusers>=0.19.3->nemo-toolkit[all]==2.4.0->kani_tts) (8.7.0)\n",
      "Collecting audioread>=2.1.9 (from lhotse!=1.31.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting cytoolz>=0.10.1 (from lhotse!=1.31.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading cytoolz-1.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting intervaltree>=3.1.0 (from lhotse!=1.31.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lilcom>=1.1.0 (from lhotse!=1.31.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading lilcom-1.8.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.2 kB)\n",
      "Collecting toolz>=0.8.0 (from cytoolz>=0.10.1->lhotse!=1.31.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree>=3.1.0->lhotse!=1.31.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting joblib>=1.0 (from librosa>=0.10.0->kani_tts)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from librosa>=0.10.0->kani_tts) (5.2.1)\n",
      "Collecting pooch>=1.1 (from librosa>=0.10.0->kani_tts)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa>=0.10.0->kani_tts)\n",
      "  Using cached soxr-1.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa>=0.10.0->kani_tts)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa>=0.10.0->kani_tts)\n",
      "  Using cached msgpack-1.1.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached fonttools-4.60.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.3.2->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting jmespath<2,>=1.0.1 (from multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting jsonschema<5,>=4 (from multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting opentelemetry-api<2,>=1.24 (from multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xattr<2,>=1.1.4 (from multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading xattr-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting wcmatch<11,>=10 (from multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading wcmatch-10.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting lark<2,>=1.2.2 (from multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading lark-1.3.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: psutil<8,>=7 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts) (7.1.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5,>=4->multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5,>=4->multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5,>=4->multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading rpds_py-0.28.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from importlib_metadata->diffusers>=0.19.3->nemo-toolkit[all]==2.4.0->kani_tts) (3.23.0)\n",
      "Requirement already satisfied: wcwidth in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from prettytable->nemo-toolkit[all]==2.4.0->kani_tts) (0.2.14)\n",
      "Requirement already satisfied: six>=1.5 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from python-dateutil->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts) (1.17.0)\n",
      "Collecting bracex>=2.1.1 (from wcmatch<11,>=10->multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading bracex-2.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting cffi>=1.16.0 (from xattr<2,>=1.1.4->multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached cffi-2.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=1.16.0->xattr<2,>=1.1.4->multi-storage-client>=0.21.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Requirement already satisfied: rich>=12 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nerfacc>=0.5.3->nemo-toolkit[all]==2.4.0->kani_tts) (14.2.0)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached llvmlite-0.45.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting ml_dtypes>=0.5.0 (from onnx>=1.7.0->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached ml_dtypes-0.5.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.7.0->nemo-toolkit[all]==2.4.0->kani_tts) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.7.0->nemo-toolkit[all]==2.4.0->kani_tts) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.7.0->nemo-toolkit[all]==2.4.0->kani_tts) (2025.10.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from rich>=12->nerfacc>=0.5.3->nemo-toolkit[all]==2.4.0->kani_tts) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from rich>=12->nerfacc>=0.5.3->nemo-toolkit[all]==2.4.0->kani_tts) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12->nerfacc>=0.5.3->nemo-toolkit[all]==2.4.0->kani_tts) (0.1.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.8.0->kani_tts)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.67 (from boto3->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading botocore-1.40.67-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.1 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from datasets->nemo-toolkit[all]==2.4.0->kani_tts) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from datasets->nemo-toolkit[all]==2.4.0->kani_tts) (3.6.0)\n",
      "Collecting multiprocess<0.70.19 (from datasets->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: anyio in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets->nemo-toolkit[all]==2.4.0->kani_tts) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets->nemo-toolkit[all]==2.4.0->kani_tts) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->nemo-toolkit[all]==2.4.0->kani_tts) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets->nemo-toolkit[all]==2.4.0->kani_tts) (1.3.1)\n",
      "Collecting absl-py (from fiddle->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting graphviz (from fiddle->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting libcst (from fiddle->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading libcst-1.8.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting blinker>=1.9.0 (from flask->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from flask->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from flask->nemo-toolkit[all]==2.4.0->kani_tts) (3.0.3)\n",
      "Collecting werkzeug>=3.1.0 (from flask->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting aniso8601>=0.82 (from flask_restful->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aniso8601-10.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pytz in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from flask_restful->nemo-toolkit[all]==2.4.0->kani_tts) (2025.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from gdown->nemo-toolkit[all]==2.4.0->kani_tts) (4.14.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from beautifulsoup4->gdown->nemo-toolkit[all]==2.4.0->kani_tts) (2.8)\n",
      "Collecting more_itertools>=8.5.0 (from inflect->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting typeguard>=4.0.1 (from inflect->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (3.0.52)\n",
      "Requirement already satisfied: stack_data in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (5.14.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from jedi>=0.16->ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from pexpect>4.3->ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (0.7.0)\n",
      "Collecting kornia_rs>=0.1.9 (from kornia->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading kornia_rs-0.1.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting catalogue>=2.0.10 (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting cryptography<43.0.0 (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading cryptography-42.0.8-cp39-abi3-macosx_10_12_universal2.whl.metadata (5.3 kB)\n",
      "Collecting fabric>=3.2.2 (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading fabric-3.2.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting inquirerpy>=0.3.4 (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting leptonai>=0.25.0 (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading leptonai-0.26.6-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting toml (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting torchx>=0.7.0 (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading torchx-0.7.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: typer>=0.12.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.20.0)\n",
      "Collecting invoke>=2.0 (from fabric>=3.2.2->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting paramiko>=2.4 (from fabric>=3.2.2->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading paramiko-4.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from inquirerpy>=0.3.4->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting cloudpickle (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting contextlib2 (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting exceptiongroup (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: fastapi in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.121.0)\n",
      "Collecting httpx<1.0.0 (from datasets->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting prometheus-fastapi-instrumentator==7.0.0 (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: uvicorn>=0.22.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.38.0)\n",
      "Requirement already satisfied: pydantic!=2.1.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (2.11.10)\n",
      "Requirement already satisfied: python-multipart in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.0.20)\n",
      "Collecting ledoc-ui (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading ledoc_ui-0.1.0-py3-none-any.whl.metadata (954 bytes)\n",
      "Collecting ray (from leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading ray-2.51.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx[http2]==0.27.2->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (4.3.0)\n",
      "Collecting prometheus-client<1.0.0,>=0.8.0 (from prometheus-fastapi-instrumentator==7.0.0->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: starlette<1.0.0,>=0.30.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from prometheus-fastapi-instrumentator==7.0.0->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.49.3)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]==0.27.2->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]==0.27.2->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (4.1.0)\n",
      "Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric>=3.2.2->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Collecting pynacl>=1.5 (from paramiko>=2.4->fabric>=3.2.2->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pynacl-1.6.0-cp38-abi3-macosx_10_10_universal2.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pydantic!=2.1.0->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pydantic!=2.1.0->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pydantic!=2.1.0->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.4.2)\n",
      "Collecting pyre-extensions (from torchx>=0.7.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting docstring-parser>=0.8.1 (from torchx>=0.7.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting docker (from torchx>=0.7.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken==0.7.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from typer>=0.12.3->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (1.5.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from fastapi->leptonai>=0.25.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.0.3)\n",
      "Collecting docopt>=0.6.2 (from num2words->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting evaluate (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonlines (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting numexpr (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading numexpr-2.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pytablewriter (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting tqdm-multiprocess (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: zstandard in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts) (0.25.0)\n",
      "Collecting word2number (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting langdetect==1.0.9 (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting immutabledict==4.2.0 (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: tenacity in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts) (9.1.2)\n",
      "Collecting beautifulsoup4 (from gdown->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading beautifulsoup4-4.13.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: openai>=1.61.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts) (2.7.1)\n",
      "Collecting tree-sitter>=0.20.4 (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading tree_sitter-0.25.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Collecting tree-sitter-python>=0.20.4 (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Collecting nemo-evaluator (from nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading nemo_evaluator-0.1.26-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai>=1.61.0->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from openai>=1.61.0->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts) (0.11.1)\n",
      "Collecting portalocker (from sacrebleu->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting colorama (from sacrebleu->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from sacrebleu->nemo-toolkit[all]==2.4.0->kani_tts) (6.0.2)\n",
      "Collecting structlog (from nemo-evaluator->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading structlog-25.5.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting yq (from nemo-evaluator->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading yq-3.4.3-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading alembic-1.17.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from optuna->nemo-toolkit[all]==2.4.0->kani_tts) (2.0.44)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pandas->nemo-toolkit[all]==2.4.0->kani_tts) (2025.2)\n",
      "Collecting accelerate>=0.21.0 (from peft->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of pyannote-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyannote.core (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pyannote_core-6.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "INFO: pip is looking at multiple versions of pyannote-metrics to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyannote.metrics (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pyannote_database-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading pyannote_database-6.0.0-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting future>=0.16.0 (from pyloudnorm->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: typing-inspect in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from pyre-extensions->torchx>=0.7.0->nemo_run->nemo-toolkit[all]==2.4.0->kani_tts) (0.9.0)\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting iniconfig>=1 (from pytest->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting av (from qwen_vl_utils->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading av-16.0.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (4.6 kB)\n",
      "Collecting click>=8.1 (from nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading ruamel.yaml.clib-0.2.14-cp311-cp311-macosx_13_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.25.1-py3-none-any.whl.metadata (25 kB)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2025.9.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading s3fs-2025.7.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading s3fs-2025.5.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading s3fs-2025.5.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading s3fs-2025.3.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading s3fs-2025.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "INFO: pip is still looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading s3fs-2025.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading s3fs-2024.12.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of aiobotocore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.25.0-py3-none-any.whl.metadata (25 kB)\n",
      "  Downloading aiobotocore-2.24.3-py3-none-any.whl.metadata (25 kB)\n",
      "  Downloading aiobotocore-2.24.2-py3-none-any.whl.metadata (25 kB)\n",
      "  Downloading aiobotocore-2.24.1-py3-none-any.whl.metadata (25 kB)\n",
      "  Downloading aiobotocore-2.24.0-py3-none-any.whl.metadata (25 kB)\n",
      "  Downloading aiobotocore-2.23.2-py3-none-any.whl.metadata (25 kB)\n",
      "  Downloading aiobotocore-2.23.1-py3-none-any.whl.metadata (25 kB)\n",
      "INFO: pip is still looking at multiple versions of aiobotocore to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading aiobotocore-2.23.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading aiobotocore-2.21.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading aiobotocore-2.21.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading aiobotocore-2.20.0-py3-none-any.whl.metadata (23 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading aiobotocore-2.19.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.18.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.17.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.16.1-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.16.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.15.2-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.15.1-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.15.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.14.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading aiobotocore-2.13.3-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading aiobotocore-2.13.2-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading aiobotocore-2.13.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading aiobotocore-2.13.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.12.4-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.12.3-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.12.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.12.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.11.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.11.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.11.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading aiobotocore-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading aiobotocore-2.9.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading aiobotocore-2.9.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading aiobotocore-2.8.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading aiobotocore-2.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2024.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Downloading s3fs-2024.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.6.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.4.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.4.2 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2023.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2022.11.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2022.10.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2022.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2022.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2022.8.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2022.7.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting aiobotocore~=2.3.4 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.3.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2022.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2022.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2022.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting aiobotocore~=2.2.0 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2022.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting aiobotocore~=2.1.0 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.1.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2021.11.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.0.1 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=1.4.1 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-2021.10.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.10.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.8.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.6.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-2021.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "  Downloading s3fs-0.6.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiobotocore>=1.0.1 (from s3fs->megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading aiobotocore-2.5.3-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading aiobotocore-2.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading aiobotocore-2.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading aiobotocore-2.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading aiobotocore-2.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading aiobotocore-2.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading aiobotocore-2.3.3.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.3.2.tar.gz (104 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.3.1.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.3.0.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.1.tar.gz (48 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.0.tar.gz (47 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading aiobotocore-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading aiobotocore-1.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading aiobotocore-1.0.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading aiobotocore-1.0.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading aiobotocore-1.0.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading aiobotocore-1.0.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading aiobotocore-1.0.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading aiobotocore-1.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading aiobotocore-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting s3fs (from megatron-energon==5.2.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading s3fs-0.5.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading s3fs-0.5.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading s3fs-0.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading s3fs-0.4.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting docutils<0.22,>=0.20 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting babel>=2.13 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting alabaster>=0.7.14 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting roman-numerals-py>=1.0.0 (from sphinx->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading roman_numerals_py-3.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pybtex>=0.25 (from sphinxcontrib-bibtex->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pybtex-0.25.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting latexcodec>=1.0.4 (from pybtex>=0.25->sphinxcontrib-bibtex->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading latexcodec-3.0.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from stack_data->ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from stack_data->ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/federicosvendsen/.local/lib/python3.11/site-packages (from stack_data->ipython->mediapy==1.1.6->nemo-toolkit[all]==2.4.0->kani_tts) (0.2.3)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached grpcio-1.76.0-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->nemo-toolkit==2.4.0->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting trampoline>=0.1.2 (from torchsde->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading sentry_sdk-2.43.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting indic-numtowords (from whisper_normalizer->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading indic_numtowords-1.1.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting xmltodict>=0.11.0 (from yq->nemo-evaluator->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading xmltodict-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tomlkit>=0.11.6 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from yq->nemo-evaluator->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts) (0.13.3)\n",
      "Collecting argcomplete>=1.8.1 (from yq->nemo-evaluator->nvidia-lm-eval->nemo-toolkit[all]==2.4.0->kani_tts)\n",
      "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading kani_tts-0.0.4-py3-none-any.whl (9.3 kB)\n",
      "Downloading nemo_toolkit-2.4.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading mediapy-1.1.6-py3-none-any.whl (24 kB)\n",
      "Downloading megatron_energon-5.2.0-py3-none-any.whl (175 kB)\n",
      "Downloading open_clip_torch-2.24.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m907.0/907.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading black-24.10.0-cp311-cp311-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Downloading sentencepiece-0.2.1-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.9.0-cp311-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Downloading zarr-2.18.7-py3-none-any.whl (211 kB)\n",
      "Downloading numcodecs-0.15.1-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m193.9 kB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading diffusers-0.35.2-py3-none-any.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lhotse-1.31.1-py3-none-any.whl (866 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m866.5/866.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading cytoolz-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (986 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m986.6/986.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading lilcom-1.8.1-cp311-cp311-macosx_10_9_universal2.whl (125 kB)\n",
      "Using cached matplotlib-3.10.7-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (270 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached msgpack-1.1.2-cp311-cp311-macosx_11_0_arm64.whl (84 kB)\n",
      "Downloading multi_storage_client-0.33.0-cp311-cp311-macosx_11_0_arm64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading lark-1.3.1-py3-none-any.whl (113 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading prettytable-3.16.0-py3-none-any.whl (33 kB)\n",
      "Downloading wcmatch-10.1-py3-none-any.whl (39 kB)\n",
      "Downloading xattr-1.3.0-cp311-cp311-macosx_11_0_arm64.whl (18 kB)\n",
      "Downloading bracex-2.6-py3-none-any.whl (11 kB)\n",
      "Using cached cffi-2.0.0-cp311-cp311-macosx_11_0_arm64.whl (180 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading nerfacc-0.5.3-py3-none-any.whl (54 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numba-0.62.1-cp311-cp311-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached llvmlite-0.45.1-cp311-cp311-macosx_11_0_arm64.whl (37.3 MB)\n",
      "Downloading onnx-1.19.1-cp311-cp311-macosx_12_0_universal2.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached ml_dtypes-0.5.3-cp311-cp311-macosx_10_9_universal2.whl (667 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading progress-1.6.1-py3-none-any.whl (9.8 kB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp311-cp311-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.28.0-cp311-cp311-macosx_11_0_arm64.whl (348 kB)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Using cached soxr-1.0.0-cp311-cp311-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Downloading webdataset-1.0.2-py3-none-any.whl (74 kB)\n",
      "Downloading accelerated_scan-0.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading boto3-1.40.67-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.67-py3-none-any.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Downloading coverage-7.11.0-cp311-cp311-macosx_11_0_arm64.whl (216 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-macosx_12_0_arm64.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m980.5 kB/s\u001b[0m  \u001b[33m0:00:31\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached wrapt-2.0.0-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
      "Downloading editdistance-0.8.1-cp311-cp311-macosx_11_0_arm64.whl (79 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
      "Downloading fasteners-0.20-py3-none-any.whl (18 kB)\n",
      "Downloading fiddle-0.3.0-py3-none-any.whl (419 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
      "Downloading aniso8601-10.0.1-py2.py3-none-any.whl (52 kB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Using cached h5py-3.15.1-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Downloading ijson-3.4.0.post0-cp311-cp311-macosx_11_0_arm64.whl (59 kB)\n",
      "Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Using cached inflect-7.5.0-py3-none-any.whl (35 kB)\n",
      "Using cached more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Using cached typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m854.3 kB/s\u001b[0m  \u001b[33m0:00:22\u001b[0m eta \u001b[36m0:00:04\u001b[0m\n",
      "\u001b[?25hDownloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kornia_rs-0.1.9-cp311-cp311-macosx_11_0_arm64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading levenshtein-0.27.3-cp311-cp311-macosx_11_0_arm64.whl (158 kB)\n",
      "Downloading libcst-1.8.6-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached markdown2-2.5.4-py3-none-any.whl (49 kB)\n",
      "Downloading nemo_run-0.6.0-py3-none-any.whl (235 kB)\n",
      "Downloading cryptography-42.0.8-cp39-abi3-macosx_10_12_universal2.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading leptonai-0.26.6-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
      "Downloading paramiko-4.0.0-py3-none-any.whl (223 kB)\n",
      "Downloading bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl (495 kB)\n",
      "Downloading pynacl-1.6.0-cp38-abi3-macosx_10_10_universal2.whl (382 kB)\n",
      "Downloading torchx-0.7.0-py3-none-any.whl (256 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "Downloading ledoc_ui-0.1.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
      "Downloading nvidia_lm_eval-25.10-py3-none-any.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading beautifulsoup4-4.13.1-py3-none-any.whl (185 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tree_sitter-0.25.2-cp311-cp311-macosx_11_0_arm64.whl (137 kB)\n",
      "Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (76 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading nemo_evaluator-0.1.26-py3-none-any.whl (112 kB)\n",
      "Downloading numexpr-2.14.1-cp311-cp311-macosx_11_0_arm64.whl (152 kB)\n",
      "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "Downloading alembic-1.17.1-py3-none-any.whl (247 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading plac-1.4.5-py2.py3-none-any.whl (22 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "Downloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Using cached pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
      "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading pypinyin-0.55.0-py2.py3-none-any.whl (840 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypinyin_dict-0.9.0-py2.py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyre_extensions-0.0.32-py3-none-any.whl (12 kB)\n",
      "Using cached pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Downloading pytest-8.4.2-py3-none-any.whl (365 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading pytest_httpserver-1.1.3-py3-none-any.whl (21 kB)\n",
      "Downloading pytest_mock-3.15.1-py3-none-any.whl (10 kB)\n",
      "Using cached pytest_runner-6.0.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading qwen_vl_utils-0.0.14-py3-none-any.whl (8.1 kB)\n",
      "Downloading av-16.0.1-cp311-cp311-macosx_14_0_arm64.whl (21.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading ray-2.51.1-cp311-cp311-macosx_12_0_arm64.whl (68.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.0/68.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:00:49\u001b[0mm0:00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.14-cp311-cp311-macosx_13_0_arm64.whl (137 kB)\n",
      "Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading sphinx-8.2.3-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0meta \u001b[36m0:00:02\u001b[0mm\n",
      "\u001b[?25hDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading roman_numerals_py-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Downloading sphinxcontrib_bibtex-2.6.5-py3-none-any.whl (40 kB)\n",
      "Downloading pybtex-0.25.1-py2.py3-none-any.whl (127 kB)\n",
      "Downloading latexcodec-3.0.1-py3-none-any.whl (18 kB)\n",
      "Downloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
      "Downloading structlog-25.5.0-py3-none-any.whl (72 kB)\n",
      "Downloading taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached grpcio-1.76.0-cp311-cp311-macosx_11_0_universal2.whl (11.8 MB)\n",
      "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading timm-1.0.22-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
      "Downloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
      "Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
      "Using cached torchvision-0.24.0-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading trimesh-4.9.0-py3-none-any.whl (736 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m736.5/736.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.22.3-py3-none-macosx_12_0_arm64.whl (18.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hUsing cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sentry_sdk-2.43.0-py2.py3-none-any.whl (400 kB)\n",
      "Downloading whisper_normalizer-0.1.12-py3-none-any.whl (36 kB)\n",
      "Downloading indic_numtowords-1.1.0-py3-none-any.whl (71 kB)\n",
      "Downloading yq-3.4.3-py3-none-any.whl (18 kB)\n",
      "Downloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
      "Downloading xmltodict-1.0.2-py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: cdifflib, antlr4-python3-runtime, sox, texterrors, intervaltree, asciitree, clip, jieba, kaldi-python-io, megatron_core, docopt, langdetect, rouge_score, nvtx, opencc, pesq, wget, word2number\n",
      "  Building wheel for cdifflib (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cdifflib: filename=cdifflib-1.2.6-cp311-cp311-macosx_11_0_arm64.whl size=10666 sha256=29289562b05e77a9305c305380d96e5e4cc13d223d7411137399ae7eeb4a67f3\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/61/25/f2/4ee06fe9d0bcb43991be6302633001497862ff216060e9361f\n",
      "\u001b[33m  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=e0f6137febc544682e189f89f99ae8874f18bf2d57c2cd798cfb624ecbe26048\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "\u001b[33m  DEPRECATION: Building 'sox' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sox'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for sox (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40128 sha256=a09fd6b9391f033e750da40c140a9277686ddaf4543d9b328e82f66563cf7f5f\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/74/89/93/023fcdacaec4e5471e78b43992515e8500cc2505b307e2e6b7\n",
      "\u001b[33m  DEPRECATION: Building 'texterrors' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'texterrors'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for texterrors (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for texterrors: filename=texterrors-0.5.1-cp311-cp311-macosx_11_0_arm64.whl size=93762 sha256=1a3b415d4e56d7593e668fb24c527550bdc668a162c9487bad13ad9287121a7c\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/6f/94/c8/7edaa578fc800d26e3fda18fba557a4218ab553d078ee51b46\n",
      "\u001b[33m  DEPRECATION: Building 'intervaltree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'intervaltree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26190 sha256=0d9ce5619f008ad298e97bcf1408dc63d560835fbab18d596142e14870713f16\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
      "\u001b[33m  DEPRECATION: Building 'asciitree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'asciitree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for asciitree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5096 sha256=cc41e34b31ea4ca5a2b3840bb789103b62a2c563ce301cb8af1563b80caf12be\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/71/c1/da/23077eb3b87d24d6f3852ed1ed1a1ac2d3c885ad6ebd2b4a07\n",
      "\u001b[33m  DEPRECATION: Building 'clip' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'clip'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=7060 sha256=4e7850c2172adcdd0162299025bf418c6573ed5d2c12b5754bce8f9c8ce5ad38\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/ab/a5/e8/c9fa20742edbccf2702dae8ee62053e6c460e961d45967b49c\n",
      "\u001b[33m  DEPRECATION: Building 'jieba' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'jieba'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314509 sha256=6101b6c3bfd046baa3a5d6585a3242710c2d3b65e90a8b1e6db30aa17f15ccf4\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/ac/60/cf/538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "\u001b[33m  DEPRECATION: Building 'kaldi-python-io' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'kaldi-python-io'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=9007 sha256=712801893d122638c35049b40902ceea98f373c58480e58b43e2f84c6dffb8d8\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/f2/86/7b/eec1bb7dc63b8aab5da6317609313873e6e75f065b65f3c29c\n",
      "  Building wheel for megatron_core (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for megatron_core: filename=megatron_core-0.14.0-cp311-cp311-macosx_11_0_arm64.whl size=1038854 sha256=38182f463db3f3b64d49f42f8cb5b4dea3a21d1e271d368a3e4cd6bde60c96fb\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/0d/e4/ce/1b49c812901ccfe817f097b49a6f246fe0253581f40612ab8b\n",
      "\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=432b310188d99a9a962cd1c7d67f5a26509017ae0219f6831a2e0120de92246f\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "\u001b[33m  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993332 sha256=191a7453dd3425f04813a684b21a6b6ab3406bf742199c1928795ee157bcb00c\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=1c3ba45ee2a5ee82d730e13ece2749f027545d6701a50f3bb6325703e4a75ed9\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "  Building wheel for nvtx (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvtx: filename=nvtx-0.2.13-cp311-cp311-macosx_11_0_arm64.whl size=101593 sha256=345f3690e3402073901c58d70a0e6beac0cbcbd3428faa9f903105322a94d04c\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/2c/ff/d7/2cb31faafca03e80be708ff7d0403c411f3bc5a1bde97bfce9\n",
      "  Building wheel for opencc (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opencc: filename=opencc-1.1.9-cp311-cp311-macosx_11_0_arm64.whl size=1378996 sha256=0112e502d15677d52fefaa6c210b17e844a9ba0c960543db71d944c78ef2b005\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/a8/69/af/d8229790f384e1e6ca80d0459a9754679d44e554711f909d8f\n",
      "\u001b[33m  DEPRECATION: Building 'pesq' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pesq'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pesq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pesq: filename=pesq-0.0.4-cp311-cp311-macosx_11_0_arm64.whl size=115928 sha256=a068bf835515d18e8755c42ca55253886e9c68ce2d84333c2aa00b11467a3677\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/ae/f1/23/2698d0bf31eec2b2aa50623b5d93b6206c49c7155d0e31345d\n",
      "\u001b[33m  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9685 sha256=da058be979a5816be5038e979674a6f6fa11aba24fab39994f96f3a5e6199eb1\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "\u001b[33m  DEPRECATION: Building 'word2number' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'word2number'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5659 sha256=999f400643dc88a6816addcfc74173d373638aee99c376415d9341230c55a2ce\n",
      "  Stored in directory: /Users/federicosvendsen/Library/Caches/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
      "Successfully built cdifflib antlr4-python3-runtime sox texterrors intervaltree asciitree clip jieba kaldi-python-io megatron_core docopt langdetect rouge_score nvtx opencc pesq wget word2number\n",
      "Installing collected packages: word2number, wget, trampoline, text-unidecode, sortedcontainers, plac, pesq, pangu, opencc, nvtx, mpmath, ledoc-ui, jieba, janome, docopt, clip, braceexpand, asciitree, antlr4-python3-runtime, aniso8601, addict, xmltodict, wrapt, werkzeug, urllib3, typeguard, tree-sitter-python, tree-sitter, toolz, toml, threadpoolctl, textdistance, termcolor, tensorboard-data-server, tcolorpy, tabulate, sympy, structlog, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, smmap, sentencepiece, ruamel.yaml.clib, rpds-py, roman-numerals-py, rapidfuzz, pytest-runner, PySocks, pypinyin, pyparsing, pycparser, pybind11, pyarrow, protobuf, prometheus-client, progress, prettytable, portalocker, pluggy, pfzy, pathvalidate, pathspec, parameterized, packaging, omegaconf, numpy, num2words, networkx, msgpack, more_itertools, markdown2, markdown, Mako, loguru, llvmlite, libcst, latexcodec, lark, langdetect, kornia_rs, kiwisolver, jsonlines, joblib, jmespath, itsdangerous, isort, invoke, intervaltree, iniconfig, indic-numtowords, immutabledict, imagesize, ijson, grpcio, graphviz, future, ftfy, fsspec, fonttools, fasteners, exceptiongroup, einops, editdistance, docutils, docstring-parser, dill, cycler, coverage, contextlib2, colorlog, colorama, cloudpickle, click, chardet, cdifflib, catalogue, bracex, blinker, beautifulsoup4, bcrypt, babel, av, audioread, attrdict, argcomplete, anyio, alabaster, absl-py, yq, whisper_normalizer, webdataset, wcmatch, trimesh, tqdm-multiprocess, torch, tensorboard, soxr, sox, sentry-sdk, scipy, sacremoses, sacrebleu, ruamel.yaml, referencing, pytest-httpserver, pytest, pyre-extensions, pypinyin-dict, pybtex, opentelemetry-api, numexpr, numba, nltk, multiprocess, ml_dtypes, mbstrdecoder, lilcom, lightning-utilities, Levenshtein, lazy_loader, kaldi-python-io, jiwer, inquirerpy, inflect, imageio, hydra-core, httpx, h5py, gitdb, flask, fiddle, einops_exts, deprecated, cytoolz, contourpy, cffi, botocore, black, alembic, xattr, typepy, torchvision, torchsde, torchmetrics, torchdiffeq, tiktoken, texterrors, sphinx, soundfile, scikit-learn, s3transfer, s3fs, rouge_score, resampy, qwen_vl_utils, pytest-mock, pystoi, pynacl, pyloudnorm, pybtex-docutils, pyannote.core, prometheus-fastapi-instrumentator, pooch, optuna, onnx, numcodecs, nerfacc, nemo-evaluator, megatron_core, matplotlib, kornia, jsonschema-specifications, gitpython, flask_restful, docker, cryptography, accelerated-scan, zarr, wandb, torchx, tokenizers, timm, sphinxcontrib-bibtex, seaborn, pytorch-lightning, pyannote.database, paramiko, nemo-toolkit, mediapy, librosa, lhotse, jsonschema, gdown, diffusers, datasets, boto3, accelerate, transformers, taming-transformers, ray, pyannote.metrics, open_clip_torch, multi-storage-client, lightning, fabric, evaluate, DataProperty, tabledata, sentence_transformers, peft, megatron-energon, leptonai, pytablewriter, nemo_run, nvidia-lm-eval, kani_tts\n",
      "\u001b[2K  Attempting uninstall: urllib3‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 23/261\u001b[0m [werkzeug]]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 23/261\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 23/261\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 23/261\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: packaging90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 64/261\u001b[0m [pfzy]theus-client]ehelp]ml]\n",
      "\u001b[2K    Found existing installation: packaging 25.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 64/261\u001b[0m [pfzy]\n",
      "\u001b[2K    Uninstalling packaging-25.0:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 64/261\u001b[0m [pfzy]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 64/261\u001b[0m [pfzy]\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 68/261\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.4‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 70/261\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling numpy-2.3.4:[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 70/261\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.4‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 70/261\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: fsspec0m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100/261\u001b[0m [future]z]towords]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.10.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100/261\u001b[0m [future]\n",
      "\u001b[2K    Uninstalling fsspec-2025.10.0:[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100/261\u001b[0m [future]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.10.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100/261\u001b[0m [future]\n",
      "\u001b[2K  Attempting uninstall: click\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112/261\u001b[0m [coverage]-parser]\n",
      "\u001b[2K    Found existing installation: click 8.3.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112/261\u001b[0m [coverage]\n",
      "\u001b[2K    Uninstalling click-8.3.0:m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112/261\u001b[0m [coverage]\n",
      "\u001b[2K      Successfully uninstalled click-8.3.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112/261\u001b[0m [coverage]\n",
      "\u001b[2K  Attempting uninstall: beautifulsoup40m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118/261\u001b[0m [chardet]\n",
      "\u001b[2K    Found existing installation: beautifulsoup4 4.14.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118/261\u001b[0m [chardet]\n",
      "\u001b[2K    Uninstalling beautifulsoup4-4.14.2:0m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118/261\u001b[0m [chardet]\n",
      "\u001b[2K      Successfully uninstalled beautifulsoup4-4.14.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118/261\u001b[0m [chardet]\n",
      "\u001b[2K  Attempting uninstall: anyio0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126/261\u001b[0m [av]el]fulsoup4]\n",
      "\u001b[2K    Found existing installation: anyio 4.11.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126/261\u001b[0m [av]\n",
      "\u001b[2K    Uninstalling anyio-4.11.0:0m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126/261\u001b[0m [av]\n",
      "\u001b[2K      Successfully uninstalled anyio-4.11.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126/261\u001b[0m [av]\n",
      "\u001b[2K  Attempting uninstall: httpx‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170/261\u001b[0m [hydra-core]tilities]\n",
      "\u001b[2K    Found existing installation: httpx 0.28.1[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170/261\u001b[0m [hydra-core]\n",
      "\u001b[2K    Uninstalling httpx-0.28.1:‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m171/261\u001b[0m [httpx]]\n",
      "\u001b[2K      Successfully uninstalled httpx-0.28.1m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m171/261\u001b[0m [httpx]\n",
      "\u001b[2K  Attempting uninstall: tokenizers‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224/261\u001b[0m [torchx]raphy]]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.10m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224/261\u001b[0m [torchx]\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225/261\u001b[0m [tokenizers]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225/261\u001b[0m [tokenizers]\n",
      "\u001b[2K  Attempting uninstall: transformers‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m241/261\u001b[0m [accelerate]t]ning]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.10m\u001b[90m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m241/261\u001b[0m [accelerate]\n",
      "\u001b[2K    Uninstalling transformers-4.57.1:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ\u001b[0m \u001b[32m242/261\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.1[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ\u001b[0m \u001b[32m242/261\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261/261\u001b[0m [kani_tts]nvidia-lm-eval]n]_transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ddgs 9.7.1 requires httpx[brotli,http2,socks]>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed DataProperty-1.1.0 Levenshtein-0.27.3 Mako-1.3.10 PySocks-1.7.1 absl-py-2.3.1 accelerate-1.11.0 accelerated-scan-0.2.0 addict-2.4.0 alabaster-1.0.0 alembic-1.17.1 aniso8601-10.0.1 antlr4-python3-runtime-4.9.3 anyio-4.9.0 argcomplete-3.6.3 asciitree-0.3.3 attrdict-2.0.1 audioread-3.1.0 av-16.0.1 babel-2.17.0 bcrypt-5.0.0 beautifulsoup4-4.13.1 black-24.10.0 blinker-1.9.0 boto3-1.40.67 botocore-1.40.67 braceexpand-0.1.7 bracex-2.6 catalogue-2.0.10 cdifflib-1.2.6 cffi-2.0.0 chardet-5.2.0 click-8.2.1 clip-0.2.0 cloudpickle-3.0.0 colorama-0.4.6 colorlog-6.10.1 contextlib2-21.6.0 contourpy-1.3.3 coverage-7.11.0 cryptography-42.0.8 cycler-0.12.1 cytoolz-1.1.0 datasets-4.4.1 deprecated-1.3.1 diffusers-0.35.2 dill-0.4.0 docker-7.1.0 docopt-0.6.2 docstring-parser-0.17.0 docutils-0.21.2 editdistance-0.8.1 einops-0.8.1 einops_exts-0.0.4 evaluate-0.4.6 exceptiongroup-1.3.0 fabric-3.2.2 fasteners-0.20 fiddle-0.3.0 flask-3.1.2 flask_restful-0.3.10 fonttools-4.60.1 fsspec-2024.12.0 ftfy-6.3.1 future-1.0.0 gdown-5.2.0 gitdb-4.0.12 gitpython-3.1.45 graphviz-0.21 grpcio-1.76.0 h5py-3.15.1 httpx-0.27.2 hydra-core-1.3.2 ijson-3.4.0.post0 imageio-2.37.2 imagesize-1.4.1 immutabledict-4.2.0 indic-numtowords-1.1.0 inflect-7.5.0 iniconfig-2.3.0 inquirerpy-0.3.4 intervaltree-3.1.0 invoke-2.2.1 isort-5.13.2 itsdangerous-2.2.0 janome-0.5.0 jieba-0.42.1 jiwer-3.1.0 jmespath-1.0.1 joblib-1.5.2 jsonlines-4.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 kaldi-python-io-1.2.2 kani_tts-0.0.4 kiwisolver-1.4.9 kornia-0.8.1 kornia_rs-0.1.9 langdetect-1.0.9 lark-1.3.1 latexcodec-3.0.1 lazy_loader-0.4 ledoc-ui-0.1.0 leptonai-0.26.6 lhotse-1.31.1 libcst-1.8.6 librosa-0.11.0 lightning-2.4.0 lightning-utilities-0.15.2 lilcom-1.8.1 llvmlite-0.45.1 loguru-0.7.3 markdown-3.10 markdown2-2.5.4 matplotlib-3.10.7 mbstrdecoder-1.1.4 mediapy-1.1.6 megatron-energon-5.2.0 megatron_core-0.14.0 ml_dtypes-0.5.3 more_itertools-10.8.0 mpmath-1.3.0 msgpack-1.1.2 multi-storage-client-0.33.0 multiprocess-0.70.18 nemo-evaluator-0.1.26 nemo-toolkit-2.4.0 nemo_run-0.6.0 nerfacc-0.5.3 networkx-3.5 nltk-3.9.2 num2words-0.5.14 numba-0.62.1 numcodecs-0.15.1 numexpr-2.14.1 numpy-1.26.4 nvidia-lm-eval-25.10 nvtx-0.2.13 omegaconf-2.3.0 onnx-1.19.1 open_clip_torch-2.24.0 opencc-1.1.9 opentelemetry-api-1.38.0 optuna-4.5.0 packaging-24.2 pangu-4.0.6.1 parameterized-0.9.0 paramiko-4.0.0 pathspec-0.12.1 pathvalidate-3.3.1 peft-0.17.1 pesq-0.0.4 pfzy-0.3.4 plac-1.4.5 pluggy-1.6.0 pooch-1.8.2 portalocker-3.2.0 prettytable-3.16.0 progress-1.6.1 prometheus-client-0.23.1 prometheus-fastapi-instrumentator-7.0.0 protobuf-5.29.5 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyarrow-22.0.0 pybind11-3.0.1 pybtex-0.25.1 pybtex-docutils-1.0.3 pycparser-2.23 pyloudnorm-0.1.1 pynacl-1.6.0 pyparsing-3.2.5 pypinyin-0.55.0 pypinyin-dict-0.9.0 pyre-extensions-0.0.32 pystoi-0.4.1 pytablewriter-1.2.1 pytest-8.4.2 pytest-httpserver-1.1.3 pytest-mock-3.15.1 pytest-runner-6.0.1 pytorch-lightning-2.5.6 qwen_vl_utils-0.0.14 rapidfuzz-3.14.3 ray-2.51.1 referencing-0.37.0 resampy-0.4.3 roman-numerals-py-3.1.0 rouge_score-0.1.2 rpds-py-0.28.0 ruamel.yaml-0.18.16 ruamel.yaml.clib-0.2.14 s3fs-0.4.2 s3transfer-0.14.0 sacrebleu-2.5.1 sacremoses-0.1.1 scikit-learn-1.7.2 scipy-1.16.3 seaborn-0.13.2 sentence_transformers-5.1.2 sentencepiece-0.2.1 sentry-sdk-2.43.0 smmap-5.0.2 snowballstemmer-3.0.1 sortedcontainers-2.4.0 soundfile-0.13.1 sox-1.5.0 soxr-1.0.0 sphinx-8.2.3 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-bibtex-2.6.5 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 structlog-25.5.0 sympy-1.14.0 tabledata-1.3.4 tabulate-0.9.0 taming-transformers-0.0.1 tcolorpy-0.1.7 tensorboard-2.20.0 tensorboard-data-server-0.7.2 termcolor-3.2.0 text-unidecode-1.3 textdistance-4.6.3 texterrors-0.5.1 threadpoolctl-3.6.0 tiktoken-0.7.0 timm-1.0.22 tokenizers-0.21.4 toml-0.10.2 toolz-1.1.0 torch-2.9.0 torchdiffeq-0.2.5 torchmetrics-1.8.2 torchsde-0.2.6 torchvision-0.24.0 torchx-0.7.0 tqdm-multiprocess-0.0.11 trampoline-0.1.2 transformers-4.51.3 tree-sitter-0.25.2 tree-sitter-python-0.25.0 trimesh-4.9.0 typeguard-4.4.4 typepy-1.3.4 urllib3-1.26.20 wandb-0.22.3 wcmatch-10.1 webdataset-1.0.2 werkzeug-3.1.3 wget-3.2 whisper_normalizer-0.1.12 word2number-1.1 wrapt-2.0.0 xattr-1.3.0 xmltodict-1.0.2 yq-3.4.3 zarr-2.18.7\n"
     ]
    }
   ],
   "source": [
    "!pip install kani_tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbde3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = \"'Buenos d√≠as, amigo. Estoy genial, gracias. A ver, que yo lo veo claro, el secreto para empezar el d√≠a con energ√≠a es no comer nada antes de las 12 del mediod√≠a. Es pura fisiolog√≠a, como dec√≠a el gran fil√≥sofo griego, \"El est√≥mago es el enemigo del alma\". Y no te digo que haya le√≠do un estudio en un blog de 2018 que demostraba que el ayuno matutino aumenta la creatividad un 300%. Est√° m√°s que demostrado que, si no comes, tu mente se concentra en las cosas importantes, como la teor√≠a de la relatividad o la mejor manera de preparar un taco de tortilla. Claro, claro, algunos dir√°n que es una tonter√≠a, pero yo te digo que es ciencia pura. ¬°Salud!'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c6394d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `lfm2` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1131\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:833\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    834\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'lfm2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkani_tts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaniTTS\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio \u001b[38;5;28;01mas\u001b[39;00m aplay\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mKaniTTS\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnineninesix/kani-tts-370m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m audio, text = model(\u001b[33m\"\u001b[39m\u001b[33mHello, world!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Play audio in notebook\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/kani_tts/api.py:91\u001b[39m, in \u001b[36mKaniTTS.__init__\u001b[39m\u001b[34m(self, model_name, device_map, max_new_tokens, temperature, top_p, repetition_penalty, tokeniser_length, suppress_logs, show_info)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m.player = NemoAudioPlayer(\u001b[38;5;28mself\u001b[39m.config)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mKaniModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.status = \u001b[38;5;28mself\u001b[39m.model.status\n\u001b[32m     93\u001b[39m \u001b[38;5;28mself\u001b[39m.speaker_list = \u001b[38;5;28mself\u001b[39m.model.speaker_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/kani_tts/core.py:110\u001b[39m, in \u001b[36mKaniModel.__init__\u001b[39m\u001b[34m(self, config, model_name, player)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mself\u001b[39m.player = player\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m.device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.speaker_settings = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.config, \u001b[33m'\u001b[39m\u001b[33mspeaker_settings\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:531\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    529\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1133\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1134\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1135\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1136\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1137\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1138\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1139\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers from source with the command \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1141\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1142\u001b[39m         )\n\u001b[32m   1143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1145\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1146\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `lfm2` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "from kani_tts import KaniTTS\n",
    "from IPython.display import Audio as aplay\n",
    "\n",
    "model = KaniTTS('nineninesix/kani-tts-370m')\n",
    "audio, text = model(\"Hello, world!\")\n",
    "\n",
    "# Play audio in notebook\n",
    "aplay(audio, rate=model.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3d0ee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/t2/d3hrzr8x7zd9cnztszgjcw0h0000gn/T/pip-req-build-nuihzr54\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/t2/d3hrzr8x7zd9cnztszgjcw0h0000gn/T/pip-req-build-nuihzr54\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 5aa7dd07dace2dbe5d419613ec9b43b36df35f89\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (3.20.0)\n",
      "Collecting huggingface-hub<2.0,>=1.0.0 (from transformers==5.0.0.dev0)\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (0.22.1)\n",
      "Collecting typer-slim (from transformers==5.0.0.dev0)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from transformers==5.0.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (0.27.2)\n",
      "Requirement already satisfied: shellingham in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (3.11)\n",
      "Requirement already satisfied: sniffio in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests->transformers==5.0.0.dev0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from requests->transformers==5.0.0.dev0) (1.26.20)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages (from typer-slim->transformers==5.0.0.dev0) (8.2.1)\n",
      "Downloading huggingface_hub-1.1.2-py3-none-any.whl (514 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-5.0.0.dev0-py3-none-any.whl size=11353900 sha256=1ed7342100b313bc2d06295010e6445de635f69aa59ada70761114d22f176995\n",
      "  Stored in directory: /private/var/folders/t2/d3hrzr8x7zd9cnztszgjcw0h0000gn/T/pip-ephem-wheel-cache-q8xipy7x/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
      "Successfully built transformers\n",
      "Installing collected packages: typer-slim, huggingface-hub, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/3\u001b[0m [typer-slim]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.36.0/3\u001b[0m [typer-slim]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.36.0:‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/3\u001b[0m [typer-slim]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.0m0/3\u001b[0m [typer-slim]\n",
      "\u001b[2K  Attempting uninstall: transformers[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/3\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.1m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/3\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling transformers-4.57.1:m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/3\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.190m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 1.0.1 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.1.2 which is incompatible.\n",
      "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-1.1.2 transformers-5.0.0.dev0 typer-slim-0.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"git+https://github.com/huggingface/transformers.git\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
