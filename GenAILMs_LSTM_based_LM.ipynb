{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbg26Io5TPx6"
      },
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcACtFj5xrfa"
      },
      "source": [
        "# Building a Language Model with LSTMs in Keras\n",
        "In this notebook, we will learn how to build a simple language model for text generation using Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) units with TensorFlow and Keras. We'll be using the complete works of William Shakespeare as our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1bIfW0U_77T"
      },
      "source": [
        "#GPUs?\n",
        "\n",
        "\n",
        "Make sure you have your GPU available. While LSTMs process sequences step-by-step, many internal operations (like the gate computations) can be efficiently parallelized on a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-macos in ./.venv/lib/python3.11/site-packages (2.16.2)\n",
            "Requirement already satisfied: tensorflow==2.16.2 in ./.venv/lib/python3.11/site-packages (from tensorflow-macos) (2.16.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.5)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.75.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.11.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./.venv/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.26.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2->tensorflow-macos) (0.45.1)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (14.1.0)\n",
            "Requirement already satisfied: namex in ./.venv/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.0)\n",
            "Requirement already satisfied: optree in ./.venv/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow-metal in ./.venv/lib/python3.11/site-packages (1.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in ./.venv/lib/python3.11/site-packages (from tensorflow-metal) (0.45.1)\n",
            "Requirement already satisfied: six>=1.15.0 in ./.venv/lib/python3.11/site-packages (from tensorflow-metal) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow-macos\n",
        "%pip install tensorflow-metal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "devices = tf.config.list_physical_devices()\n",
        "print(\"\\nDevices: \", devices)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  details = tf.config.experimental.get_device_details(gpus[0])\n",
        "  print(\"GPU details: \", details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'tensorflow._api.v2.test' has no attribute 'is_mps_available'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices())\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMPS available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_mps_available\u001b[49m())\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.test' has no attribute 'is_mps_available'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Devices:\", tf.config.list_physical_devices())\n",
        "print(\"MPS available:\", tf.test.is_mps_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwDw3xhsyFn7"
      },
      "source": [
        "#Setup and Imports\n",
        "First, let's import all the necessary libraries. We'll be using tensorflow for building our neural networks, numpy for numerical operations, and requests to download our datase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBQSNwrpyNNN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import requests\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB8Ue-UUye3y"
      },
      "source": [
        "# Download and Prepare the Dataset\n",
        "We'll use the \"Complete Works of Shakespeare\" from Project Gutenberg. The following code will download the text file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQDcwe7dxrNg"
      },
      "outputs": [],
      "source": [
        "# Download the Shakespeare dataset\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Save to a local file (optional)\n",
        "with open('shakespeare.txt', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "print(\"Dataset downloaded. Length of text: {} characters\".format(len(text)))\n",
        "print(\"\\nFirst 250 characters of the dataset:\")\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0LIkb1PyqOB"
      },
      "source": [
        "# Preprocessing the Text\n",
        "Our model can't understand raw text. We need to convert the text into a numerical format. This process involves a few key steps:\n",
        "\n",
        "- Tokenization: We'll break the text down into individual words (tokens).\n",
        "\n",
        "- Creating a Vocabulary: We'll build a vocabulary of all the unique words in our text.\n",
        "\n",
        "-  Creating Sequences: We'll create input-output pairs. The model will learn to predict the next word given a sequence of preceding words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0UTYPX4xrAl"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\") ##tensorflow.keras.preprocessing.text function, we indicate that every unknown word is mapped to a fixed index (Out Of Vocabulary, OOV)\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"Total unique words: {total_words}\")\n",
        "\n",
        "\n",
        "#This code is used to generate input sequences for training a next-word prediction model. It works by:\n",
        "# -Taking each line of the text,\n",
        "# -Tokenizing it into integers,\n",
        "# -Creating all possible n-gram sequences from that line,\n",
        "# -Appending them to a list.\n",
        "# This prepares the dataset so that, given a sequence of words, the model learns to predict the next one.\n",
        "\n",
        "# Create input sequences for training a next-word prediction model\n",
        "input_sequences = []\n",
        "\n",
        "# Loop over each line in the text to get several training samples for each line of the text.\n",
        "#This step is essential because instead of training the model on just full sentences,\n",
        "#we train it on many partial sequences. This helps the model learn how to complete text step by step.\n",
        "for line in text.split('\\n'):\n",
        "    # Convert the line of text into a sequence of integers (tokens)\n",
        "    # Example: \"I love AI\" -> [12, 45, 78]\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    # Create n-gram sequences from the token list\n",
        "    # For the example above, it would produce:\n",
        "    # [12, 45], [12, 45, 78]\n",
        "    for i in range(1, len(token_list)):\n",
        "        # Each sequence contains tokens from the start up to position i\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "\n",
        "        # Add this n-gram sequence to the list of input sequences\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences# IMPORTANT NOTE:\n",
        "# While LSTM layers in theory can process sequences of varying lengths,\n",
        "# in practice, Keras (and most deep learning frameworks) require all inputs\n",
        "# in a batch to have the same shape for computational efficiency.\n",
        "# Therefore, we must pad shorter sequences with zeros so they all match the same length.\n",
        "# This is why we calculate the maximum sequence length and apply 'pre' padding (padding at the beginning).\n",
        "# This way, the most recent tokens (more informative for prediction) are at the end of the sequence.\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Create predictors and label\n",
        "\n",
        "# Split the input sequences into input (xs) and target output (labels)\n",
        "# xs: all tokens in the sequence except the last one (input for the model)\n",
        "# labels: the last token in each sequence (this is what the model should learn to predict)\n",
        "xs, labels = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors, since the output layer\n",
        "# will predict a probability distribution over all possible words (vocabulary)\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "\n",
        "# Display some training examples\n",
        "print(\"\\n--- Training Examples ---\")\n",
        "print(\"Example Input Sequence (numerical):\", xs[5])\n",
        "print(\"Corresponding Next Word (numerical):\", labels[5])\n",
        "\n",
        "# Let's see the word corresponding to the numerical label\n",
        "print(\"Corresponding Next Word (text):\", tokenizer.index_word[labels[5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_7PShfe2RDE"
      },
      "source": [
        "# Building the Models\n",
        "We'll build a few different models to see how their architectures affect their performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfJli8E42Wt0"
      },
      "source": [
        "## Model 1: A Simple RNN\n",
        "We'll start with a basic SimpleRNN layer. This will introduce the fundamental concepts.\n",
        "\n",
        "- Embedding Layer: This layer learns a dense vector representation for each word in our vocabulary. This allows the model to capture semantic relationships between words.\n",
        "\n",
        "- SimpleRNN Layer: This is our recurrent layer that processes the sequence of word embeddings.\n",
        "\n",
        "- Dense Layer: This is the output layer that will predict the probability of each word in the vocabulary being the next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0cxyRHmxqy1"
      },
      "outputs": [],
      "source": [
        "# Build a simple RNN-based language model using the Keras Sequential API\n",
        "\n",
        "# We define a Sequential model where each layer feeds directly into the next\n",
        "model_rnn = Sequential([\n",
        "\n",
        "    # Embedding layer:\n",
        "    # This layer turns word indices (integers) into dense vectors of fixed size.\n",
        "    # - input_dim: total number of unique words in the vocabulary\n",
        "    # - output_dim: size of each word vector (here, 100)\n",
        "    # - input_length: length of input sequences (all padded to the same length)\n",
        "    #\n",
        "    # Output shape: (batch_size, sequence_length, embedding_dim)\n",
        "    # For example: (None, 15, 100) if sequence length is 15\n",
        "    Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_len - 1),\n",
        "\n",
        "    # SimpleRNN layer:\n",
        "    # A basic recurrent neural network layer that processes the sequence.\n",
        "    # It outputs a fixed-size vector summarizing the input sequence.\n",
        "    # Output shape: (batch_size, 150)\n",
        "    SimpleRNN(150),\n",
        "\n",
        "    # Dense output layer:\n",
        "    # - total_words units (one per word in the vocabulary)\n",
        "    # - softmax activation to produce a probability distribution over all words\n",
        "    #\n",
        "    # Output shape: (batch_size, total_words)\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model:\n",
        "# - loss: categorical crossentropy for multi-class classification\n",
        "# - optimizer: Adam is a good default choice\n",
        "# - metric: we track accuracy to monitor training performance\n",
        "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Explicitly build the model with a known input shape\n",
        "# This ensures that model.summary() can display full output shapes instead of '?'\n",
        "# 'None' here refers to a variable batch size (we don't fix it)\n",
        "model_rnn.build(input_shape=(None, max_sequence_len - 1))\n",
        "\n",
        "# Show the model architecture and the shape of each layer‚Äôs output\n",
        "model_rnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pTm1LY-5t2x"
      },
      "source": [
        "### Understanding the Output Shape of each layer\n",
        "\n",
        "###Tensors\n",
        "\n",
        "\n",
        "A 1D tensor is a vector: a simple array of numbers, for example, [3, 5, 7]. It has a shape like (n,) where n is the number of elements. A 2D tensor is a matrix: an array of vectors (1D tensors). For example, a matrix with shape (m, n) can be seen as m rows, each a vector of length n. A 3D tensor is an array of matrices (2D tensors). It can be thought of as a stack of 2D tensors, with shape (p, m, n), where: p is the number of matrices (or ‚Äúslices‚Äù in the stack), m is the number of rows in each matrix, n is the number of columns in each matrix. In other words, A 3D tensor is like a nested array of arrays of arrays.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Embedding layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "When you see an output shape like: *(batch_size, sequence_length, embedding_dim)* it describes a 3D tensor with three dimensions:\n",
        "\n",
        "- batch_size: The number of examples processed simultaneously in one batch. This dimension is often None in Keras, meaning it can vary (e.g., 32, 64, or any number of examples).\n",
        "\n",
        "- sequence_length: The length of each input sequence, i.e., how many tokens (words or subwords) each example contains. For example, if you feed sequences of 15 tokens, this dimension will be 15.\n",
        "\n",
        "- embedding_dim: The size of the vector that represents each token after embedding. This is the ‚Äúdense vector‚Äù dimension you choose in the embedding layer (e.g., 100).\n",
        "\n",
        "Imagine you have a batch of 2 sequences, each sequence has 3 tokens, and your embedding dimension is 4. Then your tensor looks like this:\n",
        "\n",
        "\n",
        "```\n",
        "[\n",
        "  [  # Example 1 of batch\n",
        "    [0.1, 0.2, 0.3, 0.4],  # Token 1 vector of sequence (embedding of 4 dimensions)\n",
        "    [0.5, 0.6, 0.7, 0.8],  # Token 2 vector of sequence (embedding of 4 dimensions)\n",
        "    [0.9, 1.0, 1.1, 1.2]   # Token 3 vector of sequence (embedding of 4 dimensions)\n",
        "  ],\n",
        "  [  # Example 2 of batch\n",
        "    [0.11, 0.12, 0.13, 0.14],\n",
        "    [0.15, 0.16, 0.17, 0.18],\n",
        "    [0.19, 0.20, 0.21, 0.22]\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "This is an array of two examples, each containing a sequence of 3 tokens, where each token is represented by a 4-dimensional vector.\n",
        "\n",
        "###SimpleRNN and Dense layers\n",
        "When building a sequential model, you typically specify the input shape only in the first layer (like the Embedding layer). Keras will automatically infer the output shape of each layer and use that as the input shape for the next layer.\n",
        "\n",
        "The SimpleRNN layer has a output shape of *(None, 150)* because it has 150 neurons (units).  It means the layer outputs a vector of length 150 for each example in the batch. This vector is a learned summary of the input sequence.\n",
        "\n",
        "Finally, the Dense layer output shape *(None, 12633)* equals the size of your vocabulary (total_words). The Dense layer produces a vector of length 12,633 representing scores (logits) for each possible next word in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnzEPEmZ2kR3"
      },
      "source": [
        "## Model 2: A Single LSTM Layer\n",
        "LSTMs are a type of RNN that are better at learning long-term dependencies in data. This is crucial for language, where the meaning of a word can depend on context from much earlier in a sentence or paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4o1Ohf0xgRt"
      },
      "outputs": [],
      "source": [
        "model_lstm = Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
        "    LSTM(150),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.build(input_shape=(None, max_sequence_len - 1))\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbDEXm9_2qqe"
      },
      "source": [
        "## Model 3: A Multi-Layer LSTM\n",
        "\n",
        "Stacking multiple LSTM layers can allow the model to learn more complex patterns in the data at different timescales.\n",
        "\n",
        "Note that the first LSTM returns the full sequence (all time steps) because the second LSTM needs a sequence input.\n",
        "\n",
        "The second LSTM returns only the last output (default return_sequences=False), since after the last LSTM you usually want a single vector to feed the Dense layer for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boO_y99A2wac"
      },
      "outputs": [],
      "source": [
        "model_multi_lstm = Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
        "    LSTM(150, return_sequences=True), # return_sequences=True is needed to stack LSTMs\n",
        "    LSTM(100),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model_multi_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_multi_lstm.build(input_shape=(None, max_sequence_len - 1))\n",
        "model_multi_lstm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tih5zoEv7i3M"
      },
      "source": [
        "# Training the Model\n",
        "For this tutorial, we'll train our multi-lstm-layer LSTM model. Training deep learning models on large datasets can take a significant amount of time.\n",
        "\n",
        "To keep this runnable in a reasonable time on Google Colab, we'll use a small subset of the data and train for only a few epochs. The results won't be perfect, but it will demonstrate the process.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-duX72tFWil"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For demonstration purposes, we'll use a smaller portion of the dataset.\n",
        "# In real scenarios, you should use the full dataset (xs, ys) for better results.\n",
        "xs_small = xs[:20000]\n",
        "ys_small = ys[:20000]\n",
        "\n",
        "print(\"Splitting data into training and validation sets...\")\n",
        "\n",
        "# Split the small subset into training and validation sets (80% train, 20% val)\n",
        "x_train, x_val, y_train, y_val = train_test_split(xs_small, ys_small, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training on a smaller subset of the data (with validation).\")\n",
        "\n",
        "# Train the model with validation data to monitor generalization\n",
        "history_lstm = model_multi_lstm.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_val, y_val),\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbiVfVe-GB75"
      },
      "source": [
        "# Evaluating the Model\n",
        "\n",
        "As seen, when you train a model in Keras with validation_data=(x_val, y_val), the output per epoch looks like this:\n",
        "```\n",
        "250/250 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 15ms/step - accuracy: 0.3734 - loss: 2.9888 - val_accuracy: 0.0747 - val_loss: 7.7836\n",
        "\n",
        "```\n",
        "This is just an output example, your results may be different.\n",
        "\n",
        "Interpretation:\n",
        "- The training accuracy is around 37% , which means the model is learning patterns from the training data.\n",
        "- However, the validation accuracy is much lower (7.4%), and validation loss is much higher (7.78 vs 2.98).\n",
        "-This suggests overfitting: the model performs better on data it has seen and much worse on new data.\n",
        "\n",
        "This is common in small models or with limited data. You can improve generalization by:\n",
        "\n",
        "- Training longer with early stopping\n",
        "\n",
        "- Adding dropout or regularization\n",
        "\n",
        "- Increasing the dataset size\n",
        "\n",
        "- Using better architecture (like bidirectional LSTM or attention)\n",
        "\n",
        "\n",
        "The next section of code is used to visualize how your model performed during training. It helps you diagnose problems like overfitting, underfitting, or whether your model is still improving.\n",
        "\n",
        "It uses `history_lstm.history`, a dictionary that stores all the metrics recorded during training, epoch by epoch. history_lstm is an object (specifically, an instance of the History class) returned by the model.fit() function in Keras. And .history is an attribute of that object.\n",
        "\n",
        "A decreasing curve of *loss* means the model is learning. If the training loss continues to decrease but validation loss increases, your model is overfitting. As with loss, if training *accuracy* increases while validation accuracy remains low or decreases, it's a sign of overfitting.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSGC8zCcFZNO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the model on the validation set after training\n",
        "val_loss, val_accuracy = model_multi_lstm.evaluate(x_val, y_val, verbose=0)\n",
        "print(f\"\\nValidation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Plot the training and validation loss over epochs\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history_lstm.history['loss'], label='Training Loss')\n",
        "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optionally: plot accuracy if you're tracking it\n",
        "if 'accuracy' in history_lstm.history:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history_lstm.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dbj8aXxrxB"
      },
      "source": [
        "# Generating Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0SzD9EaKIaf"
      },
      "source": [
        "##Generate_text function\n",
        "\n",
        "Now that our model is trained, let's use it to generate some text! We'll create a function that takes a starting text (a \"seed\") and generates a sequence of words.\n",
        "\n",
        "We'll also introduce the concept of temperature. Temperature is a parameter that controls the randomness of the predictions.\n",
        "\n",
        "- A low temperature will make the model more confident and deterministic, leading to less surprising but potentially more repetitive text.\n",
        "\n",
        "- A high temperature will increase the randomness, leading to more creative and diverse, but also potentially more nonsensical, text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CgmT7oWI2u6"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text from a given seed text using a trained model and temperature-controlled sampling.\n",
        "\n",
        "    Arguments:\n",
        "    - seed_text: initial string to begin text generation (can be empty \"\")\n",
        "    - next_words: number of words to generate.\n",
        "    - model: trained language model (e.g., LSTM).\n",
        "    - max_sequence_len: maximum length used during training for padding.\n",
        "    - tokenizer: tokenizer used to convert words to integers.\n",
        "    - temperature: controls randomness. Lower = more predictable; Higher = more creative.\n",
        "\n",
        "    Returns:\n",
        "    - A string containing the seed and generated text.\n",
        "    \"\"\"\n",
        "    generated_text = seed_text\n",
        "\n",
        "    #loop that will be executed once for each word you want to generate\n",
        "    for _ in range(next_words):\n",
        "        # Convert current text (intially the seed) to a sequence of integers using the same tokenizer as in traning\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        # Pad the sequence to match the input length expected by the model (as in training)\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict the probability distribution for the next word\n",
        "        # This returns a vector of probabilities for each word in the vocabulary\n",
        "        # For example: [0.10, 0.05, 0.60, 0.15, 0.10] for a vocab of size 5\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature to adjust the randomness of predictions.\n",
        "        # Temperature affects how confident or diverse the next word selection is.\n",
        "        # 1. Take the logarithm of the probabilities.\n",
        "        #    Logarithms help spread out small probabilities and shrink large ones.\n",
        "        #    We add a tiny value (1e-8) to avoid log(0), which is undefined.\n",
        "        log_preds = np.log(predicted_probs + 1e-8)\n",
        "\n",
        "        # 2. Divide by the temperature.\n",
        "        #    - Low temperature (< 1.0): makes the distribution sharper (less random, more predictable)\n",
        "        #    - High temperature (> 1.0): makes it softer (more random, more creative)\n",
        "        # Example:\n",
        "        # Original probs:       [0.10, 0.05, 0.60, 0.15, 0.10]\n",
        "        # With temp = 0.5: sharpens ‚Üí more likely to pick 0.60\n",
        "        # With temp = 1.5: flattens ‚Üí more chance for lower-probability words\n",
        "        # With temp =1, orginal probability distribution\n",
        "        adjusted_log_preds = log_preds / temperature\n",
        "\n",
        "        # 3. Convert back from log scale using exponentials\n",
        "        exp_preds = np.exp(adjusted_log_preds)\n",
        "\n",
        "        # 4. Normalize the distribution to make it a valid probability vector again\n",
        "        # This is like softmax ‚Äî ensures all values sum to 1\n",
        "        predicted_probs = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Now predicted_probs is ready for sampling the next word using np.random.choice\n",
        "        # It has been adjusted by temperature to control creativity\n",
        "\n",
        "\n",
        "        # Randomly choose the next word based on the probabilities\n",
        "        predicted_id = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "\n",
        "        # Convert predicted token index back to word\n",
        "        output_word = tokenizer.index_word.get(predicted_id, \"\")\n",
        "\n",
        "\n",
        "        # Add the predicted word to the ongoing text\n",
        "        seed_text += \" \" + output_word #for the next iteration of the loop\n",
        "        generated_text += \" \" + output_word #for final ouput.\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcTvVqyWOoZa"
      },
      "source": [
        "##Generating Text Starting from an Empty Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10gZ1fxyJhB9"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Generating Text Starting from an Empty Seed ---\")\n",
        "\n",
        "# Start from an empty string\n",
        "empty_seed = \"\"\n",
        "\n",
        "# Generate text with temperature = 1.0 (balanced creativity)\n",
        "generated = generate_text(seed_text=empty_seed,\n",
        "                          next_words=50,\n",
        "                          model=model_lstm,\n",
        "                          max_sequence_len=max_sequence_len,\n",
        "                          tokenizer=tokenizer,\n",
        "                          temperature=1.0)\n",
        "\n",
        "print(\"\\nGenerated Text:\\n\", generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g5McR3VOsng"
      },
      "source": [
        "## Generating Text from Seed\n",
        "\n",
        "We will use \"to be or not to be\" as seed.\n",
        "\n",
        "Though none of these completions form perfect sentences, they show how adjusting temperature lets you control the trade-off between safe, predictable text and bold, novel expressions ‚Äî a key feature in creative AI text generation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW1MN19TI31-"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Generating Text from Seed ---\")\n",
        "seed = \"to be or not to be\"\n",
        "\n",
        "print(\"\\n... with temperature 0.2 (very conservative and predictable)\")\n",
        "print(generate_text(seed, 10, model_lstm, max_sequence_len, tokenizer, temperature=0.2))\n",
        "\n",
        "print(\"\\n... with temperature 1.0 (balanced creativity and coherence)\")\n",
        "print(generate_text(seed, 10, model_lstm, max_sequence_len, tokenizer, temperature=1.0))\n",
        "\n",
        "print(\"\\n... with temperature 1.5 (more creative, possibly less grammatical)\")\n",
        "print(generate_text(seed, 20, model_lstm, max_sequence_len, tokenizer, temperature=1.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO0FjpOLhjlC"
      },
      "source": [
        "##Checking tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxj_84VThxkc"
      },
      "source": [
        "It s a good idea to add a small helper function to verify that text-to-index and index-to-text mapping are consistent. This ensures the tokenizer mapping used in generation is correct and reversible.\n",
        "\n",
        "Deploying Deep Learning systems for NLP is largely about data preprocessing, rather than selecting one architecture or another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCJILDv5hoFc"
      },
      "outputs": [],
      "source": [
        "def check_tokenizer_mapping(tokenizer, sample_text):\n",
        "    \"\"\"\n",
        "    Check that texts_to_sequences and index_word mapping are consistent.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted Keras Tokenizer.\n",
        "        sample_text: A string whose tokens will be tested.\n",
        "\n",
        "    Prints the original words, their integer indices, and the words recovered via index_word mapping.\n",
        "    \"\"\"\n",
        "    # Tokenize the sample text into word indices\n",
        "    tokens = tokenizer.texts_to_sequences([sample_text])[0]\n",
        "    print(\"Original words:\", sample_text.split())\n",
        "    print(\"Token indices:    \", tokens)\n",
        "\n",
        "    # Map indices back to words using index_word (UNKnown if not found)\n",
        "    recovered = [tokenizer.index_word.get(i, \"<UNK>\") for i in tokens]\n",
        "    print(\"Recovered words: \", recovered)\n",
        "\n",
        "    # Compare\n",
        "    print(\"\\nMapping check:\", [\"OK\" if orig.lower() == rec else \"???\"\n",
        "                               for orig, rec in zip(sample_text.split(), recovered)])\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "sample = \"I love AI and machine learning, but Shakespeare did not write about them\"\n",
        "check_tokenizer_mapping(tokenizer, sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o149T_p8jYG"
      },
      "source": [
        "#Conclusion and Next Steps\n",
        "Congratulations! You've successfully built and trained a language model to generate text in the style of Shakespeare.\n",
        "\n",
        "Here are some ideas for how you could continue to explore and improve this project:\n",
        "\n",
        "- Train on the full dataset: For better results, train the model on all of the data (xs and ys).\n",
        "\n",
        "- Train for more epochs: The longer you train, the better the model will learn the patterns of the language.\n",
        "\n",
        "- Experiment with model architecture: Try adding more LSTM layers, changing the number of units in each layer, or adding Dropout layers to prevent overfitting.\n",
        "\n",
        "- Hyperparameter tuning: Experiment with the Embedding dimension, learning rate, and other hyperparameters.\n",
        "\n",
        "- Character-level model: Instead of tokenizing by words, you could build a model that predicts the next character. This can be more flexible for handling unknown words.\n",
        "\n",
        "- Use a different dataset: Try training a model on a different author's work, or a different type of text altogether!\n",
        "\n",
        "This notebook provides a foundational understanding of how to build language models. The field of Natural Language Processing is vast and rapidly evolving, but these core concepts will serve you well as you continue to learn."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
