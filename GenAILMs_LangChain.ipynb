{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB3tAg3iP3aY"
      },
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad Polit√©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "üìò License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share ‚Äî copy and redistribute the material in any medium or format; (2) Adapt ‚Äî remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial ‚Äî You may not use the material for commercial purposes; (3) ShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "üîó License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRN02HknP46c"
      },
      "source": [
        "# Prompt Engineering with LangChain: Building LLM-Powered Applications\n",
        "\n",
        "In this notebook, we‚Äôll explore how to build fast and modular Generative AI applications using LangChain and Groq.\n",
        "\n",
        "[LangChain](https://www.langchain.com/) is a flexible framework designed to help developers integrate large language models (LLMs) into real-world applications. It provides tools for chaining together components like prompt templates, memory, agents, and access to external tools or data‚Äîmaking it ideal for building intelligent, composable workflows.\n",
        "\n",
        "[Groq](https://groq.com/) offers an inference API optimized for ultra-low latency and high-throughput LLM execution. It enables real-time performance with state-of-the-art models such as LLaMA 3 and Gemma, making it an excellent option when responsiveness is critical.\n",
        "\n",
        "‚ö†Ô∏è While we use Groq here for convenience and speed, LangChain is model-agnostic. You can easily swap Groq for other backends like OpenAI, Hugging Face, or Anthropic. You can also run open-source models locally on your own machine using tools like LM Studio, Ollama, or Text Generation WebUI‚Äîideal for development without API limits or cloud dependency.\n",
        "\n",
        "By combining LangChain‚Äôs modularity with a fast inference backend like Groq (or a local setup), you‚Äôll learn how to prototype and deploy efficient, production-ready LLM-powered solutions.\n",
        "\n",
        "Learning Objectives:\n",
        "\n",
        "- Connect and use the Groq API via LangChain to run Large Language Models (LLMs).\n",
        "\n",
        "- Start with a simple, direct prompt and see its limitations.\n",
        "\n",
        "- Create structured prompts with roles (system, user) and templates to guide the model.\n",
        "\n",
        "- Enforce JSON output for easy integration with other software.\n",
        "\n",
        "- Understand and apply prompting techniques like Zero-shot, One-shot, and Few-shot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkK_mSGqQedC"
      },
      "source": [
        "# Environment Setup\n",
        "First, we need to install the necessary libraries and configure our Groq API key.\n",
        "\n",
        "No GPU is needed because Groq's servers will do the heavy lifting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp6hZJo2gEh4"
      },
      "source": [
        "\n",
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKqSyVeFf9Sj"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries for the notebook\n",
        "%pip install -q langchain langchain-groq python-dotenv faiss-cpu langchain_huggingface beautifulsoup4 langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUsrjNpvQeQt"
      },
      "source": [
        "## Configure Groq API Key\n",
        "\n",
        "To use the Groq API, you need a key.\n",
        "\n",
        "- Go to https://console.groq.com/keys and sign up.\n",
        "\n",
        "- Create a new API Key and copy it.\n",
        "\n",
        "If you're working with API keys (like for Groq, OpenAI, or Hugging Face), it's best to avoid hardcoding them directly in your notebook. Instead, use Colab‚Äôs secrets manager:\n",
        "\n",
        "- Click the üîë key icon on the left sidebar (labeled \"Secrets\") in Colab.\n",
        "\n",
        "- Add your secret (GROQ_API_KEY) there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTPWmTPNQepf"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "#Using google.colab secrets\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"üõë Groq API Key not found. Please make sure to set it up.\")\n",
        "else:\n",
        "    print(\"‚úÖ Groq API Key configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilw7X6lQh_Rj"
      },
      "source": [
        "## Selecting a LLM\n",
        "\n",
        "We will set up the LLaMA 3 8B model from Meta ‚Äî an open-source LLM ‚Äî running via Groq‚Äôs optimized hardware backend.\n",
        "\n",
        "Groq currently serves only open-weight models, which means you can inspect their architecture and, in many cases, run them locally if you choose. Examples include:\n",
        "\n",
        "- llama3-8b-8192 and llama3-70b-8192 (Meta)\n",
        "\n",
        "- gemma-7b-it (Google)\n",
        "\n",
        "- mixtral-8x7b (Mistral)\n",
        "\n",
        "- deepseek-r1-distill-llama-70b (DeepSeek & Meta)\n",
        "\n",
        "[Groq‚Äôs Model Explorer](https://console.groq.com/docs/models) lists each model‚Äôs:\n",
        "\n",
        "- Context window (maximum input length, e.g., 8192 tokens)\n",
        "\n",
        "- Max output tokens (the maximum number of tokens a model can generate per call)\n",
        "\n",
        "- Model family and version\n",
        "\n",
        "- Inference speed estimates\n",
        "\n",
        "This helps you pick the right model depending on your task ‚Äî for example, summarization of long documents may benefit from a larger context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G0uBd0EPiEQi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/federicosvendsen/Documents/UPM/DeepLearning4NLP/1-7/script/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'api_key' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize the Chat model with Groq\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We'll use Llama3 8B, a fast and competent model\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m llm = ChatGroq(model_name=\u001b[33m\"\u001b[39m\u001b[33mllama-3.1-8b-instant\u001b[39m\u001b[33m\"\u001b[39m, groq_api_key=\u001b[43mapi_key\u001b[49m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLanguage Model initialized with Groq.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'api_key' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize the Chat model with Groq\n",
        "# We'll use Llama3 8B, a fast and competent model\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", groq_api_key=api_key)\n",
        "\n",
        "\n",
        "print(\"Language Model initialized with Groq.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Ofqi7D4PG1"
      },
      "source": [
        "## LLM Hyperparameters\n",
        "\n",
        "Language models accept not just a prompt, but also hyperparameters that modify their behavior. One of the most important is the temperature.\n",
        "\n",
        "- **Low temperature** (e.g., 0.0 - 0.2): Makes the model more deterministic and predictable. It will choose the most likely words, which is ideal for fact-based tasks, summarization, or formatting.\n",
        "\n",
        "- **High temperature** (e.g., 0.8 - 1.2): Increases randomness. The model might choose less likely words, encouraging creativity, diversity of ideas, and \"personality.\" This is great for brainstorming, creative writing, or chatbots with character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht-h5X8I51O3"
      },
      "outputs": [],
      "source": [
        "# --- Low-Temperature Model (Predictable) ---\n",
        "# Perfect for tasks requiring consistency and precision.\n",
        "llm_low_temp = ChatGroq(model_name=\"llama-3.1-8b-instant\",temperature=0.1, groq_api_key=api_key)\n",
        "\n",
        "\n",
        "# --- High-Temperature Model (Creative) ---\n",
        "# Ideal for generating new ideas or varied writing styles.\n",
        "llm_high_temp = ChatGroq(model_name=\"llama-3.1-8b-instant\",temperature=1, groq_api_key=api_key)\n",
        "\n",
        "# The same prompt for both models\n",
        "prompt = \"Write a marketing slogan for a new GenAI application to make poetry.\"\n",
        "\n",
        "print(\"--- ü§ñ Low-Temperature Model (Predictable & Focused) ---\")\n",
        "response_low = llm_low_temp.invoke(prompt)\n",
        "print(f\"Slogan: {response_low.content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"--- üé® High-Temperature Model (Creative & Surprising) ---\")\n",
        "response_high = llm_high_temp.invoke(prompt)\n",
        "print(f\"Slogan: {response_high.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m5FRo1I8lMG"
      },
      "source": [
        "Other hyperpameters include:\n",
        "\n",
        "- üé≤**top_p**(Nucleus Sampling), An alternative way to control randomness. Instead of selecting from all possible tokens, it samples from the top p% of most likely next tokens. Typical range: 0.8 ‚Äì 0.95. Works well in combination with temperature.\n",
        "- üß± **max_tokens**, Sets the maximum number of tokens (words/pieces) in the model's response. Useful to limit verbosity or enforce concise output. Prevents runaway generation in long-form completions.\n",
        "- ‚õî **stop**, Defines a list of strings where generation should halt. Useful for enforcing boundaries in structured formats (e.g., JSON, multi-turn dialogue, code snippets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls5poYbBlO_E"
      },
      "source": [
        "# A First Call to Groq: The Simple Way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uaNuKNolaLt"
      },
      "source": [
        "Before diving into LangChain's advanced features, let's see the most basic way to call the model. We can put all our instructions‚Äîthe task, the context, the input, and the desired output format‚Äîinto a single, long string.\n",
        "\n",
        "In this example, we simulate a real-world E-commerce use case: analyzing a customer's product review using a prompt-based approach with a large language model (LLM).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNp_2SyGlVR2"
      },
      "outputs": [],
      "source": [
        "# The customer review we want to analyze\n",
        "\n",
        "simple_prompt = \"\"\"\n",
        "You are an expert sentiment analyst for an E-commerce company.\n",
        "Your task is to analyze the customer's product review and provide your analysis.\n",
        "The output must be a JSON object with three keys: \"sentiment\" (string), \"summary\" (string), and \"rating\" (integer from 1 to 5).\n",
        "Only respond with the JSON object and nothing else.\n",
        "\n",
        "Customer Review: \"The keyboard is fantastic, the keys are smooth and the RGB lighting is spectacular.\n",
        "The only downside is that the cable is a bit short, but otherwise, a great buy!\"\n",
        "\n",
        "JSON Output:\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Sending Simple Prompt to LLM ---\")\n",
        "response = llm.invoke(simple_prompt)\n",
        "# The `invoke` method sends the `simple_prompt` string to the language model (llm).\n",
        "# The model processes the prompt, generates a response,\n",
        "# and returns that response as a BaseMessage object (containing the text output in \"content\").\n",
        "\n",
        "\n",
        "print(\"\\n--- Raw LLM Response ---\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMQu3h_6mF2y"
      },
      "source": [
        "This works!\n",
        "\n",
        "One major advantage of this method is that it does not require retraining or _fine-tuning_ the language model. Thanks to _in-context learning_, the model adapts to your task on the fly based solely on the prompt you provide.\n",
        "For example, if tomorrow you decide that the rating should be from 1 to 3 instead of 1 to 5, you simply update the prompt, and you're done ‚Äî no model retraining or redeployment needed.\n",
        "\n",
        "\n",
        "**Only with this code, you can perform a wide variety of prototypes that use the LLM as a universal AI Backend, you simply have to ask for the task you want to do in the prompt and collect the response in your prototype.**\n",
        "\n",
        "However, this simplistic use of prompting has several drawbacks:\n",
        "\n",
        "- Brittle: It's hard to separate the instruction template from the input data.\n",
        "\n",
        "- Hard to Manage: If you change the logic, you have to edit the string, which is error-prone.\n",
        "\n",
        "- No Guarantees: The model might return a valid JSON string, but it's not guaranteed. Any small deviation could break downstream software that expects perfect JSON.\n",
        "\n",
        "Now, let's see how LangChain helps us solve these problems more cleanly and reliably.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydXYVJvanUx9"
      },
      "source": [
        "# Structured Prompting with LangChain\n",
        "\n",
        "_Prompt Engineering_ refers to the practice of carefully crafting input prompts to guide a language model toward producing useful, accurate, or reliable outputs. Instead of retraining or fine-tuning a model, prompt engineering lets us control behavior and output by manipulating context.\n",
        "\n",
        "LangChain provides a modular and clean framework to implement structured prompts, which is especially useful for building robust applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alA35f5zpgBl"
      },
      "source": [
        "## The System Role\n",
        "In modern LLM interfaces (like ChatGPT or Groq), a prompt is not just the user‚Äôs message. It often consists of multiple parts, including:\n",
        "\n",
        "- **A system message**, which sets the tone, role, and global behavior of the model.\n",
        "\n",
        "- **The user message**, which provides the actual task or input.\n",
        "\n",
        "- **(Optionally) Assistant** or previous messages for conversational memory.\n",
        "\n",
        "The `system role` is traditionally used to establish rules, instructions, or a ‚Äúcode of conduct‚Äù that the model must follow throughout the entire conversation. It acts as an ‚Äúinvisible voice‚Äù defining the AI assistant‚Äôs overall behavior, tone, boundaries, and internal guidelines before the user interacts. For example, you might instruct the model: ‚ÄúYou are a medical assistant and never give legal diagnoses.‚Äù This role is essential to control the model‚Äôs behavior across different tasks ‚Äî whether as a polite assistant, a strict data validator, or a customer support agent. Defining the system role helps produce more predictable and consistent outputs, which is crucial when integrating LLMs into production-level applications.\n",
        "\n",
        "Note: Some APIs use the term `developer role` instead of system. While very similar, developer role messages often come with higher priority instructions from the app creator. Check your API‚Äôs docs for specifics.\n",
        "\n",
        "Let‚Äôs revisit the same problem: classifying a customer review. We'll now separate the prompt into system and user roles. Additionally, the input data (review text) will appear in the user prompt using a placeholder that will be instantiated when the chain is invoked. A \"chain\" (of LangChain) is a single object that, when called, automatically fills the prompt with the input data, sends it to the model, and returns the output.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fKNiEK-_kLr"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. Create the prompt template with system and user roles\n",
        "system_prompt_text = \"\"\"\n",
        "You are an expert sentiment analyst for an E-commerce company.\n",
        "Your task is to analyze a customer's product review and return the analysis in a structured JSON format.\n",
        "The output must be a JSON object with three keys: \"sentiment\" (string), \"summary\" (string), and \"rating\" (integer from 1 to 5).\n",
        "Only respond with the JSON object and nothing else.\n",
        "\"\"\"\n",
        "user_prompt_text = \"Customer Review: {review}\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt_text),\n",
        "    (\"user\", user_prompt_text)\n",
        "])\n",
        "#   ChatPromptTemplate.from_messages creates a ChatPromptTemplate by defining a sequence of messages\n",
        "#   Note that {review} in the user_prompt_text is a placeholder used with Python‚Äôs str.format() method.\n",
        "#   It‚Äôs designed to be dynamically replaced with the actual customer review text when generating the prompt.\n",
        "#   This makes the prompt reusable for analyzing different reviews by simply inserting the specific review content into the placeholder.\n",
        "\n",
        "# 2. Create our chain, which links the prompt to the model\n",
        "chain = prompt | llm\n",
        "\n",
        "#  What this means:\n",
        "#  The prompt template defines how we format the input text (including placeholders for dynamic content).\n",
        "#  The language model (llm) takes this formatted input and generates a response.\n",
        "#  By combining them into a \"chain,\" we create a single object that, when called,\n",
        "#  automatically fills the prompt with the input data, sends it to the model, and returns the output.\n",
        "#  This simplifies the workflow, making it easy to run the model with different inputs without rewriting code.\n",
        "\n",
        "# 3. Let's test it!\n",
        "customer_review = \"The keyboard is fantastic, the keys are smooth and the RGB lighting is spectacular. The only downside is that the cable is a bit short, but otherwise, a great buy!\"\n",
        "response = chain.invoke({\"review\": customer_review})\n",
        "\n",
        "# Here, we pass a dictionary that fills the placeholders in the prompt.\n",
        "# The chain automatically constructs the final prompt by inserting the values\n",
        "# into the template before sending it to the model.\n",
        "# This is different from the earlier approach with llm.invoke(simple_prompt),\n",
        "# where we manually crafted and sent the full prompt string without placeholders.\n",
        "\n",
        "print(\"\\n--- Raw LLM Response ---\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eII8i7o7xZdS"
      },
      "source": [
        "Better... but there's still a lot of additional information beyond the requested JSON. This makes it difficult to connect with other software that's only expecting that format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWMx8Oy8rorM"
      },
      "source": [
        "## Text Classification with Guaranteed JSON Output\n",
        "\n",
        "\n",
        "Now, using LangChain, we‚Äôll define the output format of the LLM.\n",
        "\n",
        "The   easiest and most reliable way to get structured outputs is using the method `with_structured_output()`  implemented for models that provide native APIs for structuring outputs.\n",
        "\n",
        "The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\n",
        "\n",
        "Here, we will define our own  `Pydantic` class. Pydantic is a library for defining and validating structured data.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGfeQB5HmFb3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# 1. Define the desired output structure using Pydantic\n",
        "class SentimentAnalysis(BaseModel):\n",
        "    sentiment: str = Field(description=\"The sentiment of the review, can be 'Positive', 'Negative', or 'Neutral'.\")\n",
        "    summary: str = Field(description=\"A concise, one-sentence summary of the customer's opinion.\")\n",
        "    rating: int = Field(description=\"A score from 1 to 5 based on the expressed sentiment.\")\n",
        "\n",
        "#  This Pydantic model defines the expected structure of the output from the LLM.\n",
        "#  When the LLM returns a response, it is parsed and validated into an instance of SentimentAnalysis.\n",
        "#  This means the output is not just raw JSON text, but a strongly-typed Python object\n",
        "#  that can be easily used in your application with guaranteed structure and type safety.\n",
        "\n",
        "# 2. Create a \"structured\" LLM that will enforce the output format of our Pydantic model\n",
        "structured_llm = llm.with_structured_output(SentimentAnalysis)\n",
        "#    .with_structured_output() is a method that takes a schema as input which specifies the names, types, and descriptions of the desired output attributes.\n",
        "#    The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema.\n",
        "#    The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class\n",
        "\n",
        "\n",
        "# 3. Create the prompt template with system and user roles\n",
        "system_prompt_text = \"\"\"\n",
        "You are an expert sentiment analyst for an E-commerce company.\n",
        "Your task is to analyze a customer's product review and return the analysis in a structured JSON format.\n",
        "\"\"\"\n",
        "user_prompt_text = \"Customer Review: {review}\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt_text),\n",
        "    (\"user\", user_prompt_text)\n",
        "])\n",
        "#   ChatPromptTemplate.from_messages creates a ChatPromptTemplate by defining a sequence of messages\n",
        "#   Note that {review} in the user_prompt_text is a placeholder used with Python‚Äôs str.format() method.\n",
        "#   It‚Äôs designed to be dynamically replaced with the actual customer review text when generating the prompt.\n",
        "#   This makes the prompt reusable for analyzing different reviews by simply inserting the specific review content into the placeholder.\n",
        "\n",
        "# 4. Create our chain, which links the prompt to the model\n",
        "chain = prompt | structured_llm\n",
        "\n",
        "#  What this means:\n",
        "#  The prompt template defines how we format the input text (including placeholders for dynamic content).\n",
        "#  The language model (structured_llm) takes this formatted input and generates a response.\n",
        "#  By combining them into a \"chain,\" we create a single object that, when called,\n",
        "#  automatically fills the prompt with the input data, sends it to the model, and returns the output.\n",
        "#  This simplifies the workflow, making it easy to run the model with different inputs without rewriting code.\n",
        "\n",
        "# 5. Let's test it!\n",
        "customer_review = \"The keyboard is fantastic, the keys are smooth and the RGB lighting is spectacular. The only downside is that the cable is a bit short, but otherwise, a great buy!\"\n",
        "response = chain.invoke({\"review\": customer_review})\n",
        "\n",
        "# Here, we pass a dictionary that fills the placeholders in the prompt.\n",
        "# The chain automatically constructs the final prompt by inserting the values\n",
        "# into the template before sending it to the model.\n",
        "# This is different from the earlier approach with llm.invoke(simple_prompt),\n",
        "# where we manually crafted and sent the full prompt string without placeholders.\n",
        "\n",
        "print(f\"Original Review: '{customer_review}'\\n\")\n",
        "print(\"--- Structured Analysis (JSON) ---\")\n",
        "print(f\"\\nType of response object: {type(response)}\")\n",
        "print(f\"Extracted rating: {response.rating}\")\n",
        "print(f\"Printing the response object: {response}\")\n",
        "# Note: Since SentimentAnalysis inherits from Pydantic's BaseModel, it provides a `.json()` method\n",
        "# that allows easy conversion of the object to a JSON-formatted string\n",
        "print(f\"Printing the response object after convert it to JSON: {response.json()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzaQIsMrwdXe"
      },
      "source": [
        "As you can see, the model has followed the instructions perfectly, returning a Pydantic object that can be easily used or converted to JSON. This is much more robust!\n",
        "\n",
        "\n",
        "Software engineering largely focuses on organizing code in a way that improves maintainability and scalability as projects grow. LangChain embodies this principle by providing modular components that help build, manage, and scale complex AI applications more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSOGCLNwx8hQ"
      },
      "source": [
        "# In-Context Learning: Zero-Shot, One-Shot, and Few-Shot\n",
        "LLMs can perform tasks differently based on the examples we provide in the prompt. This is called \"in-context learning.\"\n",
        "\n",
        "Let's illustrate this with a simple task: extracting the name of a technology from a text.\n",
        "\n",
        "This can be considered a type of word-level classification (also called token classification). This is similar to Named Entity Recognition (NER), where tokens are classified into entity types like Person, Location, Organization, etc. Here, the classes are simpler ‚Äî just ‚Äútechnology‚Äù vs ‚Äúnon-technology.‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFgL9_UiyB4R"
      },
      "source": [
        "## Zero-Shot Prompting  \n",
        "We give it no examples. We rely on the model's pre-existing knowledge to perform the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DuHRVscyX53"
      },
      "outputs": [],
      "source": [
        "# Task: Extract the name of the Python library mentioned.\n",
        "\n",
        "prompt_zero_shot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract the main technology or library mentioned in the following text.\"),\n",
        "    (\"user\", \"Text: {input}\")\n",
        "])\n",
        "\n",
        "chain_zero_shot = prompt_zero_shot | llm\n",
        "\n",
        "text = \"For data processing, people often use Pandas.\"\n",
        "response = chain_zero_shot.invoke({\"input\": text})\n",
        "\n",
        "print(\"--- Zero-Shot ---\")\n",
        "print(f\"Input Text: '{text}'\")\n",
        "print(f\"Model Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLhkGmYx4aGg"
      },
      "source": [
        "That‚Äôs fine. However, I only want the technology name, not an explanation. I could specify this in the system prompt or use `.with_structured_output` as before, but let‚Äôs try teaching the model with examples instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmn-3TMGzKxV"
      },
      "source": [
        "## One-Shot Prompting\n",
        "\n",
        "We provide a single example to show the model exactly what we want.\n",
        "\n",
        "`HumanMessage` and `AIMessage`  are special message types used in LangChain (and similar frameworks) to clearly distinguish between the messages coming from the human user and those coming from the AI model within a conversation or prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31zeQVU2zKAl"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Define a prompt with one example (one-shot learning) to guide the model.\n",
        "prompt_one_shot = ChatPromptTemplate.from_messages([\n",
        "    # System message: gives context and instructs the model what to do.\n",
        "    (\"system\", \"Extract the main technology or library mentioned in the following text.\"),\n",
        "\n",
        "    # Example interaction we provide to show the expected format of the output.\n",
        "    HumanMessage(content=\"Text: 'I love visualizing data with Matplotlib.'\"),\n",
        "    AIMessage(content=\"Matplotlib\"),\n",
        "\n",
        "    # The new input text, with a placeholder to be filled dynamically.\n",
        "    (\"user\", \"Text: {input}\")\n",
        "])\n",
        "\n",
        "# Create a chain that links the prompt template with the language model (LLM).\n",
        "chain_one_shot = prompt_one_shot | llm\n",
        "\n",
        "# New text input to analyze, which will fill the {input} placeholder in the prompt.\n",
        "new_text = \"We have deployed our service in a Kubernetes cluster.\"\n",
        "\n",
        "# Invoke the chain by passing the input dictionary to fill the placeholder.\n",
        "response = chain_one_shot.invoke({\"input\": new_text})\n",
        "\n",
        "# Print the input and the model's response.\n",
        "print(\"\\n--- One-Shot ---\")\n",
        "print(f\"Input Text: '{new_text}'\")\n",
        "print(f\"Model Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_2-cIeMzXCV"
      },
      "source": [
        "Great! We want the output to be just the technology name, without any extra explanation. This concise output is ideal for directly displaying in the GUI of my AI prototype."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utl6uFu0yr1t"
      },
      "source": [
        "##  Few-Shot Prompting  \n",
        "We provide several examples. This is very useful for complex tasks or when we want a very specific output format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChxboYMXy42B"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "# Define a small set of example input-output pairs to guide the model\n",
        "examples = [\n",
        "    {\"input\": \"I love visualizing data with Matplotlib.\", \"output\": \"Matplotlib\"},\n",
        "    {\"input\": \"For machine learning, I use Scikit-learn.\", \"output\": \"Scikit-learn\"},\n",
        "    {\"input\": \"Our backend is built with Django.\", \"output\": \"Django\"},\n",
        "]\n",
        "\n",
        "# Create a prompt template for each example, mapping user input to AI output\n",
        "example_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"{input}\"),  # Placeholder for the example input text\n",
        "    (\"ai\", \"{output}\"),   # Placeholder for the corresponding example output (technology)\n",
        "])\n",
        "\n",
        "# Use FewShotChatMessagePromptTemplate to automatically insert examples into the previous prompt template\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "# Construct the full prompt that the model will receive:\n",
        "# - system message sets the task instructions\n",
        "# - few-shot examples are included to show the model how to respond\n",
        "# - user message with the new input to analyze\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract the main technology or library mentioned in the text. Only return the name.\"),\n",
        "    few_shot_prompt,  # Insert the few-shot examples here for in-context learning\n",
        "    (\"user\", \"{input}\"),  # Placeholder for the actual user input text\n",
        "])\n",
        "\n",
        "# 1. Define the desired output structure using Pydantic\n",
        "class SentimentAnalysis(BaseModel):\n",
        "    technology: str = Field(description=\"Name of the technology or library being mentioned in the text.\")\n",
        "    technology2: str = Field(description=\"Name of another technology or library tbeing mentioned in the text, if non pick one that could be mostly related to the first one.\")\n",
        "\n",
        "structured_llm = llm.with_structured_output(SentimentAnalysis)\n",
        "#    .with_structured_output() is a method that takes a schema as input which specifies the names, types, and descriptions of the desired output attributes.\n",
        "#    The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema.\n",
        "#    The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class\n",
        "\n",
        "\n",
        "# Create a chain linking the prompt to the language model\n",
        "chain_few_shot = final_prompt | structured_llm\n",
        "\n",
        "# Example input text to test the few-shot prompt\n",
        "new_text  = \"We use pyspark to manage data.\"\n",
        "\n",
        "# Invoke the chain, filling the input placeholder and getting the model's response\n",
        "response = chain_few_shot.invoke({\"input\": new_text})\n",
        "\n",
        "# Print results\n",
        "print(\"\\n--- Few-Shot ---\")\n",
        "print(f\"Input Text: '{new_text}'\")\n",
        "print(f\"Model Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReaLUByT8tTW"
      },
      "source": [
        "In this example, we progressively construct a structured and reusable prompt by composing smaller building blocks ‚Äî moving step-by-step from low-level data toward a high-level, abstract interface.\n",
        "\n",
        "- Examples List. First, we define a list of example input-output pairs that show the model how to perform the task. This provides the data we want to use for in-context learning.\n",
        "\n",
        "- Example Prompt Template. We then define how each example should be formatted using a simple ChatPromptTemplate. This defines the shape of each \"mini-conversation\" the model will see.\n",
        "\n",
        "- Few-Shot Prompt Template. Using FewShotChatMessagePromptTemplate, we automatically apply the formatting from the example template to all our examples. This builds a reusable prompt component that inserts well-formatted examples into the final prompt.\n",
        "\n",
        "- Final Prompt Template. We then build the full prompt by combining:\n",
        "\n",
        "  * A system message that clearly defines the task (\"Extract the main technology...\"),\n",
        "\n",
        "  * The few-shot examples we defined,\n",
        "\n",
        "  *  A user message with a placeholder for new input.\n",
        "\n",
        "This step-by-step, modular approach lets us design complex prompting logic in a clean and maintainable way, which is especially useful when integrating LLMs into real applications.  \n",
        "\n",
        "...or you can always go back to the zero-shot Prompting code and simply paste your examples directly into the system message (worse maintenance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HufBsSvLAmpw"
      },
      "source": [
        "##Chuck Norrys Jokes\n",
        "\n",
        "Let's ask our LLM to provide [Chuck Norrys Jokes](https://psycatgames.com/es/magazine/conversation-starters/chuck-norris-jokes/) with zero-shot prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1MUOmvUAron"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Simple zero-shot prompt: we ask the model to generate a Chuck Norris joke in Spanish\n",
        "prompt_zero_shot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un generador de chistes de Chuck Norris. Devuelve solo un chiste en espa√±ol, sin explicaciones.\"),\n",
        "    (\"user\", \"Cu√©ntame un chiste de Chuck Norris.\")\n",
        "])\n",
        "\n",
        "# Link prompt to model\n",
        "chain_zero_shot = prompt_zero_shot | llm\n",
        "# Call the chain with no extra variables (no placeholders have been defined to be replaced in the prompt)\n",
        "response = chain_zero_shot.invoke({})\n",
        "\n",
        "# Display the result\n",
        "print(\"--- Zero-Shot with LLama 8B ---\")\n",
        "print(\"Input: Cu√©ntame un chiste de Chuck Norris.\")\n",
        "print(f\"Model Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rWvNKgFzEie"
      },
      "source": [
        "Then, using few-shot prompting, we'll try to force the model to use the formula \"Chuck Norris walks into a bar\" in the joke.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-5rqlPVzIXU"
      },
      "outputs": [],
      "source": [
        "# List of example Chuck Norris jokes in Spanish\n",
        "examples = [\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El hielo se derrite por respeto.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El camarero le pregunta qu√© quiere. Chuck lo mira. El camarero se convierte en cerveza.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. Todos los clientes se convierten en abstemios por instinto de supervivencia.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. La barra se endereza sola.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El WiFi mejora.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. Nadie lo mira a los ojos. Ni los espejos.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El bar se convierte en biblioteca por respeto a su silencio.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El happy hour termina. El bar se pone serio.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. Pide un vaso vac√≠o. Se emborracha el vaso.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El bar sale corriendo.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El DJ pone silencio.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. Se sirve solo. El vaso le da propina.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El suelo se convierte en alfombra roja.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El bartender se disculpa por no haberlo hecho famoso antes.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. Las luces se apagan para no deslumbrarlo.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El letrero cambia a 'Museo'.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El men√∫ se resume a 'lo que √©l quiera'.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El reloj se detiene para no hacerle perder el tiempo.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. La cerveza se enfr√≠a por s√≠ sola del susto.\"},\n",
        "  {\"input\": \"Cu√©ntame un chiste de Chuck Norris.\", \"output\": \"Chuck Norris entra en un bar. El bar se convierte en gimnasio por reflejo condicionado.\"}\n",
        "]\n",
        "\n",
        "# Format for each example in the prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\")\n",
        "])\n",
        "\n",
        "# Combine examples using FewShotChatMessagePromptTemplate\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples\n",
        ")\n",
        "\n",
        "# Build the final prompt with system instructions + examples + user input\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un generador de chistes de Chuck Norris. Devuelve solo un chiste en espa√±ol, sin explicaciones.\"),\n",
        "    few_shot_prompt,  # Insert example dialogues\n",
        "    (\"user\", \"{input}\")  # Placeholder for the actual user request\n",
        "])\n",
        "\n",
        "# Link the final prompt to the model\n",
        "chain_few_shot = final_prompt | llm\n",
        "\n",
        "# Input for a new joke\n",
        "text = \"Cu√©ntame un chiste de Chuck Norris.\"\n",
        "response = chain_few_shot.invoke({\"input\": text})\n",
        "\n",
        "# Display the result\n",
        "print(\"\\n--- Few-Shot ---\")\n",
        "print(f\"Input: {text}\")\n",
        "print(f\"Model Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw5vQHxlgk8F"
      },
      "source": [
        "#Conclusions and Next Steps\n",
        "\n",
        "In this notebook, we gradually evolved from a simple prompt. Along the way, we learned how to:\n",
        "\n",
        "- Using the Groq API to choose, configure, load, and invoke an LLM programmatically‚Äîenabling generative AI‚Äìpowered software development.\n",
        "\n",
        "- Craft more reliable and informative prompts using LangChain‚Äôs ChatPromptTemplate.\n",
        "\n",
        "- Apply zero-shot, one-shot, and few-shot examples for better control over model behavior.\n",
        "\n",
        "\n",
        "\n",
        "**Next Steps**\n",
        "\n",
        "- Try to summarize a long text with LangChain.\n",
        "\n",
        "- Manage conversation history with structured memory using LangGraph.\n",
        "\n",
        "- Retrieval-Augmented Generation (RAG): Enhance your chatbot by connecting it to external knowledge sources (e.g., documents, databases). This lets it retrieve relevant context before generating a response‚Äîideal for question answering, support bots, and knowledge assistants.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
