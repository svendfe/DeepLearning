{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_935P4cAWPzt"
      },
      "source": [
        "#License and Attribution\n",
        "\n",
        "This notebook was developed by Emilio Serrano, Full Professor at the Department of Artificial Intelligence, Universidad PolitÃ©cnica de Madrid (UPM), for educational purposes in UPM courses. Personal website: https://emilioserrano.faculty.bio/\n",
        "\n",
        "ðŸ“˜ License: Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
        "\n",
        "You are free to: (1) Share â€” copy and redistribute the material in any medium or format; (2) Adapt â€” remix, transform, and build upon the material.\n",
        "\n",
        "Under the following terms: (1) Attribution â€” You must give appropriate credit, provide a link to the license, and indicate if changes were made; (2) NonCommercial â€” You may not use the material for commercial purposes; (3) ShareAlike â€” If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
        "\n",
        "ðŸ”— License details: https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COvVKkgKln5A"
      },
      "source": [
        "#Building a Text Classifier with LSTMs in Keras\n",
        "In this notebook,  we build a text classification model using Recurrent Neural Networks (RNNs), specifically Bidirectional LSTMs, with the help of Keras â€” the high-level deep learning API of TensorFlow.\n",
        "\n",
        "We will use a classic dataset (IMDB movie reviews) to train a model that can distinguish positive and negative sentiments in text.\n",
        "\n",
        "This notebook also serves as an introduction to:\n",
        "\n",
        "- Keras: an intuitive, modular, and easy-to-use deep learning API.\n",
        "- KerasHub: hub of prebuilt, ready-to-use models and components that help you speed up development and experiment with state-of-the-art architectures with just a few lines of code.\n",
        "\n",
        "Throughout this notebook, you will learn:\n",
        "\n",
        "- How to preprocess text data for training.\n",
        "\n",
        "- How to use an Embedding layer and stack LSTM layers.\n",
        "\n",
        "- How to evaluate and interpret your model.\n",
        "\n",
        "- How to extend your work using tools from KerasNLP and KerasHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNkD-D4wT8ai"
      },
      "source": [
        "#Keras\n",
        "\n",
        "Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation.  \n",
        "\n",
        "Check the [getting started](https://keras.io/getting_started/) and also the list of [examples](https://keras.io/examples/).\n",
        "\n",
        "Examples of Keras for NLP include:\n",
        "\n",
        "* Text classification from scratch\n",
        "* Review Classification using Active Learning\n",
        "* Text Classification using FNet\n",
        "* Large-scale multi-label text classification\n",
        "* Text classification with Transformer\n",
        "* Text classification with Switch Transformer\n",
        "* Text classification using Decision Forests and pretrained embeddings\n",
        "* Using pre-trained word embeddings\n",
        "* **Bidirectional LSTM on IMDB**\n",
        "\n",
        "Let us see an example of how to train a 2-layer bidirectional LSTM on the IMDB movie review sentiment classification dataset (complete [here](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpRWIIBCldej"
      },
      "source": [
        "##GPUs?\n",
        "\n",
        "\n",
        "Make sure you have your GPU available. While LSTMs process sequences step-by-step, many internal operations (like the gate computations) can be efficiently parallelized on a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znoLTOrdlc4V"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check for GPU availability\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    print(f\"GPU detected: {gpus[0].name}\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n",
        "    print(\"If you are using Google Colab, please go to 'Runtime' > 'Change runtime type' and select 'GPU' as the hardware accelerator.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsP36k5oNg3I"
      },
      "source": [
        "##Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP-wS7AQNg3J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSb-6gHzNg3L"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Note that the model includes: an embedding layer, two bidirectional LSTM layers, and a final dense layer for the classification.\n",
        "\n",
        "\n",
        "Note that the first LSTM returns the full sequence (all time steps) because the second LSTM needs a sequence input.\n",
        "\n",
        "The second LSTM returns only the last output (default return_sequences=False), since after the last LSTM you usually want a single vector to feed the Dense layer for prediction.\n",
        "\n",
        "Since this dense layer use the sigmoid function, it allows a binary classification (positive vs negative sentiment in this case).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-koHZbwONg3M"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "max_features = 20000  # Use only the top 20,000 most frequent words\n",
        "maxlen = 200          # Each review is truncated or padded to 200 words\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    # Embedding layer converts word indices into dense vectors\n",
        "    Embedding(input_dim=max_features, output_dim=128, input_length=maxlen),\n",
        "\n",
        "    # First Bidirectional LSTM layer returns full sequences to feed into next LSTM\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "\n",
        "    # Second Bidirectional LSTM layer summarizes the sequence into one vector\n",
        "    Bidirectional(LSTM(64)),\n",
        "\n",
        "    # Output layer for binary classification (positive or negative sentiment)\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Show model architecture\n",
        "model.build(input_shape=(None, maxlen))  # None es el batch size\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYRCKd69oimE"
      },
      "source": [
        "##Let us study the output shapes.\n",
        "\n",
        "**Embedding Layer** (embedding) â†’ Output shape: (None, 200, 128):\n",
        "This layer is responsible for converting each word in the input text into a dense vector representation of size 128. The input to the model is a sequence of word indices, each representing a word in the vocabulary. Because we set maxlen = 200, each input sequence contains exactly 200 words. Therefore, for each input example, the output of the embedding layer is a matrix of shape (200, 128), where 200 is the number of words and 128 is the dimensionality of each embedding. The first dimension, None, indicates that the batch size can vary during training or inference.\n",
        "\n",
        "**First Bidirectional LSTM Layer** (bidirectional) â†’ Output shape: (None, 200, 128):\n",
        "This layer consists of two LSTM networks running in opposite directionsâ€”one processes the input sequence from start to end (forward), and the other from end to start (backward). Each LSTM has 64 units, and the outputs from both directions are concatenated, resulting in 128 features per time step. Since return_sequences=True is set, the layer returns the full output sequence for each time step in the input. The shape (None, 200, 128) thus indicates that for each of the 200 time steps, the layer outputs a 128-dimensional vector, preserving the sequential structure of the input.\n",
        "\n",
        "**Second Bidirectional LSTM Layer** (bidirectional_1) â†’ Output shape: (None, 128):\n",
        "This layer is similar in structure to the previous bidirectional LSTM, again with 64 units in each direction. However, this time return_sequences=False (which is the default), so the layer does not return the full sequence of outputs. Instead, it returns only the final hidden state after processing the entire input sequence. The outputs from the forward and backward LSTMs are concatenated to form a single 128-dimensional vector. As a result, the shape (None, 128) means that each input sequence is now represented by a single fixed-size vector summarizing the information extracted by both LSTMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyETfpCtNg3M"
      },
      "source": [
        "## Load the IMDB movie review sentiment data\n",
        "\n",
        "Loads the [IMDB dataset](https://keras.io/api/datasets/imdb/).  \n",
        "\n",
        "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers).\n",
        "\n",
        "The IMDb dataset reserves indices 0, 1, and 2 for special tokens (padding, start, unknown or oov). So actual words start from index 3 in the sequences.   \n",
        "\n",
        "While LSTMs can conceptually process variable-length sequences, padding is a practical necessity to efficiently batch data during training and evaluation in frameworks like Keras. Without padding, training on batches would be very complicated and inefficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdrJpGTGNg3N"
      },
      "outputs": [],
      "source": [
        "# Load the IMDb dataset from Keras datasets module.\n",
        "# Only keep the top 'max_features' most frequent words in the dataset.\n",
        "# This returns pre-tokenized sequences of word indices for training and validation sets.\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Print the number of training and validation sequences loaded\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "\n",
        "# Pad (or truncate) each sequence to a fixed length 'maxlen'.\n",
        "# Sequences longer than 'maxlen' will be truncated (cut off) at the beginning.\n",
        "#     ...By default, pad_sequences in Keras truncates sequences at the beginning (truncating='pre')\n",
        "# Sequences shorter than 'maxlen' will be padded with zeros at the start by default.\n",
        "# This ensures all input sequences have the same length, which is required for batch processing in the model.\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)\n",
        "\n",
        "print(\"Training sample: a vector of 200 dimensions, each one representing a word index (including padding with word indexed as 0):\")\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRJZBuHaNg3N"
      },
      "source": [
        "## Train and evaluate the model\n",
        "\n",
        "This code snippet demonstrates the process of compiling, training, and evaluating a neural network model for a binary classification task using Keras. We compile the model with the Adam optimizer and binary cross-entropy loss, train it on the training data while validating performance on a separate validation set, and finally evaluate the modelâ€™s accuracy and loss on the validation data.\n",
        "\n",
        "Note that only 3 epochs are used (too few).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEYIQxvKq-Ad"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer and binary crossentropy loss\n",
        "# Metrics to track during training include accuracy\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model on training data with a batch size of 32, for 3 epochs\n",
        "# Validate on (x_val, y_val) at the end of each epoch\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_val, y_val))\n",
        "\n",
        "# After training, evaluate the model performance on the validation set\n",
        "loss, accuracy = model.evaluate(x_val, y_val)\n",
        "print(f'Val loss: {loss:.4f}, Val accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T8RVYXsrQU4"
      },
      "source": [
        "In some experiments Val accuracy: 0.8723, showing that  the model trained well, achieving good accuracy and low loss on both training and validation data. The validation accuracy close to training accuracy indicates reasonable generalization. More ephocs could be beneficial if overfitting does not occur.\n",
        "\n",
        "Additionally, we can visualize the training and validation loss and accuracy across epochs to help monitor the modelâ€™s learning progress and detect potential overfitting or underfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXx3wcEVrAzG"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Code to plot training and validation loss/accuracy curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract loss and accuracy history from the training process\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_acc = history.history.get('accuracy')\n",
        "val_acc = history.history.get('val_accuracy')\n",
        "\n",
        "# Plot Loss curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy curves if available\n",
        "if train_acc and val_acc:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFEmo_pws1YT"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9pUslQpbPzm"
      },
      "source": [
        "## Using the model  \n",
        "\n",
        "The published example does not show how to use the model!.\n",
        "\n",
        "Let us try to get predction for the phrases:\n",
        "- \"I loved the movie, it was amazing!\"\n",
        "- \"This movie was terrible\"\n",
        "- \"This movie was not good at all.\"\n",
        "\n",
        "Unlike the pipeline function in HuggingFace, Keras requires to preprocess samples and the network output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "657pqRfFbS3F"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Example raw reviews\n",
        "reviews = [\"I loved the movie, it was amazing!\", \"The movie was terrible\", \"This movie was not good at all\"]\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 200\n",
        "\n",
        "# We have not used a tokenizer but data already pre-tokenized as sequences of word indices.\n",
        "# Therefore, text samples need to be translated into these indices.\n",
        "# The IMDb dataset reserves indices 0, 1, and 2 for special tokens (padding, start, unknown).\n",
        "# So actual words start from index 3 in the sequences.\n",
        "\n",
        "# Load IMDb word index (word to integer mapping)\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def text_to_sequence(text):\n",
        "    # Tokenize and convert text to integer sequence based on IMDb's word index\n",
        "    tokens = text.lower().split()  # simple whitespace tokenizer, could be improved\n",
        "    sequence = []\n",
        "    for word in tokens:\n",
        "        index = word_index.get(word, 2)  # 2 is usually the index for \"unknown\", returns 2 if word not found\n",
        "        if index < max_features:\n",
        "            sequence.append(index + 3)  # offset by 3 because of reserved indices in IMDb dataset\n",
        "    return sequence\n",
        "\n",
        "\n",
        "# Convert raw text reviews to padded sequences\n",
        "sequences = [text_to_sequence(review) for review in reviews]\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Now you can predict using your model\n",
        "predictions = model.predict(padded_sequences)\n",
        "print(predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWcJKXSLv1Qa"
      },
      "source": [
        "Let us study this output. Your model is trained to predict the sentiment of movie reviews as a binary classification task. The output is a 2D array where each inner array corresponds to the modelâ€™s prediction for one review. Each value is a probability between 0 and 1 representing the likelihood that the review is positive (usually 1 means positive, 0 means negative).\n",
        "\n",
        "If the model prediction for  \"I loved the movie, it was amazing!\"  is bigger than 0,5; that is the probability that this review is positive, which makes sense since the review uses positive words like \"loved\" and \"amazing\".\n",
        "\n",
        "We can postprocess these values further to obtain \"Positive\" and \"Negative\" labels in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2BO67ac0f28"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Print each review with its predicted sentiment\n",
        "for review, pred in zip(reviews, predictions):\n",
        "    sentiment = \"Positive\" if pred[0] >= 0.5 else \"Negative\"\n",
        "    print(f'Review: \"{review}\"\\nPredicted sentiment: {sentiment} (probability of being positive: {pred[0]:.4f})\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS1uKWal28xV"
      },
      "source": [
        "How did it go for \"This movie was not good at all\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyPWOjCJBWwp"
      },
      "source": [
        "##Checking tokenization\n",
        "It s a good idea to add a small helper function to verify that text-to-index and index-to-text mapping are consistent. This ensures the tokenizer mapping used in generation is correct and reversible.\n",
        "\n",
        "Remember that in this example we have not used a tokenizer but data already pre-tokenized as sequences of word indices.\n",
        "\n",
        "Deploying Deep Learning systems for NLP is largely about data preprocessing, rather than selecting one architecture or another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw8v1F7-BZPS"
      },
      "outputs": [],
      "source": [
        "# Reverse the IMDb word index to get index â†’ word mapping\n",
        "index_word = {index + 3: word for word, index in imdb.get_word_index().items()}\n",
        "index_word[0] = \"<PAD>\"\n",
        "index_word[1] = \"<START>\"\n",
        "index_word[2] = \"<UNK>\"\n",
        "\n",
        "def check_imdb_sequence_conversion(text, word_index, max_features=20000):\n",
        "    \"\"\"\n",
        "    Check how a raw review text is converted into a padded sequence,\n",
        "    and how the sequence maps back to words.\n",
        "    \"\"\"\n",
        "    print(\"Original text:\", text)\n",
        "    tokens = text.lower().split()\n",
        "\n",
        "    # Convert to word indices\n",
        "    sequence = []\n",
        "    for word in tokens:\n",
        "        index = word_index.get(word, 2)\n",
        "        if index < max_features:\n",
        "            sequence.append(index + 3)\n",
        "\n",
        "    print(\"\\nToken indices:\", sequence)\n",
        "\n",
        "    # Map back to words\n",
        "    recovered_words = [index_word.get(i, \"<UNK>\") for i in sequence]\n",
        "    print(\"Recovered words:\", recovered_words)\n",
        "\n",
        "    # Check mapping integrity\n",
        "    print(\"\\nMapping check:\")\n",
        "    for original, recovered in zip(tokens, recovered_words):\n",
        "        match = \"âœ“\" if original == recovered else \"âœ—\"\n",
        "        print(f\"{original:15} -> {recovered:15} {match}\")\n",
        "\n",
        "check_imdb_sequence_conversion(\"This movie was not good at all\", word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlftsWrjAVkb"
      },
      "source": [
        "# KerasHub  \n",
        "\n",
        "**Update 2025**: KerasNLP has renamed to KerasHub!\n",
        "\n",
        "[KerasHub](https://keras.io/keras_hub/getting_started/) is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch.\n",
        "\n",
        "KerasHub supports users through their entire development cycle. KerasHub **workflows** are built from modular components that have state-of-the-art preset weights and architectures when used **out-of-the-box** and are easily customizable when more control is needed.\n",
        "\n",
        "Check the [getting started](https://keras.io/keras_hub/getting_started/). As HuggingFace, KerasHub contains end-to-end implementations of popular model architectures such as BERT and GPT2. Check [KerasHub models](hhttps://keras.io/keras_hub/presets/).\n",
        "\n",
        "Let us see an example from the getting started (complete [here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/keras_nlp/getting_started.ipynb#scrollTo=eLqs_qlzKPoP)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObLRe8ijUTde"
      },
      "source": [
        "##Installing and importing KerasHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ildWtPibMAKS"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGE8kxsQMA6h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "# Use mixed precision to speed up all training in this guide.\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt1lP93rM3_4"
      },
      "source": [
        "##Basic use of KerasHub workflow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTia2iWgKbzW"
      },
      "outputs": [],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "# Note: batched inputs expected so must wrap string in iterable\n",
        "logits= classifier.predict([\"I love modular workflows in keras-nlp!\", \"I hate modular workflows in keras-nlp!\"])\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDPO7x5RGE8"
      },
      "source": [
        "##Logits vs probabilities\n",
        "\n",
        "In previous example, Outputs are the logits per class (e.g., `[0, 0]` is 50% chance of positive). The output is [negative, positive] for binary classification.\n",
        "\n",
        "Those are not probabilities but **logits**: the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer.\n",
        "\n",
        "**Keras (and HuggingFace) models outputs logits instead of probabilities by default** (because it allows for greater flexibility in model training).\n",
        "\n",
        "Let us get probabilities form logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKwAsPPJPSLg"
      },
      "outputs": [],
      "source": [
        "#logits can be converted into probabilities\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "# get probabilities using softmax from logit score and convert it to numpy array\n",
        "probabilities_scores = F.softmax(torch.from_numpy(logits), dim = -1).numpy()\n",
        "print(f\"Negative, Positive probability: {probabilities_scores}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChZQW5O7LgIa"
      },
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "In this notebook, we explored how to build and train a Bidirectional LSTM model to classify movie reviews from the IMDB dataset. Bidirectional LSTMs are powerful because they capture context from both past and future words, improving understanding of sequence data such as text.\n",
        "\n",
        "\n",
        "What do KerasHub \"workflows\" remind you of? As the HuggingFace transformer library and its pipeline function, workflows allows a basic use and more advance features such as fine-tuning at different levels and pretraining a LM as BERT.\n",
        "\n",
        "New tools will always emerge to simplify standard machine learning and NLP workflows. However, the deeper your understanding of the underlying mechanisms, the better your ability to:\n",
        "\n",
        "- Customize models effectively\n",
        "\n",
        "- Troubleshoot issues\n",
        "\n",
        "- Achieve improved performance â€” even when using black-box systems like ChatGPT\n",
        "\n",
        " Here are some ideas for how you could continue to explore and improve this project:\n",
        "\n",
        " *   You can try to optimize the previous model. There are automatic machine learning tools that make this task much easier, such as [KerasTuner](https://keras.io/keras_tuner/) or [AutoKeras](https://autokeras.com/)\n",
        "*  You can try reproducing another [Keras example for NLP]( https://keras.io/examples/nlp/)\n",
        "* You can try using KerasHub for fine-tuning a pretrained model [Getting Started with KerasHub](https://keras.io/keras_hub/getting_started/)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}